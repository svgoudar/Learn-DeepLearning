{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61ad7e1",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Adam Optmizers\n",
    "\n",
    "Adam combines the **best features of previous optimizers**:\n",
    "\n",
    "1. **Momentum (from SGD with momentum)**\n",
    "\n",
    "   * Smooths updates of weights and biases to reduce zigzag/noisy paths.\n",
    "   * Uses **exponentially weighted moving average** (EWMA) of past gradients.\n",
    "\n",
    "2. **RMSProp features**\n",
    "\n",
    "   * Dynamic learning rate that adapts per parameter.\n",
    "   * Smoothens large or small gradient updates to prevent learning rate from exploding or vanishing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Weight Update Formula**\n",
    "\n",
    "For weights (w) and bias (b):\n",
    "\n",
    "$$\n",
    "w_t = w_{t-1} - \\eta_t \\cdot vdw_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_t = b_{t-1} - \\eta_t \\cdot vdb_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\eta_t$ = **dynamic learning rate**\n",
    "* $vdw_t$, $vdb_t$ = **smoothed gradients** via momentum:\n",
    "\n",
    "$$\n",
    "vdw_t = \\beta \\cdot vdw_{t-1} + (1-\\beta) \\cdot \\frac{\\partial L}{\\partial w_{t-1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "vdb_t = \\beta \\cdot vdb_{t-1} + (1-\\beta) \\cdot \\frac{\\partial L}{\\partial b_{t-1}}\n",
    "$$\n",
    "\n",
    "* $\\beta$ = smoothing factor (e.g., 0.9)\n",
    "* Exponential weighted averages are used for **both momentum and RMSProp-style dynamic learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Adam Works**\n",
    "\n",
    "1. **Momentum smoothing**: Past gradients influence current update â†’ reduces oscillations.\n",
    "2. **Adaptive learning rate**: Parameters with large gradients are scaled down; small gradients scaled up.\n",
    "3. **Bias correction**: Ensures initial steps are not underestimated due to EWMA starting at zero.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Adam is Popular**\n",
    "\n",
    "* Combines **momentum + RMSProp + adaptive learning rate**\n",
    "* Faster convergence\n",
    "* Works well for deep neural networks, CNNs, RNNs, and most other architectures\n",
    "* Robust to noisy or sparse gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321d776",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
