{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6fac94",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Adadelta and RMSProp\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem Recap**\n",
    "\n",
    "* **Adagrad** introduced dynamic learning rate:\n",
    "  $$\n",
    "  \\eta_t = \\frac{\\eta}{\\sqrt{\\alpha_t} + \\epsilon}\n",
    "  $$\n",
    "* **Issue:** In deep networks, $\\alpha_t$ (sum of squared gradients) can become **very large** → $\\eta_t$ becomes **almost zero** → weight updates stop → learning stalls.\n",
    "\n",
    "---\n",
    "\n",
    "### **Adadelta & RMSProp Solution**\n",
    "\n",
    "* Goal: Prevent learning rate from shrinking too much.\n",
    "* Key ideas:\n",
    "\n",
    "  1. Use **exponential weighted average (EWA)** of past squared gradients instead of raw sum.\n",
    "  2. Restrict extreme growth of gradient history.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**\n",
    "\n",
    "1. Compute smoothed squared gradient:\n",
    "   $$\n",
    "   sdw_t = \\beta \\cdot sdw_{t-1} + (1 - \\beta) \\cdot \\left(\\frac{\\partial L}{\\partial w_{t-1}}\\right)^2\n",
    "   $$\n",
    "\n",
    "* $\\beta$ = smoothing factor (e.g., 0.95)\n",
    "* This keeps the gradient history from exploding.\n",
    "\n",
    "2. Update weight with **dynamic learning rate**:\n",
    "   $$\n",
    "   w_t = w_{t-1} - \\frac{\\eta}{\\sqrt{sdw_t} + \\epsilon} \\cdot \\frac{\\partial L}{\\partial w_{t-1}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "* **Dynamic Learning Rate:** Like Adagrad, but avoids shrinking too much.\n",
    "* **Smoothing:** Exponential weighted average reduces noise in gradient updates.\n",
    "* **Initialization:** (sdw_0 = 0) at the start.\n",
    "* **Difference between Adadelta & RMSProp:** Minor variations in implementation; conceptually the same.\n",
    "\n",
    "---\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "1. Prevents learning rate from becoming too small (unlike Adagrad).\n",
    "2. Smooths gradient updates → more stable convergence.\n",
    "3. Works well in deep neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Differences Between Adadelta & RMSProp**\n",
    "\n",
    "* **RMSProp**: Standard use of smoothed squared gradients in denominator.\n",
    "* **Adadelta**: Further modifies the update to remove the need for a global learning rate (\\eta) by also tracking previous parameter updates (adaptive step size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0a35e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
