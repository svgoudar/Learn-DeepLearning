{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0ec97e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## SGD with Momentum\n",
    "\n",
    "* In **Mini-batch SGD**, we update weights using a small batch of data.\n",
    "* This reduces the noise compared to SGD (single data point) but **some noise still exists**.\n",
    "* Noise causes **zigzag updates** in the loss surface, slowing convergence toward the global minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **SGD with Momentum**\n",
    "\n",
    "Momentum is introduced to **smooth these zigzag updates**.\n",
    "\n",
    "#### **Weight Update Formula**\n",
    "\n",
    "Without momentum (standard SGD):\n",
    "$$\n",
    "w_t = w_{t-1} - \\eta \\frac{\\partial L}{\\partial w_{t-1}}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "* $w_t$ = new weight\n",
    "* $w_{t-1}$ = previous weight\n",
    "* $\\eta$ = learning rate\n",
    "* $L$ = loss function\n",
    "\n",
    "With momentum:\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1-\\beta) \\frac{\\partial L}{\\partial w_{t-1}}\n",
    "$$\n",
    "$$\n",
    "w_t = w_{t-1} - \\eta v_t\n",
    "$$\n",
    "Where:\n",
    "\n",
    "* $v_t$ = velocity term (smoothed gradient)\n",
    "* $\\beta$ = momentum factor (0 < β < 1)\n",
    "* High β → previous gradients dominate → smoother updates\n",
    "* Low β → current gradient dominates → less smoothing\n",
    "\n",
    "---\n",
    "\n",
    "### Exponential Weighted Average\n",
    "\n",
    "* Momentum uses **exponential weighted average (EWA)** of past gradients.\n",
    "* Formula for smoothing time-series or gradients:\n",
    "  $\n",
    "  V_t = \\beta V_{t-1} + (1-\\beta) A_t\n",
    "  $\n",
    "* $A_t$ = current value (gradient at time t)\n",
    "* $V_t$ = smoothed gradient\n",
    "* Effect: reduces noise and zigzag in weight updates.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "* Without momentum: updates bounce back and forth → slow convergence.\n",
    "* With momentum: updates accumulate direction → smoother path to global minimum.\n",
    "* Analogy: momentum “pushes” updates in the right direction, dampening oscillations.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. Reduces noise in Mini-batch SGD.\n",
    "2. Faster convergence to global minimum.\n",
    "3. Smoother weight updates → more stable training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee96497",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
