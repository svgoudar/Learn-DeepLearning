{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8974d291",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Gradient Descent Optimizer\n",
    "\n",
    "Gradient Descent (GD) is an **optimizer** used to minimize the loss (or cost) function by updating the weights of a neural network during **backpropagation**.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose of an Optimizer\n",
    "\n",
    "* Forward propagation calculates predicted outputs ( \\hat{y} ) from inputs ( X ).\n",
    "* Loss function measures error between predicted ( \\hat{y} ) and actual ( y ).\n",
    "* Optimizer updates **weights** to reduce this loss iteratively.\n",
    "\n",
    "**Weight update formula:**\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w_{\\text{old}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\eta$ = learning rate (step size)\n",
    "* $\\frac{\\partial L}{\\partial w}$ = gradient of loss w.r.t weights\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Descent Process\n",
    "\n",
    "1. Initialize weights randomly.\n",
    "2. Forward propagate all data points to calculate predictions.\n",
    "3. Compute the **cost function** over all data points.\n",
    "\n",
    "   * For example, MSE:\n",
    "     $$\n",
    "     \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "     $$\n",
    "4. Backpropagate gradients and update weights using GD formula.\n",
    "5. Repeat for multiple **epochs** until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### Epochs vs Iterations\n",
    "\n",
    "* **Epoch:** One complete pass through all training data.\n",
    "* **Iteration:** One weight update using a subset of data (mini-batch).\n",
    "* In **classic gradient descent**, all data points are used for one update â†’\n",
    "  **1 epoch = 1 iteration**.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization\n",
    "\n",
    "* Loss surface is typically convex (parabola for simple cases).\n",
    "* Gradient descent moves weights **downhill** toward the **global minimum** of the loss.\n",
    "* When gradient = 0, weights stop updating (convergence).\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Guaranteed convergence for convex loss surfaces.\n",
    "* Simple and foundational concept for all other optimizers.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* **Resource intensive:** Requires all data in memory for each update.\n",
    "\n",
    "  * For large datasets, memory (RAM) and GPU usage is very high.\n",
    "* Slow for very large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "Gradient Descent works well for small datasets but struggles with very large datasets due to memory and computational requirements. Variants like **Stochastic Gradient Descent (SGD)** or **Mini-batch GD** address this limitation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
