{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6703ff1b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Adagrad Optimizer\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem Recap**\n",
    "\n",
    "* In standard SGD or Mini-batch SGD, the **learning rate (η)** is fixed.\n",
    "* Fixed learning rate:\n",
    "\n",
    "  * Too high → overshoot global minimum.\n",
    "  * Too low → slow convergence.\n",
    "* Ideal: **dynamic learning rate** — bigger steps initially, smaller steps near minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Adagrad Idea**\n",
    "\n",
    "* **Adagrad = Adaptive Gradient Descent**.\n",
    "* Adjusts learning rate individually for each parameter.\n",
    "* Parameters with **frequent updates → smaller learning rate**.\n",
    "* Parameters with **infrequent updates → larger learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Weight Update Formula**\n",
    "\n",
    "$$\n",
    "w_t = w_{t-1} - \\eta_t \\frac{\\partial L}{\\partial w_{t-1}}\n",
    "$$\n",
    "\n",
    "Where the **dynamic learning rate** is:\n",
    "$$\n",
    "\\eta_t = \\frac{\\eta}{\\sqrt{\\alpha_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "* $\\eta$ = initial learning rate\n",
    "* $\\alpha_t = \\sum_{i=1}^{t} (\\frac{\\partial L}{\\partial w_i})^2$ → accumulated squared gradients\n",
    "* $\\epsilon$ = small constant to avoid division by zero\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Initially, αₜ is small → ηₜ is large → bigger steps → faster convergence.\n",
    "* Over time, αₜ grows → ηₜ decreases → smaller steps → precise approach to minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "* Think of it as **automatic step size adjustment**:\n",
    "\n",
    "  * High gradient history → slow down (avoid overshoot).\n",
    "  * Low gradient history → speed up (less sensitive parameters).\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "1. Dynamic learning rate for each parameter.\n",
    "2. Faster convergence initially.\n",
    "3. Handles sparse data well (features updated infrequently get bigger steps).\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "1. **Aggressive decay**: In very deep networks, αₜ can become very large.\n",
    "\n",
    "   * ηₜ → almost zero.\n",
    "   * Weight updates stop → learning stalls.\n",
    "2. Cannot recover from too small learning rate → convergence can freeze.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
