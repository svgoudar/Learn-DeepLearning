{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62ceef4",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## ANN vs CNN\n",
    "\n",
    "### In ANN (Artificial Neural Network)\n",
    "\n",
    "1. **Input:** Numeric feature vector\n",
    "2. **Operation:**\n",
    "   $z = W^T x + b$\n",
    "3. **Activation:** Apply ReLU or another activation on each neuron\n",
    "4. **Forward pass → loss calculation → backpropagation**\n",
    "5. **Weight updates using optimizer**\n",
    "\n",
    "Everything is fully connected: each input connects to each neuron via weights.\n",
    "\n",
    "---\n",
    "\n",
    "### In CNN (Convolutional Neural Network)\n",
    "\n",
    "1. **Input:** Image (grayscale or RGB)\n",
    "2. **Filters (kernels):**\n",
    "\n",
    "   * 3×3, 5×5, or other sizes\n",
    "   * Multiple filters (F1, F2, F3 … Fn)\n",
    "   * Initialized **randomly**, not fixed like edge/shape detectors\n",
    "3. **Convolution operation:**\n",
    "\n",
    "   * Slide filter over the image\n",
    "   * Perform elementwise multiplication + sum\n",
    "   * Produces **feature maps**\n",
    "4. **Activation:**\n",
    "   Apply **ReLU** on each feature map value:\n",
    "   $$\n",
    "   \\text{ReLU}(x) = \\max(0, x)\n",
    "   $$\n",
    "   ✔ Prevents vanishing gradients\n",
    "   ✔ Simple derivative (0 or 1) → easy backprop\n",
    "5. **Backpropagation:**\n",
    "\n",
    "   * Gradients update filter weights\n",
    "   * Same mechanism as ANN\n",
    "6. **Next step (after ReLU):** Max Pooling (downsampling)\n",
    "\n",
    "---\n",
    "\n",
    "### Why ReLU is applied after convolution?\n",
    "\n",
    "* Derivative is easy (0 or 1)\n",
    "* Required for backprop to update filter weights\n",
    "* Prevents vanishing gradient\n",
    "* Preserves non-linearity\n",
    "\n",
    "---\n",
    "\n",
    "**Key Conceptual Difference**\n",
    "\n",
    "| Step            | ANN                          | CNN                                 |\n",
    "| --------------- | ---------------------------- | ----------------------------------- |\n",
    "| Input           | Feature vector               | Image (2D/3D)                       |\n",
    "| Learnable units | Weights for every connection | Filters (kernels)                   |\n",
    "| Operation       | Dot product per neuron       | Convolution over spatial regions    |\n",
    "| Activation      | After weighted sum           | After convolution (per feature map) |\n",
    "| Parameters      | Large (dense)                | Fewer (shared weights)              |\n",
    "| Processing      | Whole input at once          | Local receptive fields              |\n",
    "\n",
    "---\n",
    "\n",
    "If you want a one-liner:\n",
    "\n",
    "**ANN learns weights for each input → neuron connection.\n",
    "CNN learns filters that scan images to extract patterns (edges, textures, objects). ReLU enables gradient flow so filters get updated.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554f9c5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
