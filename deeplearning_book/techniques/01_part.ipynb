{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd7d431",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## Exploding Gradient Problem \n",
    "\n",
    "* Exploding gradients happen when gradients (derivatives of the loss with respect to weights) become **extremely large** during backpropagation.\n",
    "* This makes weight updates huge, causing the model to **overshoot the minimum** or diverge entirely.\n",
    "* It’s the opposite of vanishing gradients, where updates become too small.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why It Happens**\n",
    "\n",
    "1. **Backpropagation multiplies gradients layer by layer**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial o_3} \\cdot \\frac{\\partial o_3}{\\partial o_2} \\cdot \\frac{\\partial o_2}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial w_1}\n",
    "   $$\n",
    "\n",
    "2. **Large weights amplify gradients**:\n",
    "\n",
    "   * If weights are initialized too big (e.g., 500, 1000), each multiplication in the chain rule makes the gradient even larger.\n",
    "   * Result: `w_new = w_old - learning_rate * gradient` becomes **too large**, causing huge swings in weight values.\n",
    "\n",
    "3. **Effect on training**:\n",
    "\n",
    "   * Loss may oscillate or shoot up instead of decreasing.\n",
    "   * Model may fail to converge to the global minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Factors**\n",
    "\n",
    "* **Weight Initialization**: Large initial weights → exploding gradients.\n",
    "* **Activation Functions**: Certain activations (like sigmoid/tanh) can also amplify or saturate gradients.\n",
    "* **Deep Networks**: More layers → more multiplications → higher risk.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solutions**\n",
    "\n",
    "* Use **careful weight initialization techniques**, like:\n",
    "\n",
    "  * **Xavier/Glorot Initialization** for sigmoid/tanh\n",
    "  * **He Initialization** for ReLU\n",
    "* Use **gradient clipping**: Limit the gradient magnitude to a maximum value.\n",
    "* Prefer **ReLU activations** for deep networks to reduce vanishing/exploding risks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
