{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f375c5",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## Dropout Layer\n",
    "\n",
    "### **Problem: Overfitting**\n",
    "\n",
    "* Occurs when a model performs well on **training data** but poorly on **test data**.\n",
    "\n",
    "  * Example: Training accuracy = 90%, Test accuracy = 60%.\n",
    "* Overfitting happens because the network memorizes patterns in training data, losing generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analogy with Random Forest**\n",
    "\n",
    "* Random forest reduces overfitting by:\n",
    "\n",
    "  1. **Feature sampling** → each tree sees only a subset of features.\n",
    "  2. **Row sampling** → each tree sees only a subset of data points.\n",
    "* Similarly, in neural networks, **dropout** randomly “removes” neurons during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dropout in Neural Networks**\n",
    "\n",
    "1. During **training**:\n",
    "\n",
    "   * Each neuron is **randomly deactivated** with a probability (p).\n",
    "   * Example: If (p = 0.5) and a layer has 4 neurons → 2 neurons are randomly turned off.\n",
    "   * This happens **independently in each forward pass** (each batch/epoch).\n",
    "   * Purpose: prevents co-adaptation of neurons → forces network to learn **redundant, robust representations**.\n",
    "\n",
    "2. During **backpropagation**:\n",
    "\n",
    "   * Deactivated neurons **do not participate** in weight updates.\n",
    "   * Active neurons update as usual.\n",
    "\n",
    "---\n",
    "\n",
    "### **During Testing / Inference**\n",
    "\n",
    "* Dropout is **not applied**.\n",
    "* To account for missing neurons during training, the weights are **scaled by the dropout probability** (p).\n",
    "\n",
    "  * Example: If (p = 0.5), all trained weights are multiplied by 0.5.\n",
    "* Ensures outputs are consistent with the expectations of the trained network.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "* Dropout probability (p) is a **hyperparameter** (typically 0.2–0.5).\n",
    "* Reduces **overfitting** and improves **generalization**.\n",
    "* Can be applied to:\n",
    "\n",
    "  * Input layer → feature sampling.\n",
    "  * Hidden layers → neuron sampling.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Phase             | Dropout Applied? | Weight Adjustment                       |\n",
    "| ----------------- | ---------------- | --------------------------------------- |\n",
    "| Training          | Yes              | No adjustment, just deactivate neurons  |\n",
    "| Testing/Inference | No               | Multiply weights by dropout probability |\n",
    "\n",
    "---\n",
    "\n",
    "Dropout works like “ensemble learning within a single network” — each forward pass sees a slightly different network, preventing over-reliance on any single neuron.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
