{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5440fb11",
   "metadata": {},
   "source": [
    "\n",
    "## Intuition\n",
    "\n",
    "A **Recurrent Neural Network** is designed to learn **sequences** — data where the **order matters**.\n",
    "\n",
    "Think of reading a sentence:\n",
    "\n",
    "* Each new word’s meaning depends on the words before it.\n",
    "* You don’t forget the whole sentence when you read the next word — you **carry context forward**.\n",
    "\n",
    "That’s exactly what RNNs do:\n",
    "\n",
    "* They take **one input at a time**.\n",
    "* They **store context** in a hidden state.\n",
    "* They use that hidden state to influence the next prediction.\n",
    "\n",
    "So, an RNN acts like a **memory-augmented neural network**.\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical View\n",
    "\n",
    "#### Sequence Modeling Problem\n",
    "\n",
    "We are given a sequence of inputs:\n",
    "$$\n",
    "x_1, x_2, x_3, ..., x_T\n",
    "$$\n",
    "and we want to predict either:\n",
    "\n",
    "* An output at each time step: $y_1, y_2, ..., y_T$\n",
    "* Or one final output after all steps (e.g., sentiment score).\n",
    "\n",
    "We need a function that can model dependencies between inputs **across time**.\n",
    "\n",
    "Ordinary neural nets (ANNs) can only model:\n",
    "$$\n",
    "y = f(x)\n",
    "$$\n",
    "RNNs extend this to:\n",
    "$$\n",
    "y_t = f(x_t, x_{t-1}, ..., x_1)\n",
    "$$\n",
    "— using a **hidden state** to summarize the past.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "At each time step ( t ):\n",
    "\n",
    "### (a) Hidden State Update\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "### (b) Output\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "| Symbol       | Meaning                               |\n",
    "| ------------ | ------------------------------------- |\n",
    "| $x_t$      | Input at time ( t )                   |\n",
    "| $h_t$      | Hidden state (memory) at time ( t )   |\n",
    "| $y_t$      | Output at time ( t )                  |\n",
    "| $W_x$      | Input → hidden weights                |\n",
    "| $W_h$      | Hidden → hidden weights (recurrent)   |\n",
    "| $W_y$      | Hidden → output weights               |\n",
    "| $f$        | Activation (tanh / ReLU)              |\n",
    "| $g$        | Output activation (sigmoid / softmax) |\n",
    "| $b_h, b_y$ | Bias terms                            |\n",
    "\n",
    "---\n",
    "\n",
    "### Why Recurrence Works\n",
    "\n",
    "The equation\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "creates a **recurrence relation**:\n",
    "\n",
    "* Each hidden state $h_t$ depends on the previous one $h_{t-1}$.\n",
    "* This “connects time steps together.”\n",
    "\n",
    "When you unfold the recurrence:\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h f(W_x x_{t-1} + W_h h_{t-2} + b_h) + b_h)\n",
    "$$\n",
    "You can see that $h_t$ becomes a **function of all past inputs** $x_t, x_{t-1}, ..., x_1$.\n",
    "\n",
    "Thus, RNNs can remember the history of inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Geometric / Functional Intuition\n",
    "\n",
    "* $W_x x_t$ → captures **current input information**.\n",
    "* $W_h h_{t-1}# → brings **past memory** forward.\n",
    "* Activation $f(\\cdot)$ → squashes combined signal, deciding how much of past and present to keep.\n",
    "* This recurrence lets the network **learn patterns over time**, like “not good” → negative, “very good” → positive.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Forward Pass (Step-by-Step)\n",
    "\n",
    "At $t = 1$:\n",
    "\n",
    "$$\n",
    "h_1 = f(W_x x_1 + b_h)\n",
    "$$\n",
    "\n",
    "At $t = 2$:\n",
    "$$\n",
    "h_2 = f(W_x x_2 + W_h h_1 + b_h)\n",
    "$$\n",
    "\n",
    "At $t = 3$:\n",
    "$$\n",
    "h_3 = f(W_x x_3 + W_h h_2 + b_h)\n",
    "$$\n",
    "\n",
    "\n",
    "Final output (for classification, say sentiment):\n",
    "$$\n",
    "\\hat{y} = \\sigma(W_y h_T + b_y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Backward Pass — Learning\n",
    "\n",
    "To train the RNN, we minimize a loss:\n",
    "$$\n",
    "L = \\sum_t \\ell(y_t, \\hat{y_t})\n",
    "$$\n",
    "\n",
    "and compute gradients by **Backpropagation Through Time (BPTT)** — which unfolds the RNN across all time steps and applies the chain rule backward through each.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Theoretical Challenges\n",
    "\n",
    "Because $h_t$ depends on many previous $h$’s, gradients flow through long chains of matrix multiplications during backpropagation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h} \\propto \\prod_t W_h^T f'(a_t)\n",
    "$$\n",
    "\n",
    "If eigenvalues of $W_h$ are:\n",
    "\n",
    "* \\< 1 → gradients **vanish** (become near zero)\n",
    "* \\> 1 → gradients **explode**\n",
    "\n",
    "This explains the **vanishing/exploding gradient problem**, which limits how far back RNNs can “remember.”\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Long-Term Memory Fixes\n",
    "\n",
    "To overcome that:\n",
    "\n",
    "* **LSTM (Long Short-Term Memory)** introduces gates (forget, input, output) to control memory flow.\n",
    "* **GRU (Gated Recurrent Unit)** simplifies LSTM with fewer gates.\n",
    "\n",
    "Both stabilize gradient flow and allow learning of longer dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Practical Example (Numbers)\n",
    "\n",
    "Suppose:\n",
    "\n",
    "* $x_t \\in \\mathbb{R}^5$\n",
    "* $h_t \\in \\mathbb{R}^3$\n",
    "\n",
    "Then:\n",
    "\n",
    "* $W_x$: 5×3\n",
    "* $W_h$: 3×3\n",
    "* $W_y$: 3×1\n",
    "\n",
    "At each step:\n",
    "$$\n",
    "z_t = W_x x_t + W_h h_{t-1} + b_h\n",
    "$$\n",
    "$$\n",
    "h_t = \\tanh(z_t)\n",
    "$$\n",
    "$$\n",
    "\\hat{y_t} = \\sigma(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "You iterate this through the sequence. The RNN learns weights $W_x, W_h, W_y$ that best map the sequential inputs to outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Summary Table\n",
    "\n",
    "| Concept                  | Role                                         |\n",
    "| ------------------------ | -------------------------------------------- |\n",
    "| Hidden state ( h_t )     | Memory that carries information through time |\n",
    "| Recurrent weight ( W_h ) | Connects previous state to current           |\n",
    "| Activation function      | Non-linear mixing of current and past info   |\n",
    "| BPTT                     | Training method for temporal sequences       |\n",
    "| Problem                  | Vanishing/exploding gradients                |\n",
    "| Solution                 | LSTM / GRU / Attention                       |\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "* Theoretically, RNNs extend neural nets to functions over sequences:\n",
    "  $h_t = f(W_x x_t + W_h h_{t-1})$.\n",
    "* Mathematically, they model **dynamical systems** with recurrent state updates.\n",
    "* Intuitively, they act as a **short-term memory** that learns temporal patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31855279",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
