{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fda6e5d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## RNN\n",
    "\n",
    "**Recurrent Neural Network (RNN)** is a type of neural network **designed to process sequential data**, such as text, speech, time series, or videos.\n",
    "It differs from a standard neural network (ANN) because it **remembers previous inputs** using an internal **hidden state (memory)**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why RNN?\n",
    "\n",
    "In many problems, **context and order** of inputs matter.\n",
    "For example:\n",
    "\n",
    "| Sentence                | Meaning  |\n",
    "| ----------------------- | -------- |\n",
    "| “The food is good.”     | Positive |\n",
    "| “The food is not good.” | Negative |\n",
    "\n",
    "A normal **feedforward neural network (ANN)** treats each input independently.\n",
    "RNNs, however, can use **previous words (context)** to understand the full meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "RNNs introduce a **loop (recurrence)** that allows information to **persist** across time steps.\n",
    "\n",
    "At each time step:\n",
    "\n",
    "* The network reads one element of the sequence (e.g., a word).\n",
    "* It updates a hidden state that carries information from the past.\n",
    "\n",
    "```\n",
    "x1 → [RNN cell] → h1 → [RNN cell] → h2 → [RNN cell] → h3 → Output\n",
    "       ↑               ↑\n",
    "       |               |\n",
    "      feedback        feedback\n",
    "```\n",
    "\n",
    "![alt text](../images/rnn.png)\n",
    "---\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "At time step ( t ):\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "| Symbol       | Meaning                                            |\n",
    "| ------------ | -------------------------------------------------- |\n",
    "| $x_t$      | Input at time $t$ (e.g., word vector)            |\n",
    "| $h_t$      | Hidden state (memory) at time $t$               |\n",
    "| $y_t$      | Output at time $t$                               |\n",
    "| $W_x$      | Weight matrix from input → hidden                  |\n",
    "| $W_h$      | Weight matrix from hidden → hidden                 |\n",
    "| $W_y$      | Weight matrix from hidden → output                 |\n",
    "| $f$        | Activation function (usually `tanh` or `ReLU`)     |\n",
    "| $g$        | Output activation (usually `softmax` or `sigmoid`) |\n",
    "| $b_h, b_y$ | Bias terms                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Input Sequence:**\n",
    "   Feed one input at a time — for example, words in a sentence:\n",
    "\n",
    "   ```\n",
    "   x1 = “The”, x2 = “food”, x3 = “is”, x4 = “good”\n",
    "   ```\n",
    "\n",
    "2. **Hidden State Update:**\n",
    "   At each step, RNN updates the hidden state based on:\n",
    "\n",
    "   * The current input\n",
    "   * The previous hidden state\n",
    "\n",
    "   Example:\n",
    "\n",
    "   ```\n",
    "   h1 = f(Wx·x1 + Wh·h0 + b)\n",
    "   h2 = f(Wx·x2 + Wh·h1 + b)\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "3. **Output Generation:**\n",
    "   At the end, RNN produces an output (e.g., sentiment label 0 or 1).\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Sentiment Analysis\n",
    "\n",
    "For “The food is good”:\n",
    "\n",
    "| Time Step | Input  | Hidden State              | Output        |\n",
    "| --------- | ------ | ------------------------- | ------------- |\n",
    "| t=1       | “The”  | h₁ = f(Wx·x₁ + b)         | —             |\n",
    "| t=2       | “Food” | h₂ = f(Wx·x₂ + Wh·h₁ + b) | —             |\n",
    "| t=3       | “Good” | h₃ = f(Wx·x₃ + Wh·h₂ + b) | ŷ = positive |\n",
    "\n",
    "RNN uses **previous hidden states** to understand that “not good” = negative sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of RNN Architectures\n",
    "\n",
    "| Type             | Description                      | Use Case                        |\n",
    "| ---------------- | -------------------------------- | ------------------------------- |\n",
    "| **Many-to-One**  | Multiple inputs → One output     | Sentiment analysis              |\n",
    "| **One-to-Many**  | One input → Sequence output      | Text generation, music          |\n",
    "| **Many-to-Many** | Sequence input → Sequence output | Translation, speech recognition |\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "✅ Maintains **temporal (sequence) relationships**\n",
    "✅ Works well with **variable-length inputs**\n",
    "✅ Learns **context** through hidden state\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "❌ **Vanishing Gradient Problem:**\n",
    "Gradients shrink as they are propagated back through many time steps — the network “forgets” long-term dependencies.\n",
    "\n",
    "❌ **Exploding Gradient Problem:**\n",
    "Gradients grow too large, making training unstable.\n",
    "\n",
    "❌ **Slow Training:**\n",
    "Sequential processing prevents parallelization.\n",
    "\n",
    "---\n",
    "\n",
    "### Solutions to RNN Limitations\n",
    "\n",
    "| Variant                                | Improvement                                                          |\n",
    "| -------------------------------------- | -------------------------------------------------------------------- |\n",
    "| **LSTM (Long Short-Term Memory)**      | Uses gates to retain long-term memory                                |\n",
    "| **GRU (Gated Recurrent Unit)**         | Simplified LSTM, faster                                              |\n",
    "| **Bidirectional RNN**                  | Reads sequence forward and backward for better context               |\n",
    "| **Attention Mechanism / Transformers** | Allows model to focus on relevant words directly (used in GPT, BERT) |\n",
    "\n",
    "---\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "* Text generation (e.g., writing assistants)\n",
    "* Sentiment analysis\n",
    "* Speech recognition\n",
    "* Machine translation\n",
    "* Stock market prediction\n",
    "* Video frame prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Feature      | RNN                                      |\n",
    "| ------------ | ---------------------------------------- |\n",
    "| Input        | Sequential                               |\n",
    "| Memory       | Yes (via hidden state)                   |\n",
    "| Key Equation | ( h_t = f(W_x x_t + W_h h_{t-1} + b_h) ) |\n",
    "| Activation   | tanh / ReLU                              |\n",
    "| Limitation   | Vanishing gradient                       |\n",
    "| Upgrades     | LSTM, GRU, Attention                     |\n",
    "\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
