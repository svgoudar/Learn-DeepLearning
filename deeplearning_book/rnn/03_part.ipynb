{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968beef9",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Workflows\n",
    "\n",
    "RNNs model **sequential or time-dependent data** such as text, audio, and signals.\n",
    "They process one element of a sequence at a time and retain **context** using a **hidden state**.\n",
    "\n",
    "Goal example:\n",
    "Input = “The food is good” → Output = Sentiment = Positive.\n",
    "\n",
    "---\n",
    "\n",
    "### End-to-End Workflow\n",
    "\n",
    "#### Step 1 – Data Preparation\n",
    "\n",
    "1. **Collect** sequential data $text, audio, series).\n",
    "2. **Clean & tokenize** it into ordered elements $words, timesteps).\n",
    "3. **Vectorize / embed** each token into numeric form:\n",
    "\n",
    "   * One-Hot Encoding, TF-IDF, or pretrained Embeddings $Word2Vec, GloVe, etc.).\n",
    "4. **Pad / truncate** to equal sequence length if batched.\n",
    "5. **Split** into train / validation / test sets.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2 – Define Network Architecture\n",
    "\n",
    "1. **Input Layer:** shape = $sequence_length, input_dim).\n",
    "2. **RNN Layer's:**\n",
    "\n",
    "   * Simple RNN / LSTM / GRU cells.\n",
    "   * Each cell computes\n",
    "     $$\n",
    "     h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "     $$\n",
    "3. **Output Layer:**\n",
    "\n",
    "   * Dense + activation (sigmoid/softmax) for classification or regression.\n",
    "\n",
    "Optional:\n",
    "\n",
    "* Dropout for regularization.\n",
    "* Bidirectional RNN for context from both directions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3 – Forward Propagation (Computation Phase)\n",
    "\n",
    "For each time step $t$:\n",
    "\n",
    "1. **Receive** input $x_t$.\n",
    "2. **Combine** with previous hidden state $h_{t-1}$:\n",
    "   $$\n",
    "   h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "   $$\n",
    "3. **Generate** output:\n",
    "   $$\n",
    "   y_t = g(W_y h_t + b_y)\n",
    "   $$\n",
    "4. **Store** $h_t$ → used by the next time step.\n",
    "\n",
    "At the final step, output or loss is computed.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4 – Loss Computation\n",
    "\n",
    "Compute error between predictions and true labels:\n",
    "$$\n",
    "L = \\sum_{t=1}^{T} \\ell(y_t, \\hat{y_t})\n",
    "$$\n",
    "Common losses: cross-entropy (classification), MSE (regression).\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5 – Backward Propagation Through Time (BPTT)\n",
    "\n",
    "1. **Unroll** the RNN across all timesteps.\n",
    "2. Apply the **chain rule** backward through each step:\n",
    "\n",
    "   * Derivatives of loss w.r.t. weights $W_x, W_h, W_y$.\n",
    "   * Accumulate gradients across timesteps.\n",
    "3. **Handle gradient issues:**\n",
    "\n",
    "   * Clip exploding gradients.\n",
    "   * Use LSTM/GRU for vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 6 – Parameter Update\n",
    "\n",
    "Use an optimizer (SGD, Adam, RMSprop) to update weights:\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 7 – Iteration\n",
    "\n",
    "Repeat Steps 3 → 6 for many epochs until convergence (loss stabilizes or accuracy saturates).\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 8 – Evaluation & Inference\n",
    "\n",
    "1. Evaluate on validation/test data (accuracy, F1-score, perplexity).\n",
    "2. For inference, feed one input sequence and obtain predictions step-wise.\n",
    "\n",
    "---\n",
    "\n",
    "**Conceptual Flow Summary**\n",
    "\n",
    "| Phase         | Function                       | Details                             |\n",
    "| ------------- | ------------------------------ | ----------------------------------- |\n",
    "| Data Prep     | Convert sequence → numeric     | Tokenize, embed                     |\n",
    "| Forward Pass  | Compute predictions            | Hidden = f(Input + Previous Hidden) |\n",
    "| Loss          | Compare output to label        | Cross-entropy, MSE                  |\n",
    "| Backward Pass | Compute gradients through time | BPTT                                |\n",
    "| Update        | Adjust parameters              | SGD / Adam                          |\n",
    "| Evaluate      | Measure model performance      | Accuracy, loss                      |\n",
    "| Deploy        | Predict on new sequences       | Stepwise inference                  |\n",
    "\n",
    "---\n",
    "\n",
    "**Challenges and Mitigations**\n",
    "\n",
    "| Problem                   | Cause                                    | Solution                       |\n",
    "| ------------------------- | ---------------------------------------- | ------------------------------ |\n",
    "| Vanishing gradient        | Small derivatives through many timesteps | LSTM / GRU / gradient clipping |\n",
    "| Exploding gradient        | Large derivatives                        | Gradient clipping              |\n",
    "| Long-term dependency      | Limited memory of simple RNN             | Use LSTM / GRU                 |\n",
    "| Sequential training speed | Non-parallel nature                      | Truncated BPTT / transformers  |\n",
    "\n",
    "---\n",
    "\n",
    "**Compact Mathematical Recap**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_t &= f(W_x x_t + W_h h_{t-1} + b_h) \\\n",
    "y_t &= g(W_y h_t + b_y) \\\n",
    "L &= \\sum_t \\ell(y_t, \\hat{y_t})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x},;\n",
    "\\frac{\\partial L}{\\partial W_h},;\n",
    "\\frac{\\partial L}{\\partial W_y}\n",
    "\\Rightarrow \\text{updated via gradient descent.}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**In short**\n",
    "\n",
    "RNN workflow =\n",
    "**Data prep → Sequence input → Forward pass → Compute loss → Backprop through time → Update weights → Evaluate.**\n",
    "\n",
    "It is the same loop as a feedforward network, but **extended through time** using a **shared hidden state** that carries past context."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
