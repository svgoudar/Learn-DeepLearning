{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138b7edd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Variants\n",
    "\n",
    "RNNs process **sequential data** by maintaining a **hidden state** (memory) that carries information across time steps.\n",
    "Different types of RNNs exist to handle different kinds of **sequence relationships** — one-to-one, one-to-many, many-to-one, and many-to-many.\n",
    "\n",
    "---\n",
    "\n",
    "### Major Types of RNN Architectures\n",
    "\n",
    "| **Type**                        | **Input–Output Relationship**    | **Example Use Case**                    |\n",
    "| ------------------------------- | -------------------------------- | --------------------------------------- |\n",
    "| **1️⃣ One-to-One (Vanilla NN)** | Single input → single output     | Image classification                    |\n",
    "| **2️⃣ One-to-Many**             | Single input → sequence output   | Image captioning                        |\n",
    "| **3️⃣ Many-to-One**             | Sequence input → single output   | Sentiment analysis                      |\n",
    "| **4️⃣ Many-to-Many**            | Sequence input → sequence output | Machine translation, speech recognition |\n",
    "\n",
    "---\n",
    "\n",
    "### Structural Types of RNNs\n",
    "\n",
    "#### **A. Simple (Vanilla) RNN**\n",
    "\n",
    "**Definition:**\n",
    "The basic recurrent network where each output depends on the current input and the previous hidden state.\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "**Use:** Short-term sequential patterns.\n",
    "**Limitation:** Suffers from *vanishing/exploding gradients* for long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Long Short-Term Memory (LSTM)**\n",
    "\n",
    "**Definition:**\n",
    "An improved RNN designed to handle **long-term dependencies** using **gates** to control information flow.\n",
    "\n",
    "**Key Gates:**\n",
    "\n",
    "* **Forget gate:** Decides what to discard.\n",
    "* **Input gate:** Decides what new info to store.\n",
    "* **Output gate:** Decides what to output.\n",
    "\n",
    "**Equations (simplified):**\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f) &\\text{(forget gate)}\\\n",
    "i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i) &\\text{(input gate)}\\\n",
    "\\tilde{C}*t &= \\tanh(W_c [h*{t-1}, x_t] + b_c) &\\text{(candidate)}\\\n",
    "C_t &= f_t * C_{t-1} + i_t * \\tilde{C}*t &\\text{(cell state)}\\\n",
    "o_t &= \\sigma(W_o [h*{t-1}, x_t] + b_o) &\\text{(output gate)}\\\n",
    "h_t &= o_t * \\tanh(C_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Use:** Long text, speech, and time-series tasks.\n",
    "**Example:** Chatbots, stock prediction.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### C. Gated Recurrent Unit (GRU)\n",
    "\n",
    "**Definition:**\n",
    "A simplified version of LSTM with fewer gates — combines forget and input gates into a single **update gate**.\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_t &= \\sigma(W_z [h_{t-1}, x_t]) &\\text{(update gate)}\\\n",
    "r_t &= \\sigma(W_r [h_{t-1}, x_t]) &\\text{(reset gate)}\\\n",
    "\\tilde{h}*t &= \\tanh(W_h [r_t * h*{t-1}, x_t])\\\n",
    "h_t &= (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Faster training than LSTM.\n",
    "* Similar performance.\n",
    "  **Use:** Any sequential data where efficiency is key.\n",
    "\n",
    "---\n",
    "\n",
    "#### **D. Bidirectional RNN (BiRNN)**\n",
    "\n",
    "**Definition:**\n",
    "Processes sequence **in both directions** (forward and backward) to capture **past and future context** simultaneously.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "$$\n",
    "h_t = [\\overrightarrow{h_t}; \\overleftarrow{h_t}]\n",
    "$$\n",
    "\n",
    "**Use:**\n",
    "\n",
    "* Speech recognition\n",
    "* Text classification\n",
    "* Named Entity Recognition\n",
    "\n",
    "**Limitation:**\n",
    "Cannot be used for real-time prediction (needs full sequence).\n",
    "\n",
    "---\n",
    "\n",
    "#### **E. Deep (Stacked) RNN**\n",
    "\n",
    "**Definition:**\n",
    "Multiple RNN layers stacked vertically, allowing hierarchical feature extraction.\n",
    "\n",
    "**Mechanism:**\n",
    "The hidden state from one layer becomes the input to the next:\n",
    "\n",
    "$$\n",
    "h_t^{(l)} = f(W^{(l)}_x h_t^{(l-1)} + W^{(l)}*h h*{t-1}^{(l)} + b^{(l)})\n",
    "$$\n",
    "\n",
    "**Use:** Complex language or sequence modeling (deep context understanding).\n",
    "\n",
    "---\n",
    "\n",
    "#### **F. Echo State Networks (ESN)**\n",
    "\n",
    "**Definition:**\n",
    "A special RNN where only the **output weights** are trained; internal weights are fixed random values forming a **reservoir**.\n",
    "Efficient for certain time-series problems.\n",
    "\n",
    "**Use:** Signal processing, dynamical system modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### **G. Attention-based RNN (Seq2Seq + Attention)**\n",
    "\n",
    "**Definition:**\n",
    "Enhances many-to-many RNNs by learning **which parts of the input sequence to focus on** for each output step.\n",
    "\n",
    "**Use:**\n",
    "Machine translation, text summarization, question answering.\n",
    "\n",
    "**Note:**\n",
    "The concept of attention led directly to **Transformers**, which replaced RNNs in modern NLP.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Type              | Handles Long-Term Context | Parallelizable | Speed    | Key Use Case                   |\n",
    "| ----------------- | ------------------------- | -------------- | -------- | ------------------------------ |\n",
    "| Simple RNN        | ❌                         | ❌              | Fast     | Short text, time series        |\n",
    "| LSTM              | ✅                         | ❌              | Moderate | Language, speech               |\n",
    "| GRU               | ✅                         | ❌              | Faster   | Text, stock data               |\n",
    "| Bidirectional RNN | ✅                         | ❌              | Slow     | Sentiment, NER                 |\n",
    "| Deep RNN          | ✅                         | ❌              | Slow     | Hierarchical sequence features |\n",
    "| Attention RNN     | ✅✅                        | ⚠️             | Moderate | Translation, summarization     |\n",
    "\n",
    "---\n",
    "\n",
    "**In Short**\n",
    "\n",
    "RNN types evolve to address **memory, speed, and dependency length** issues:\n",
    "\n",
    "* **Simple RNN:** Basic memory.\n",
    "* **LSTM:** Adds gates for long-term memory.\n",
    "* **GRU:** Simplified LSTM, faster.\n",
    "* **BiRNN:** Looks both ways in the sequence.\n",
    "* **Deep/Stacked RNN:** Adds multiple layers.\n",
    "* **Attention RNN:** Learns to focus selectively.\n",
    "\n",
    "\n",
    "| **Feature**              | **Vanilla RNN**                              | **LSTM (Long Short-Term Memory)**                    | **GRU (Gated Recurrent Unit)**                     | **BiRNN / BiLSTM / BiGRU**                       | **Attention-based RNN**                            | **Transformer**                             |\n",
    "| ------------------------ | -------------------------------------------- | ---------------------------------------------------- | -------------------------------------------------- | ------------------------------------------------ | -------------------------------------------------- | ------------------------------------------- |\n",
    "| **Core Idea**            | Single hidden state passes info through time | Adds **cell state** + **gates** for long-term memory | Combines forget & input gates into **update gate** | Processes sequence **both forward and backward** | Adds **attention** to focus on relevant past steps | Removes recurrence, uses **self-attention** |\n",
    "| **Memory Type**          | Short-term only                              | Long + short-term via cell                           | Long + short-term (simplified)                     | Both directions of sequence                      | Long-term via attention weights                    | Global context via attention                |\n",
    "| **Number of Gates**      | None                                         | 3 (input, forget, output)                            | 2 (update, reset)                                  | Same as chosen cell type                         | Same as chosen cell type                           | None (uses attention heads)                 |\n",
    "| **Gradient Stability**   | Poor (vanishing gradient)                    | Stable                                               | Stable (slightly less than LSTM)                   | Stable                                           | Stable (improved by attention)                     | Stable (no recurrence)                      |\n",
    "| **Computation Speed**    | Fast (few parameters)                        | Slowest (4 weight sets per step)                     | Faster than LSTM                                   | Slower (two passes)                              | Slower (extra attention computations)              | Fast (parallelizable)                       |\n",
    "| **Model Complexity**     | Simple                                       | High                                                 | Moderate                                           | High (double direction)                          | High                                               | Very High                                   |\n",
    "| **Training Parallelism** | Low (sequential)                             | Low                                                  | Low                                                | Low                                              | Low                                                | High (full parallel)                        |\n",
    "| **Best For**             | Short sequences                              | Long sequences                                       | Mid-to-long sequences                              | Context-rich tasks (e.g. translation, speech)    | Context-sensitive sequential data                  | NLP, vision, time series (modern standard)  |\n",
    "| **Memory Control**       | None                                         | Explicit (via gates)                                 | Simplified (fewer gates)                           | Depends on direction and gating                  | Gating + attention weights                         | Attention weights only                      |\n",
    "| **Parameter Count**      | Lowest                                       | Highest                                              | Lower than LSTM                                    | Double due to both directions                    | Depends on architecture                            | Very high (multi-heads)                     |\n",
    "| **Interpretability**     | Low                                          | Moderate                                             | Moderate                                           | Moderate                                         | Better (visualizable attention)                    | Best (attention visualization)              |\n",
    "| **Applications**         | Basic sequence prediction                    | Text, speech, translation                            | Similar to LSTM but faster                         | NLP, speech recognition                          | Sequence tasks needing focus                       | NLP, CV, time-series forecasting            |\n",
    "\n",
    "\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
