{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55142a9",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## **Attention-Based RNN**\n",
    "\n",
    "\n",
    "An **Attention-Based RNN** extends the traditional **encoderâ€“decoder RNN** architecture by allowing the decoder to **â€œattendâ€ to (focus on)** specific parts of the input sequence during output generation.\n",
    "\n",
    "In simple RNNs, the entire input sequence is compressed into a **single context vector**, which becomes a bottleneck for long or complex sequences.\n",
    "The **attention mechanism** removes that bottleneck by letting the model **dynamically select** relevant encoder states at each decoding step.\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Overview**\n",
    "\n",
    "#### ðŸ”¹ **Standard Encoderâ€“Decoder RNN**\n",
    "\n",
    "* **Encoder** processes all input tokens â†’ produces hidden states.\n",
    "* **Decoder** generates output tokens using the **final encoder state** only.\n",
    "\n",
    "\n",
    "![alt text](../../images/attentions_rnn.png)\n",
    "\n",
    "Problem: Information from earlier inputs fades away â€” long-term dependency loss.\n",
    "\n",
    "#### ðŸ”¹ **Attention-Based RNN**\n",
    "\n",
    "* Instead of relying only on the last hidden state, the decoder **looks at all encoder hidden states**.\n",
    "* It learns **weights (attention scores)** that indicate how important each encoder state is for producing the current output token.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "Think of reading a paragraph and answering a question.\n",
    "You donâ€™t remember the entire paragraph word-for-word.\n",
    "Instead, you **look back (â€œattendâ€)** to the parts that help answer the question.\n",
    "\n",
    "Similarly, in an attention-based RNN:\n",
    "\n",
    "* The decoder doesnâ€™t rely only on the final summary vector.\n",
    "* It **revisits** the encoderâ€™s hidden states and **weights** them dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Workflow**\n",
    "\n",
    "Let the input sequence be $X = [x_1, x_2, ..., x_T]$\n",
    "Encoder hidden states: $h_1, h_2, ..., h_T$\n",
    "Decoder hidden state at time step *t*: $s_t$\n",
    "\n",
    "### Step 1: **Compute Attention Scores**\n",
    "\n",
    "Compare each encoder state $h_i$ with current decoder state $s_{t-1}$:\n",
    "\n",
    "$$\n",
    "e_{t,i} = \\text{score}(s_{t-1}, h_i)\n",
    "$$\n",
    "\n",
    "Where â€œscoreâ€ can be:\n",
    "\n",
    "* **Dot product** â†’ $e_{t,i} = s_{t-1}^\\top h_i$\n",
    "* **Additive (Bahdanau)** â†’ $e_{t,i} = v_a^\\top \\tanh(W_s s_{t-1} + W_h h_i)$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: **Convert Scores to Weights**\n",
    "\n",
    "Use softmax to normalize:\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha_{t,i}$ = how much attention is given to encoder step *i* at decoder step *t*.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: **Compute Context Vector**\n",
    "\n",
    "Weighted sum of encoder hidden states:\n",
    "$$\n",
    "c_t = \\sum_i \\alpha_{t,i} h_i\n",
    "$$\n",
    "\n",
    "This **context vector** represents relevant parts of the input for the current output step.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: **Generate Decoder Output**\n",
    "\n",
    "Decoder uses both its current hidden state and the context vector:\n",
    "$$\n",
    "s_t = f(s_{t-1}, y_{t-1}, c_t)\n",
    "$$\n",
    "$$\n",
    "\\hat{y_t} = \\text{softmax}(W_o [s_t; c_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Workflow Summary**\n",
    "\n",
    "| Step | Component          | Description                                                        |\n",
    "| ---- | ------------------ | ------------------------------------------------------------------ |\n",
    "| 1    | **Encoder**        | Reads input sequence, produces hidden states $h_1, h_2, ... h_T$ |\n",
    "| 2    | **Attention**      | Computes similarity between decoder state and encoder states       |\n",
    "| 3    | **Weighting**      | Applies softmax to get attention distribution $\\alpha_t$         |\n",
    "| 4    | **Context Vector** | Weighted sum of encoder states (focus region)                      |\n",
    "| 5    | **Decoder**        | Combines context + previous state â†’ outputs next token             |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Use Case (Machine Translation)**\n",
    "\n",
    "* Input: â€œJe tâ€™aimeâ€\n",
    "* Encoder: Creates hidden states for each French word.\n",
    "* When generating â€œIâ€, attention focuses on â€œJeâ€.\n",
    "* When generating â€œloveâ€, attention shifts to â€œtâ€™aimeâ€.\n",
    "\n",
    "Each decoding step â€œlooks backâ€ differently.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Attention**\n",
    "\n",
    "| Type                       | Description                                                | Used In                |\n",
    "| -------------------------- | ---------------------------------------------------------- | ---------------------- |\n",
    "| **Additive (Bahdanau)**    | Uses small feedforward network to compute alignment scores | Early NMT models       |\n",
    "| **Multiplicative (Luong)** | Uses dot-product for scoring (faster)                      | Seq2Seq with attention |\n",
    "| **Self-Attention**         | Input attends to itself                                    | Transformer models     |\n",
    "| **Global Attention**       | Considers all encoder states                               | Machine translation    |\n",
    "| **Local Attention**        | Focuses on a window of encoder states                      | Speech processing      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "| Advantage                       | Description                                                              |\n",
    "| ------------------------------- | ------------------------------------------------------------------------ |\n",
    "| **Solves long-term dependency** | No single fixed-size context vector                                      |\n",
    "| **Interpretable**               | Visualize attention weights â€” shows which input tokens influenced output |\n",
    "| **Improves accuracy**           | Significant gains in translation, summarization, etc.                    |\n",
    "| **Adaptive focus**              | Decoder learns what to attend to dynamically                             |\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "| Disadvantage                   | Description                                        |\n",
    "| ------------------------------ | -------------------------------------------------- |\n",
    "| **Higher computation**         | Calculates attention weights for each decoder step |\n",
    "| **More memory**                | Must store all encoder states                      |\n",
    "| **Not parallelizable**         | Still sequential â€” unlike Transformers             |\n",
    "| **May overfit small datasets** | More parameters to learn                           |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use\n",
    "\n",
    "| Use Case                | Reason                         |\n",
    "| ----------------------- | ------------------------------ |\n",
    "| **Machine translation** | Aligns input and output tokens |\n",
    "| **Text summarization**  | Identifies key parts of input  |\n",
    "| **Image captioning**    | Focuses on relevant regions    |\n",
    "| **Speech recognition**  | Handles variable-length inputs |\n",
    "| **Question answering**  | Finds relevant text passages   |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Aspect             | Traditional RNN     | Attention-Based RNN        |\n",
    "| ------------------ | ------------------- | -------------------------- |\n",
    "| Context            | Single fixed vector | Dynamic context per output |\n",
    "| Long sequences     | Poor performance    | Handles easily             |\n",
    "| Interpretability   | None                | Attention heatmaps         |\n",
    "| Accuracy           | Moderate            | Higher                     |\n",
    "| Computational cost | Low                 | Higher                     |\n",
    "\n",
    "---\n",
    "\n",
    "**In short**\n",
    "\n",
    "> **Attention = Learn to look at the right parts of the input when producing each output.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b40b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
