{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c22862",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Backward & Forward Propogation with RNN context\n",
    "\n",
    "RNNs are used for **sequential data** — text, speech, time series, etc.\n",
    "\n",
    "They process inputs **one at a time**, keeping **memory** of previous steps through **hidden states**.\n",
    "\n",
    "The **training** of an RNN has two major parts:\n",
    "\n",
    "1. **Forward Propagation** → compute outputs step by step through time.\n",
    "2. **Backward Propagation Through Time (BPTT)** → compute gradients and update weights.\n",
    "\n",
    "---\n",
    "\n",
    "### Notation\n",
    "\n",
    "At time step $t$:\n",
    "\n",
    "| Symbol       | Description                                |\n",
    "| ------------ | ------------------------------------------ |\n",
    "| $x_t$      | Input vector                               |\n",
    "| $h_t$      | Hidden state (memory)                      |\n",
    "| $y_t$      | Output                                     |\n",
    "| $W_x$      | Input → hidden weights                     |\n",
    "| $W_h$      | Hidden → hidden weights                    |\n",
    "| $W_y$      | Hidden → output weights                    |\n",
    "| $b_h, b_y$ | Bias terms                                 |\n",
    "| $f$        | Activation function (usually tanh or ReLU) |\n",
    "| $g$        | Output activation (sigmoid or softmax)     |\n",
    "\n",
    "---\n",
    "\n",
    "###  Forward Propagation (Step-by-Step)\n",
    "\n",
    "#### Equation 1 — Hidden State Update\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "Each new hidden state $h_t$ depends on:\n",
    "\n",
    "* Current input $x_t$\n",
    "* Previous hidden state $h_{t-1}$\n",
    "\n",
    "This allows **temporal context**.\n",
    "\n",
    "---\n",
    "\n",
    "### Equation 2 — Output\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "This gives the model’s prediction at time step $t$.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Flow (Sequence Length = 3)\n",
    "\n",
    "```\n",
    "x1 → h1 → y1\n",
    "      ↑\n",
    "x2 → h2 → y2\n",
    "      ↑\n",
    "x3 → h3 → y3\n",
    "```\n",
    "\n",
    "**Operations:**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_1 &= f(W_x x_1 + W_h h_0 + b_h) \\\n",
    "h_2 &= f(W_x x_2 + W_h h_1 + b_h) \\\n",
    "h_3 &= f(W_x x_3 + W_h h_2 + b_h) \\\n",
    "y_3 &= g(W_y h_3 + b_y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Computation\n",
    "\n",
    "If there is a loss at each step:\n",
    "\n",
    "$$\n",
    "L = \\sum_{t=1}^T \\ell(y_t, \\hat{y_t})\n",
    "$$\n",
    "\n",
    "If only final output matters (e.g. classification):\n",
    "$$\n",
    "L = \\ell(y_T, \\hat{y_T})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Backward Propagation (Backpropagation Through Time — BPTT)\n",
    "\n",
    "Now, we update the parameters using gradient descent.\n",
    "\n",
    "Because $h_t$ depends on all previous hidden states, **we must propagate gradients backward through time**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1 — Unroll the RNN\n",
    "\n",
    "Unrolling means visualizing the recurrence as a deep feedforward network through time steps:\n",
    "\n",
    "```\n",
    "x1 → h1 → y1\n",
    "x2 → h2 → y2\n",
    "x3 → h3 → y3\n",
    "```\n",
    "\n",
    "Here, **the same weights (W_x, W_h, W_y)** are shared across all time steps.\n",
    "\n",
    "---\n",
    "\n",
    "### Compute Gradients Backward\n",
    "\n",
    "We compute gradients in **reverse order (from last time step to first)**.\n",
    "\n",
    "At time $t$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x}, \\quad\n",
    "\\frac{\\partial L}{\\partial W_h}, \\quad\n",
    "\\frac{\\partial L}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "We use the **chain rule** because each hidden state depends on the previous one.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3 — Chain Rule Dependency\n",
    "\n",
    "For the output layer:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_t}{\\partial W_y} = \\frac{\\partial L_t}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "For the hidden state:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_t}{\\partial h_t} =\n",
    "\\frac{\\partial L_t}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial h_t}\n",
    "\n",
    "* \\frac{\\partial L_{t+1}}{\\partial h_{t+1}} \\cdot \\frac{\\partial h_{t+1}}{\\partial h_t}\n",
    "$$\n",
    "\n",
    "That second term accounts for **future dependencies** — the core reason it’s called *backpropagation through time.*\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4 — Weight Updates\n",
    "\n",
    "After computing the gradients, parameters are updated via gradient descent:\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Intuition of Gradient Flow\n",
    "\n",
    "Unrolling an RNN across $T$ steps gives this recursive gradient structure:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_h}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = W_h^T f'(a_{t-1})\n",
    "$$\n",
    "\n",
    "So gradient depends on repeated multiplication of $W_h$ and derivative $f'$.\n",
    "\n",
    "If $||W_h|| < 1$ repeatedly, gradients **vanish** → network “forgets” long-term dependencies.\n",
    "If $||W_h|| > 1$, gradients **explode** → unstable training.\n",
    "\n",
    "---\n",
    "\n",
    "### Vanishing and Exploding Gradient Problem\n",
    "\n",
    "RNNs suffer from these because:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_t} \\propto \\prod_{k=t}^{T} W_h^T f'(a_k)\n",
    "$$\n",
    "\n",
    "When $T$ (sequence length) is large, this product can:\n",
    "\n",
    "* Shrink to zero (vanishing gradients)\n",
    "* Blow up to infinity (exploding gradients)\n",
    "\n",
    "This is why **LSTM** and **GRU** were developed — to regulate how much past information is retained or forgotten.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Key Equations\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_t &= f(W_x x_t + W_h h_{t-1} + b_h) \\\n",
    "y_t &= g(W_y h_t + b_y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_y} &= \\sum_t \\frac{\\partial L_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial W_y} \\\n",
    "\\frac{\\partial L}{\\partial W_x} &= \\sum_t \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_x} \\\n",
    "\\frac{\\partial L}{\\partial W_h} &= \\sum_t \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial W_h}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Intuitive Summary**\n",
    "\n",
    "| Step                 | Purpose                                   | Concept                             |\n",
    "| -------------------- | ----------------------------------------- | ----------------------------------- |\n",
    "| Forward pass         | Compute predictions                       | Combine past + current info         |\n",
    "| Compute loss         | Measure error                             | $L = \\sum_t \\ell(y_t, \\hat{y_t})$ |\n",
    "| Backward pass (BPTT) | Propagate gradients backward through time | Adjust weights for all time steps   |\n",
    "| Weight update        | Optimize via gradient descent             | Learn temporal patterns             |\n",
    "\n",
    "---\n",
    "\n",
    "**In Short**\n",
    "\n",
    "* **Forward propagation:** moves information forward through time.\n",
    "  It updates hidden states using current input + previous state.\n",
    "\n",
    "* **Backward propagation:** moves gradients backward through time.\n",
    "  It updates weights by linking all time steps via the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc63d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
