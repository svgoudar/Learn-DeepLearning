{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04edb247",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Intiution\n",
    "\n",
    "A **Feedforward Neural Network (FNN)** models a **function ( f(x; \\theta) )** that maps an input vector ( x \\in \\mathbb{R}^n ) to an output ( y \\in \\mathbb{R}^m ).\n",
    "The parameters ( \\theta = {W, b} ) (weights and biases) are **learned** such that:\n",
    "$$\n",
    "y \\approx f(x; \\theta)\n",
    "$$\n",
    "\n",
    "Itâ€™s called â€œfeedforwardâ€ because information flows **one way**:\n",
    "$$\n",
    "x \\rightarrow \\text{hidden layers} \\rightarrow \\hat{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Structure\n",
    "\n",
    "An FNN is composed of **layers of neurons**, where each layer performs a simple operation:\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "$$\n",
    "a^{(l)} = f(z^{(l)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $l$ = layer index\n",
    "* $a^{(0)} = x$ (input)\n",
    "* $W^{(l)}$: weight matrix for layer (l)\n",
    "* $b^{(l)}$: bias vector\n",
    "* $f(\\cdot)$: activation function (non-linear)\n",
    "\n",
    "The final layer output $a^{(L)} = \\hat{y}$ is the modelâ€™s **prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Theoretical Intuition\n",
    "\n",
    "Letâ€™s break the FNN into **three conceptual blocks**:\n",
    "\n",
    "---\n",
    "\n",
    "#### Linear Transformation (Feature Extraction)\n",
    "\n",
    "Each neuron computes a **weighted sum**:\n",
    "$$\n",
    "z_i = \\sum_{j} w_{ij}x_j + b_i\n",
    "$$\n",
    "\n",
    "This is a **linear transformation**:\n",
    "It projects the input vector into another space, stretching or rotating it.\n",
    "\n",
    "In matrix form:\n",
    "$$\n",
    "z = W x + b\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Interpretation**:\n",
    "Each layer re-represents the data in a new coordinate system, learning directions (features) that best explain the dataâ€™s patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### Non-Linearity (Feature Composition)\n",
    "\n",
    "The activation function ( f(z) ) (e.g. ReLU, Sigmoid, Tanh) adds **non-linearity**:\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "Without this step, stacking multiple layers collapses into one linear map:\n",
    "$$\n",
    "W_3(W_2(W_1x)) = W'x\n",
    "$$\n",
    "â†’ still linear.\n",
    "With non-linearity, the network can learn **curved, complex functions**.\n",
    "\n",
    "ðŸ”¹ **Interpretation**:\n",
    "Non-linearity allows each layer to â€œbendâ€ the input space â€” this lets the network model complex relationships like curved boundaries or hierarchies (edges â†’ shapes â†’ objects).\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer Composition (Hierarchy of Features)\n",
    "\n",
    "Each successive layer composes previous transformations:\n",
    "\n",
    "$$\n",
    "f(x) = f^{(L)}(W^{(L)} f^{(L-1)}(W^{(L-1)} \\dots f^{(1)}(W^{(1)}x + b^{(1)}) + b^{(2)} \\dots ))\n",
    "$$\n",
    "\n",
    "This **composition of non-linear functions** allows the network to approximate **any continuous function** â€” proven by the **Universal Approximation Theorem**.\n",
    "\n",
    "ðŸ”¹ **Interpretation**:\n",
    "Each layer learns progressively abstract representations:\n",
    "\n",
    "* Layer 1 â†’ basic features (edges, correlations)\n",
    "* Layer 2 â†’ combinations (shapes, phrases)\n",
    "* Layer 3 â†’ abstract patterns (objects, meaning)\n",
    "\n",
    "---\n",
    "\n",
    "### Learning â€” Backpropagation + Gradient Descent\n",
    "\n",
    "The network learns parameters $W, b$ that minimize **loss** between prediction $\\hat{y}$ and true output $y$.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$$\n",
    "L = \\text{Loss}(y, \\hat{y})\n",
    "$$\n",
    "Common examples:\n",
    "\n",
    "* Mean Squared Error: $L = \\frac{1}{2}(y - \\hat{y})^2$\n",
    "* Cross-Entropy: $L = -\\sum y_i \\log(\\hat{y_i})$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "We minimize (L) by adjusting weights in direction of steepest descent:\n",
    "$$\n",
    "W := W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "$$\n",
    "b := b - \\eta \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "where $\\eta$ = learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Backpropagation computes gradients efficiently using the **chain rule**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\cdot \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}\n",
    "$$\n",
    "Each layer passes error backward, adjusting weights to reduce loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Letâ€™s take one hidden layer for simplicity:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{(1)} &= W^{(1)}x + b^{(1)} \\\n",
    "a^{(1)} &= f(z^{(1)}) \\\n",
    "z^{(2)} &= W^{(2)}a^{(1)} + b^{(2)} \\\n",
    "\\hat{y} &= f(z^{(2)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Loss:\n",
    "$$\n",
    "L = (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Gradient for output layer:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(2)}} = -2(y - \\hat{y}) \\cdot f'(z^{(2)}) \\cdot a^{(1)}\n",
    "$$\n",
    "\n",
    "Gradient for hidden layer:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} = \\left[-2(y - \\hat{y})f'(z^{(2)})W^{(2)}f'(z^{(1)})\\right]x\n",
    "$$\n",
    "\n",
    "This shows how errors â€œflow backwardâ€ to earlier layers â€” the **core of backpropagation**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Theoretical Insights\n",
    "\n",
    "### Universal Approximation Theorem\n",
    "\n",
    "A neural network with **one hidden layer and sufficient neurons** can approximate **any continuous function** on compact input space.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{N} \\alpha_i \\sigma(w_i^T x + b_i)\n",
    "$$\n",
    "can approximate any $f(x)$ given enough neurons.\n",
    "\n",
    "â†’ Neural networks are **function approximators**.\n",
    "\n",
    "---\n",
    "\n",
    "### Role of Depth\n",
    "\n",
    "Deeper networks can represent functions **exponentially more efficiently** (fewer neurons for same accuracy).\n",
    "Depth â†’ hierarchical abstraction (like brain cortex layers).\n",
    "\n",
    "---\n",
    "\n",
    "### Probabilistic View\n",
    "\n",
    "Feedforward networks can also be seen as **parameterized conditional models**:\n",
    "$$\n",
    "p(y|x; \\theta)\n",
    "$$\n",
    "They model the probability of output given input, making them suitable for classification and regression.\n",
    "\n",
    "---\n",
    "\n",
    "#### Geometric View\n",
    "\n",
    "Each layer **warps input space**:\n",
    "\n",
    "* Linear layers rotate/stretch space.\n",
    "* Nonlinear activations bend it.\n",
    "* The final decision boundary becomes a complex surface.\n",
    "\n",
    "Thus, neural networks **reshape feature space** until classes become linearly separable.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition Recap\n",
    "\n",
    "| Stage             | M`athematical Form                   | Theoretical Meaning           |\n",
    "| ----------------- | ----------------------------------- | ----------------------------- |\n",
    "| Weighted sum      | $z = Wx + b$                        | Linear projection             |\n",
    "| Activation        | $a = f(z)$                          | Nonlinear deformation         |\n",
    "| Composition       | $f^{(L)} \\circ \\dots \\circ f^{(1)}$ | Hierarchical feature learning |\n",
    "| Loss minimization | $\\min L(y, \\hat{y})$                | Learn function mapping        |\n",
    "| Backpropagation   | Chain rule on gradients             | Efficient optimization        |\n",
    "`\n",
    "---\n",
    "\n",
    "### Example Intuitive Analogy\n",
    "\n",
    "Think of a **Feedforward Neural Network** as:\n",
    "\n",
    "* Each layer is a **filter** refining the input.\n",
    "* Linear layers = weighted â€œmixingâ€ of features.\n",
    "* Activation = deciding which features to emphasize.\n",
    "* Backprop = continuous self-correction to improve the mapping.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Concept        | Description                               |\n",
    "| -------------- | ----------------------------------------- |\n",
    "| **Goal**       | Learn a function ( f: X \\to Y ) from data |\n",
    "| **Mechanism**  | Compose linear + nonlinear transforms     |\n",
    "| **Training**   | Backpropagation with gradient descent     |\n",
    "| **Power**      | Universal function approximator           |\n",
    "| **Limitation** | Needs lots of data and tuning             |\n",
    "\n",
    "---\n",
    "\n",
    "**In One Line**\n",
    "\n",
    "> A **Feedforward Neural Network** is a layered system that linearly transforms inputs, bends them through nonlinear activations, and learns parameters via backpropagation to approximate any target function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271008ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
