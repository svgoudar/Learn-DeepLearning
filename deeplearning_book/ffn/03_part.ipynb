{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83966184",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "# Training & Hyper-Parameter Tuning \n",
    "\n",
    "## Training\n",
    "\n",
    "Training an FNN means **learning optimal parameters (weights and biases)** so that the network can map input data (x) to correct outputs (y).\n",
    "\n",
    "The training follows a well-defined workflow:\n",
    "\n",
    "---\n",
    "\n",
    "### Initialize Parameters\n",
    "\n",
    "* Randomly initialize weights $W^{(l)}$ (small random numbers, not all zeros).\n",
    "* Initialize biases $b^{(l)} = 0$.\n",
    "\n",
    "  * Common initializations:\n",
    "\n",
    "    * **Xavier (Glorot)** for sigmoid/tanh\n",
    "    * **He initialization** for ReLU\n",
    "\n",
    "This helps avoid issues like **vanishing/exploding gradients**.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Each layer computes:\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "$$\n",
    "a^{(l)} = f(z^{(l)})\n",
    "$$\n",
    "\n",
    "The final layer output (a^{(L)} = \\hat{y}) is the model’s **prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### Compute Loss\n",
    "\n",
    "The **loss function** measures how far predictions are from true outputs:\n",
    "$$\n",
    "L = \\text{Loss}(y, \\hat{y})\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "\n",
    "* **Regression:** Mean Squared Error (MSE)\n",
    "  $$\n",
    "  L = \\frac{1}{m}\\sum (y - \\hat{y})^2\n",
    "  $$\n",
    "* **Classification:** Cross-Entropy Loss\n",
    "  $$\n",
    "  L = -\\frac{1}{m}\\sum y \\log(\\hat{y})\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation (Gradient Computation)\n",
    "\n",
    "Compute partial derivatives of loss (L) w.r.t. all parameters using **chain rule**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(l)}}, \\quad \\frac{\\partial L}{\\partial b^{(l)}}\n",
    "$$\n",
    "\n",
    "Error flows backward:\n",
    "\n",
    "* Output → hidden layers → input.\n",
    "* Each layer updates its parameters based on its contribution to total loss.\n",
    "\n",
    "This is known as **backpropagation**.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameter Update\n",
    "\n",
    "Weights and biases are updated using **gradient descent**:\n",
    "\n",
    "$$\n",
    "W^{(l)} := W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
    "$$\n",
    "$$\n",
    "b^{(l)} := b^{(l)} - \\eta \\frac{\\partial L}{\\partial b^{(l)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\eta$ = **learning rate** (step size)\n",
    "* Determines how big each parameter update is\n",
    "\n",
    "---\n",
    "\n",
    "### Iterate (Epochs)\n",
    "\n",
    "Repeat:\n",
    "\n",
    "* Forward pass\n",
    "* Loss computation\n",
    "* Backpropagation\n",
    "* Weight update\n",
    "\n",
    "for multiple **epochs** (full passes through training data).\n",
    "Training continues until:\n",
    "\n",
    "* Loss converges (stabilizes), or\n",
    "* Validation performance stops improving.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluate and Validate\n",
    "\n",
    "After each epoch:\n",
    "\n",
    "* Compute **validation loss** to monitor overfitting.\n",
    "* If validation loss increases while training loss decreases → model is overfitting.\n",
    "\n",
    "Use early stopping if necessary.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Intuition**\n",
    "\n",
    "| Step            | Purpose             |\n",
    "| --------------- | ------------------- |\n",
    "| Forward pass    | Predict output      |\n",
    "| Loss            | Measure error       |\n",
    "| Backpropagation | Compute gradients   |\n",
    "| Optimization    | Adjust parameters   |\n",
    "| Validation      | Prevent overfitting |\n",
    "\n",
    "---\n",
    "\n",
    "## Huperparameter Tuning Stratergies\n",
    "\n",
    "Hyperparameters are **external settings** (not learned) that control model behavior and training efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Hyperparameters in FNN\n",
    "\n",
    "| Category           | Hyperparameter          | Description                                 |\n",
    "| ------------------ | ----------------------- | ------------------------------------------- |\n",
    "| **Architecture**   | Number of hidden layers | Controls model depth                        |\n",
    "|                    | Neurons per layer       | Controls capacity                           |\n",
    "|                    | Activation functions    | Controls non-linearity                      |\n",
    "| **Training**       | Learning rate           | Step size for gradient descent              |\n",
    "|                    | Batch size              | Number of samples per gradient update       |\n",
    "|                    | Epochs                  | How long to train                           |\n",
    "|                    | Optimizer               | Update rule (SGD, Adam, RMSProp)            |\n",
    "| **Regularization** | Dropout rate            | Fraction of neurons dropped during training |\n",
    "|                    | L1/L2 penalty           | Adds weight constraints                     |\n",
    "| **Initialization** | Weight scheme           | (He, Xavier, Random uniform)                |\n",
    "| **Scheduler**      | Learning rate decay     | Dynamically adjust learning rate            |\n",
    "\n",
    "---\n",
    "\n",
    "### Tuning Strategies\n",
    "\n",
    "#### (a) Grid Search\n",
    "\n",
    "* Try all combinations of hyperparameters.\n",
    "* Example:\n",
    "\n",
    "  ```text\n",
    "  learning_rate = [0.01, 0.001]\n",
    "  hidden_layers = [2, 3, 4]\n",
    "  neurons = [64, 128, 256]\n",
    "  ```\n",
    "* Train model for each combo → pick one with best validation accuracy.\n",
    "* **Pros:** Exhaustive\n",
    "* **Cons:** Computationally expensive\n",
    "\n",
    "---\n",
    "\n",
    "#### (b) Random Search\n",
    "\n",
    "* Randomly select hyperparameter combinations.\n",
    "* Usually faster and often as effective as grid search.\n",
    "* **Good first step** for coarse tuning.\n",
    "\n",
    "---\n",
    "\n",
    "#### (c) Bayesian Optimization\n",
    "\n",
    "* Builds a probabilistic model of performance across hyperparameter space.\n",
    "* Chooses next set of hyperparameters based on previous results.\n",
    "* Efficient and smart search.\n",
    "* Tools: **Optuna**, **Hyperopt**, **Ray Tune**\n",
    "\n",
    "---\n",
    "\n",
    "#### (d) Learning Rate Scheduling\n",
    "\n",
    "* Start with higher learning rate and gradually reduce.\n",
    "* Schedulers:\n",
    "\n",
    "  * **Step Decay**\n",
    "  * **Exponential Decay**\n",
    "  * **ReduceLROnPlateau**\n",
    "* Keeps training stable and efficient.\n",
    "\n",
    "---\n",
    "\n",
    "#### (e) Early Stopping\n",
    "\n",
    "* Monitor validation loss.\n",
    "* Stop training when it stops improving.\n",
    "* Prevents overfitting and saves computation.\n",
    "\n",
    "---\n",
    "\n",
    "#### (f) Cross-Validation\n",
    "\n",
    "* Split data into ( k ) folds (e.g., 5-fold).\n",
    "* Train on ( k-1 ) folds, validate on 1.\n",
    "* Average performance across folds for stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Regularization Techniques\n",
    "\n",
    "To prevent overfitting:\n",
    "\n",
    "* **L2 regularization (weight decay):**\n",
    "  Adds penalty term to loss:\n",
    "  $$\n",
    "  L' = L + \\lambda \\sum W^2\n",
    "  $$\n",
    "* **Dropout:** Randomly deactivate neurons during training.\n",
    "* **Batch Normalization:** Normalizes activations between layers for stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Optimizer Tuning\n",
    "\n",
    "| Optimizer    | Description                       | Key Parameters             |\n",
    "| ------------ | --------------------------------- | -------------------------- |\n",
    "| **SGD**      | Basic gradient descent            | Learning rate              |\n",
    "| **Momentum** | Accelerates SGD                   | Momentum factor            |\n",
    "| **RMSProp**  | Adaptive learning rate per weight | Decay rate                 |\n",
    "| **Adam**     | Combines RMSProp + Momentum       | $\\eta, \\beta_1, \\beta_2$ |\n",
    "\n",
    "Usually, **Adam** is the best default choice.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Practical Workflow for Tuning\n",
    "\n",
    "1. Start with baseline FNN (simple structure).\n",
    "2. Tune learning rate → use learning rate finder.\n",
    "3. Tune hidden layers & neurons → increase until overfitting.\n",
    "4. Add regularization (dropout/L2).\n",
    "5. Try optimizers (Adam, RMSProp).\n",
    "6. Fine-tune batch size and epochs.\n",
    "7. Use validation metrics for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Workflow Summary\n",
    "\n",
    "| Phase                     | Action                            | Purpose                    |\n",
    "| ------------------------- | --------------------------------- | -------------------------- |\n",
    "| **Initialization**        | Choose model structure            | Define architecture        |\n",
    "| **Training**              | Forward + Backpropagation         | Fit data                   |\n",
    "| **Validation**            | Monitor loss/accuracy             | Detect overfitting         |\n",
    "| **Hyperparameter Search** | Adjust learning rate, depth, etc. | Improve performance        |\n",
    "| **Regularization**        | Apply dropout, L2                 | Increase generalization    |\n",
    "| **Final Evaluation**      | Test data                         | Assess real-world accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "✅ Start simple, scale up gradually\n",
    "✅ Use validation loss as your tuning signal\n",
    "✅ Use **learning rate warm-up & decay**\n",
    "✅ Apply dropout and normalization in deeper models\n",
    "✅ Automate search using Optuna or KerasTuner\n",
    "\n",
    "---\n",
    "\n",
    "### In One Line:\n",
    "\n",
    "> Training an FNN means optimizing its weights through forward and backward passes, while **hyperparameter tuning** is the process of adjusting external configurations — like learning rate, depth, and regularization — to achieve the best generalization performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f1e28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
