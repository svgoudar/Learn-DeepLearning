{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bee5d8",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# FeedForward Neural Network\n",
    "\n",
    "\n",
    "A **Feedforward Neural Network (FNN)** — also called a **Multilayer Perceptron (MLP)** — is the most fundamental form of artificial neural network.\n",
    "It forms the **basis of Deep Learning**, where data flows **strictly in one direction**, from **input → hidden layers → output**, without any feedback loops.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Concept Overview**\n",
    "\n",
    "Feedforward networks model a mathematical function that maps input features to outputs through a series of weighted transformations and nonlinear activations.\n",
    "\n",
    "### **Flow**\n",
    "\n",
    "```\n",
    "Input Layer → Hidden Layers → Output Layer\n",
    "```\n",
    "\n",
    "Each layer passes information **forward only**.\n",
    "No cycles or memory of past data (unlike RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71ea36",
   "metadata": {},
   "source": [
    "![alt text](../images/fnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35da749",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Components**\n",
    "\n",
    "| Component                   | Description                                                                                   |\n",
    "| --------------------------- | --------------------------------------------------------------------------------------------- |\n",
    "| **Input layer**             | Receives the input data (e.g., pixels, features). No computation; just passes data forward.   |\n",
    "| **Weights (W)**             | Each connection between neurons has a weight that determines how much influence an input has. |\n",
    "| **Bias (b)**                | A constant added to help the model shift activation thresholds.                               |\n",
    "| **Hidden layers**           | Intermediate layers that transform data using weighted sums and activation functions.         |\n",
    "| **Activation function (f)** | Adds non-linearity to model complex relationships. Common: ReLU, Sigmoid, Tanh.               |\n",
    "| **Output layer**            | Produces final prediction (e.g., class probabilities, regression output).                     |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Mathematical Working**\n",
    "\n",
    "For one neuron:\n",
    "$$\n",
    "z = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n",
    "$$\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "For a layer:\n",
    "$$\n",
    "a^{(l)} = f(W^{(l)}a^{(l-1)} + b^{(l)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $a^{(l)}$: output (activations) of layer ( l )\n",
    "* $W^{(l)}$: weight matrix\n",
    "* $b^{(l)}$: bias vector\n",
    "* $f$: activation function\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Training Process**\n",
    "\n",
    "### **Step 1: Forward Propagation**\n",
    "\n",
    "* Input data is multiplied by weights and passed through activation functions.\n",
    "* Produces predicted output ( \\hat{y} ).\n",
    "\n",
    "### **Step 2: Compute Loss**\n",
    "\n",
    "* Measure difference between predicted and actual output.\n",
    "* Example:\n",
    "\n",
    "  * **Regression:** Mean Squared Error (MSE)\n",
    "  * **Classification:** Cross-Entropy Loss\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "### **Step 3: Backpropagation**\n",
    "\n",
    "* Compute gradients of loss with respect to each weight (using chain rule).\n",
    "* Tells how much each weight contributed to the error.\n",
    "\n",
    "### **Step 4: Weight Update**\n",
    "\n",
    "* Update each weight:\n",
    "  $$\n",
    "  W = W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "  $$\n",
    "  where $\\eta$ = learning rate (step size).\n",
    "\n",
    "This process repeats for many **epochs** until the loss converges.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Activation Functions**\n",
    "\n",
    "| Function       | Equation                                   | Range  | Use Case                                  |\n",
    "| -------------- | ------------------------------------------ | ------ | ----------------------------------------- |\n",
    "| **Sigmoid**    | $f(x)=\\frac{1}{1+e^{-x}}$                | (0,1)  | Binary classification                     |\n",
    "| **Tanh**       | $f(x)=\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1,1) | Normalized data                           |\n",
    "| **ReLU**       | $f(x)=\\max(0,x)$                         | [0,∞)  | Deep networks; avoids vanishing gradients |\n",
    "| **Leaky ReLU** | $f(x)=x$ if (x>0), else (0.01x)          | (-∞,∞) | Handles dead neurons                      |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Example Workflow**\n",
    "\n",
    "1. Input: features (e.g., 10 values)\n",
    "2. Hidden Layer 1: 8 neurons → activation ReLU\n",
    "3. Hidden Layer 2: 4 neurons → activation ReLU\n",
    "4. Output Layer: 1 neuron → activation Sigmoid (binary output)\n",
    "5. Loss: Binary Cross Entropy\n",
    "6. Optimizer: Gradient Descent or Adam\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Advantages**\n",
    "\n",
    "* Learns nonlinear relationships.\n",
    "* Works on both classification and regression.\n",
    "* Forms the foundation for deep architectures like CNNs, RNNs, Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Limitations**\n",
    "\n",
    "* Requires large data for good performance.\n",
    "* Training can be slow for deep models.\n",
    "* Sensitive to scaling and weight initialization.\n",
    "* No temporal or spatial awareness (unlike RNNs or CNNs).\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Example (Python with Keras)**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=4, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Feature           | Description                                        |\n",
    "| ----------------- | -------------------------------------------------- |\n",
    "| **Data flow**     | One direction (no feedback)                        |\n",
    "| **Learning rule** | Backpropagation + Gradient Descent                 |\n",
    "| **Best for**      | Structured/tabular data, basic pattern recognition |\n",
    "| **Limitation**    | No memory or temporal understanding                |\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a785d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
