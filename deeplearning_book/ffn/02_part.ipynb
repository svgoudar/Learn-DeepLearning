{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a29285f",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Workflow\n",
    "\n",
    "FNNs follow a **sequential pipeline**:\n",
    "$$\n",
    "\\text{Input Data} ;→; \\text{Forward Pass} ;→; \\text{Loss Computation} ;→; \\text{Backward Pass} ;→; \\text{Parameter Update}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 **1. Data Preparation**\n",
    "\n",
    "### Input Features and Labels\n",
    "\n",
    "You start with a dataset:\n",
    "$$\n",
    "{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(m)}, y^{(m)})}\n",
    "$$\n",
    "\n",
    "* $x^{(i)}$: input vector (e.g., pixel values, numeric features)\n",
    "* $y^{(i)}$: target output (e.g., class label or numeric value)\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "* Normalize or standardize input features\n",
    "* One-hot encode categorical labels (for classification)\n",
    "* Split into **train**, **validation**, and **test** sets\n",
    "\n",
    "---\n",
    "\n",
    "### Initialize the Model Parameters\n",
    "\n",
    "For each layer (l):\n",
    "\n",
    "* Randomly initialize **weights** ( W^{(l)} )\n",
    "* Initialize **biases** ( b^{(l)} ) to zeros or small constants\n",
    "\n",
    "Example:\n",
    "$$\n",
    "W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}, \\quad b^{(l)} \\in \\mathbb{R}^{n_l}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Goal: Compute the predicted output ( \\hat{y} ) for input ( x ).\n",
    "\n",
    "For each layer ( l = 1, 2, ..., L ):\n",
    "\n",
    "1. **Linear transformation:**\n",
    "   $$\n",
    "   z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "   $$\n",
    "   (for the first layer, ( a^{(0)} = x ))\n",
    "\n",
    "2. **Activation function:**\n",
    "   $$\n",
    "   a^{(l)} = f(z^{(l)})\n",
    "   $$\n",
    "\n",
    "After the final layer, ( a^{(L)} = \\hat{y} ) is the model’s prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Compute the Loss\n",
    "\n",
    "Compare prediction ( \\hat{y} ) and true output ( y ) using a **loss function**:\n",
    "$$\n",
    "L = \\text{Loss}(y, \\hat{y})\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "\n",
    "* **Mean Squared Error (MSE):**\n",
    "  $$\n",
    "  L = \\frac{1}{m} \\sum (y - \\hat{y})^2\n",
    "  $$\n",
    "* **Cross Entropy (for classification):**\n",
    "  $$\n",
    "  L = -\\frac{1}{m} \\sum y \\log(\\hat{y})\n",
    "  $$\n",
    "\n",
    "Loss gives a **numerical measure of error** — how far predictions are from ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation (Gradient Computation)\n",
    "\n",
    "Goal: Compute how much each parameter contributed to the loss.\n",
    "\n",
    "#### Using Chain Rule:\n",
    "\n",
    "For each layer ( l ):\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\cdot \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "Each layer computes:\n",
    "\n",
    "* Gradient of loss wrt weights: ( \\frac{\\partial L}{\\partial W^{(l)}} )\n",
    "* Gradient of loss wrt biases: ( \\frac{\\partial L}{\\partial b^{(l)}} )\n",
    "\n",
    "These gradients represent **how to adjust parameters** to reduce loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Weight Update (Optimization)\n",
    "\n",
    "Once gradients are computed, update weights and biases:\n",
    "\n",
    "$$\n",
    "W^{(l)} := W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
    "$$\n",
    "$$\n",
    "b^{(l)} := b^{(l)} - \\eta \\frac{\\partial L}{\\partial b^{(l)}}\n",
    "$$\n",
    "\n",
    "where ( \\eta ) = learning rate (step size).\n",
    "\n",
    "**Optimizers**:\n",
    "\n",
    "* **SGD** (Stochastic Gradient Descent)\n",
    "* **Adam** (adaptive momentum)\n",
    "* **RMSProp**, **Adagrad**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### Repeat for Multiple Epochs\n",
    "\n",
    "An **epoch** = one full pass through all training samples.\n",
    "\n",
    "Repeat:\n",
    "\n",
    "1. Forward pass\n",
    "2. Loss computation\n",
    "3. Backpropagation\n",
    "4. Parameter update\n",
    "\n",
    "until convergence (loss stops decreasing significantly).\n",
    "\n",
    "---\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "After training:\n",
    "\n",
    "* Test the model on unseen data\n",
    "* Use metrics like **accuracy**, **precision**, **recall**, **F1**, or **RMSE**\n",
    "\n",
    "Example for classification:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Model Tuning\n",
    "\n",
    "You can refine performance by:\n",
    "\n",
    "* Adjusting number of layers or neurons\n",
    "* Changing activation functions\n",
    "* Optimizing learning rate\n",
    "* Adding **regularization** (L2, dropout)\n",
    "* Early stopping to avoid overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **10. Deployment**\n",
    "\n",
    "Once the model performs well:\n",
    "\n",
    "* Save parameters (( W, b ))\n",
    "* Deploy as API or embedded model\n",
    "* Monitor predictions and retrain periodically with new data\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Full Workflow Summary Table\n",
    "\n",
    "| Step | Process                  | Input            | Output                      |\n",
    "| ---- | ------------------------ | ---------------- | --------------------------- |\n",
    "| 1    | Data preparation         | Raw data         | Normalized features, labels |\n",
    "| 2    | Parameter initialization | Input dimensions | Random $W, b$             |\n",
    "| 3    | Forward propagation      | $x$            | Predicted $\\hat{y}$       |\n",
    "| 4    | Compute loss             | $y, \\hat{y}$   | Error value                 |\n",
    "| 5    | Backpropagation          | Loss             | Gradients of $W, b$       |\n",
    "| 6    | Weight update            | Gradients        | New $W, b$                |\n",
    "| 7    | Training loop            | Dataset          | Trained model               |\n",
    "| 8    | Evaluation               | Test set         | Metrics                     |\n",
    "| 9    | Tuning                   | Model parameters | Improved accuracy           |\n",
    "| 10   | Deployment               | Final model      | Production-ready system     |\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Summary\n",
    "\n",
    "```\n",
    "         ┌────────────────────────────────────────────────────┐\n",
    "         │              FEEDFORWARD WORKFLOW                  │\n",
    "         ├────────────────────────────────────────────────────┤\n",
    "         │ 1. Data Preparation → Normalize, Split             │\n",
    "         │ 2. Initialize Weights and Biases                   │\n",
    "         │ 3. Forward Propagation (Input → Output)            │\n",
    "         │ 4. Compute Loss (Compare y, ŷ)                     │\n",
    "         │ 5. Backpropagation (Compute Gradients)             │\n",
    "         │ 6. Update Weights (Gradient Descent / Adam)        │\n",
    "         │ 7. Repeat for Multiple Epochs                      │\n",
    "         │ 8. Evaluate on Test Data                           │\n",
    "         │ 9. Tune Hyperparameters                            │\n",
    "         │10. Deploy and Monitor                              │\n",
    "         └────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Intuition Recap**\n",
    "\n",
    "| Concept             | Explanation                                         |\n",
    "| ------------------- | --------------------------------------------------- |\n",
    "| **Feedforward**     | Data flows one way (no loops)                       |\n",
    "| **Learning**        | Minimize error through gradient descent             |\n",
    "| **Backpropagation** | Adjusts weights efficiently                         |\n",
    "| **Activation**      | Adds non-linearity to learn complex patterns        |\n",
    "| **Iteration**       | Gradually improves mapping between input and output |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> The **Feedforward Neural Network workflow** is a cyclic process of forward computation, loss evaluation, backward gradient propagation, and parameter optimization — repeated over data until the network learns an accurate input-output mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309be1b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
