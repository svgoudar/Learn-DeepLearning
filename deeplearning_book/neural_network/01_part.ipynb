{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7f7cb9",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0babfb4",
   "metadata": {},
   "source": [
    "# Intuition \n",
    "\n",
    "Think of a neural network as a **function approximator**.\n",
    "\n",
    "* It learns to map **inputs** (x) to **outputs** (y).\n",
    "* It’s inspired by the brain: neurons receive signals, process them, and send outputs to other neurons.\n",
    "\n",
    "**Example – House Price Prediction**:\n",
    "\n",
    "* Input: house size, number of bedrooms, zip code, neighborhood wealth\n",
    "* Output: predicted house price\n",
    "* A neuron can combine these inputs in some way and output an intermediate concept (e.g., “family size” or “neighborhood quality”)\n",
    "* Multiple neurons in **hidden layers** can combine simple concepts to learn **complex relationships**\n",
    "\n",
    "**Key idea:** Each neuron is like a **mini-function**. Stacking many neurons allows the network to represent highly complicated functions.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Intuition\n",
    "\n",
    "###  Neuron as a Function\n",
    "\n",
    "A single neuron computes:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_i$ = input features\n",
    "* $w_i$ = weights (importance of each input)\n",
    "* $b$ = bias (shifts the output)\n",
    "* $f$ = activation function (introduces non-linearity)\n",
    "* $a$ = output of neuron\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* The neuron **linearly combines inputs** $(w_i x_i)$, then **non-linearly transforms** the result with $f$ to create richer patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Layers as Function Composition\n",
    "\n",
    "A network with **two layers**:\n",
    "\n",
    "$$\n",
    "a^{(1)} = f_1(W^{(1)} x + b^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = f_2(W^{(2)} a^{(1)} + b^{(2)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $W^{(1)}, W^{(2)}$ = weight matrices for layers 1 and 2\n",
    "* $b^{(1)}, b^{(2)}$ = bias vectors\n",
    "* $f_1, f_2$ = activation functions\n",
    "* $a^{(1)}$ = output of first (hidden) layer\n",
    "* $y$ = final output\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* Each layer transforms the input space into a new representation.\n",
    "* Stacking layers allows the network to **learn hierarchical features** (e.g., pixels → edges → shapes → objects in images).\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Weights (Training)\n",
    "\n",
    "1. **Forward Propagation:** Compute predicted output $\\hat{y}$ from inputs.\n",
    "2. **Compute Loss:** Measure error between predicted $\\hat{y}$ and true output $y$:\n",
    "   $$\n",
    "   \\text{Loss} = L(\\hat{y}, y)\n",
    "   $$\n",
    "\n",
    "* Example: MSE for regression: $$L = \\frac{1}{N}\\sum (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "3. **Backpropagation:** Compute gradients of loss w.r.t weights:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w_i}\n",
    "   $$\n",
    "\n",
    "4. **Update Weights (Gradient Descent):**\n",
    "   $$\n",
    "   w_i \\leftarrow w_i - \\eta \\frac{\\partial L}{\\partial w_i}\n",
    "   $$\n",
    "\n",
    "* $\\eta$ = learning rate\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* The network **learns by adjusting weights** to reduce the difference between predicted and true outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Non-Linearity is Key\n",
    "\n",
    "* Without activation functions, multiple layers collapse into a **single linear transformation**.\n",
    "* Non-linear activations (ReLU, sigmoid, tanh) allow the network to approximate **any continuous function**.\n",
    "* **Mathematical insight:** A sufficiently large neural network with non-linear activations is a **universal function approximator**.\n",
    "\n",
    "---\n",
    "\n",
    "### Geometric Intuition\n",
    "\n",
    "* Each neuron can be seen as defining a **hyperplane** in input space.\n",
    "* The activation determines **which side of the hyperplane is “activated”**.\n",
    "* Combining many neurons creates complex **decision boundaries** for classification or mapping functions for regression.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Intuition\n",
    "\n",
    "| Aspect        | Intuition                                            | Math                                                |\n",
    "| ------------- | ---------------------------------------------------- | --------------------------------------------------- |\n",
    "| Neuron        | Mini-function combining inputs                       | (a = f(\\sum w_i x_i + b))                           |\n",
    "| Layer         | Transform features into higher-level representations | (a^{(l)} = f(W^{(l)} a^{(l-1)} + b^{(l)}))          |\n",
    "| Network       | Function approximator mapping x → y                  | Composition of layers                               |\n",
    "| Learning      | Adjust weights to minimize error                     | Gradient descent/backprop                           |\n",
    "| Non-linearity | Capture complex patterns                             | Activation functions (ReLU, Sigmoid)                |\n",
    "| Depth         | Hierarchical feature learning                        | Multiple layers = hierarchical function composition |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. Neural networks are **composable functions**.\n",
    "2. Each neuron performs a **linear combination + non-linear activation**.\n",
    "3. Stacking neurons (layers) allows modeling **complex functions**.\n",
    "4. Training adjusts weights to **fit the data** using gradient-based optimization.\n",
    "5. Non-linearity is critical to avoid networks reducing to a simple linear function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
