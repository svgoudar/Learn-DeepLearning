{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3911f1f3",
   "metadata": {},
   "source": [
    "\n",
    "# Perceptron\n",
    "\n",
    "A **Perceptron** is the simplest form of a neural network and is also called a **single-layer neural network**. It is mainly used for **binary classification** problems.\n",
    "\n",
    "## Example Scenario\n",
    "\n",
    "You have a dataset with:\n",
    "\n",
    "* **Features**: IQ and Study Hours\n",
    "* **Output**: Pass or Fail (binary)\n",
    "\n",
    "The perceptron predicts whether a person will pass or fail based on these inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Architecture of a Perceptron\n",
    "\n",
    "### 1️⃣ Input Layer\n",
    "\n",
    "* Contains one node for each input feature.\n",
    "* Example:\n",
    "\n",
    "  * $x_1 = \\text{IQ}$\n",
    "  * $x_2 = \\text{Study Hours}$\n",
    "\n",
    "### 2️⃣ Hidden Layer (Single Neuron)\n",
    "\n",
    "Although called “hidden layer,” in a perceptron there is just **one neuron** performing computation.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Core Components\n",
    "\n",
    "### ✔ Weights (( w_1, w_2 ))\n",
    "\n",
    "* Each input is connected to the neuron via a weight.\n",
    "* Weights control the influence of each feature.\n",
    "\n",
    "### ✔ Bias (( b ))\n",
    "\n",
    "* A constant added to prevent the model from producing zero output.\n",
    "* Helps the neuron activate even when the weighted sum is zero.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Computation Steps\n",
    "\n",
    "### ✅ Step 1: Weighted Sum (Linear Combination)\n",
    "\n",
    "$$\n",
    "z = \\sum (w_i \\cdot x_i) + b = w_1x_1 + w_2x_2 + b\n",
    "$$\n",
    "\n",
    "This is similar to the equation of a linear model:\n",
    "$$\n",
    "y = mx + c \\quad \\text{or} \\quad \\beta_1x_1 + \\beta_2x_2 + b\n",
    "$$\n",
    "\n",
    "### ✅ Step 2: Activation Function\n",
    "\n",
    "The activation function transforms ( z ) into an output (0 or 1).\n",
    "\n",
    "Common activation functions:\n",
    "\n",
    "* **Step Function**\n",
    "\n",
    "  * If $z \\leq 0$ → 0\n",
    "  * If $z > 0$ → 1\n",
    "* **Sigmoid Function**\n",
    "\n",
    "  * Converts output to range (0,1)\n",
    "  * Uses 0.5 as threshold\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Error and Weight Update\n",
    "\n",
    "* The perceptron compares predicted output with actual output.\n",
    "* If wrong, it **updates weights** to reduce error.\n",
    "* This process continues until performance improves.\n",
    "\n",
    "Error = (Real Output – Predicted Output)\n",
    "\n",
    "This leads to **forward propagation and weight updates** (backward update).\n",
    "\n",
    "---\n",
    "\n",
    "Here’s a **clear and structured summary** of the content you provided about **Single-Layer Perceptron vs Multi-Layer Perceptron (MLP)**:\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptron Models Overview\n",
    "\n",
    "There are two main types of perceptron models:\n",
    "\n",
    "1. **Single-Layer Perceptron (SLP)**\n",
    "2. **Multi-Layer Perceptron (MLP)** – also called **Artificial Neural Network (ANN)** or **Multilayer Neural Network**\n",
    "\n",
    "The earlier discussion focused on the **Single-Layer Perceptron**.\n",
    "\n",
    "---\n",
    "\n",
    "## How Single-Layer Perceptron Works\n",
    "\n",
    "* Inputs are multiplied with weights.\n",
    "* A **bias** is added.\n",
    "* The result goes through an **activation function** (like step or sigmoid).\n",
    "* Output is binary: **0 or 1**.\n",
    "* If prediction is wrong, weights are **randomly updated** and the feedforward process repeats.\n",
    "\n",
    "This process is called **Feed Forward Neural Network** (left to right).\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Single-Layer Perceptron\n",
    "\n",
    "✔ Works well only for:\n",
    "\n",
    "* **Binary classification**\n",
    "* **Linearly separable data**\n",
    "\n",
    "Example:\n",
    "If data points can be separated by a straight line, SLP can classify them.\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ Limitations of Single-Layer Perceptron\n",
    "\n",
    "* Cannot solve **non-linearly separable problems**\n",
    "* Weight updates are inefficient (random changes)\n",
    "* No mechanism for **loss calculation** or **error minimization**\n",
    "* No backpropagation\n",
    "* Struggles with complex patterns\n",
    "\n",
    "Example of non-linear data: XOR problem (classes mixed such that a line cannot separate them)\n",
    "\n",
    "---\n",
    "\n",
    "## Why Move to Multi-Layer Perceptron (MLP)?\n",
    "\n",
    "MLP overcomes the limitations of SLP and can handle **complex and non-linear** problems.\n",
    "\n",
    "Key techniques used in MLP:\n",
    "\n",
    "### 1. Forward Propagation\n",
    "\n",
    "(Like in SLP – weighted sum + activation)\n",
    "\n",
    "### 2. Backward Propagation\n",
    "\n",
    "Used to update weights efficiently based on errors.\n",
    "\n",
    "### 3. Loss/Cost Functions\n",
    "\n",
    "Measure how far predictions are from actual results.\n",
    "\n",
    "### 4. Optimizers\n",
    "\n",
    "Algorithms (like SGD, Adam) to update weights systematically.\n",
    "\n",
    "### 5. Multiple Activation Functions\n",
    "\n",
    "Examples: ReLU, Sigmoid, Tanh\n",
    "\n",
    "These enable deep learning models to solve complex tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Transition to Deep Learning\n",
    "\n",
    "Using multilayer perceptrons, we can build **deep neural networks** that handle:\n",
    "\n",
    "* Nonlinear classification\n",
    "* Complex decision boundaries\n",
    "* Multiclass problems\n",
    "* Real-world applications (e.g., image, speech, NLP)\n",
    "\n",
    "\n",
    "\n",
    "**Key Insights**\n",
    "\n",
    "* Perceptron is a **linear classifier**, meaning it draws a **straight line** to separate two classes.\n",
    "* It works only if data is **linearly separable**.\n",
    "* Limitations in solving complex problems lead to **multilayer neural networks** (ANNs).\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
