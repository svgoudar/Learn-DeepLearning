{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302de9b1",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "A **neural network (NN)** is a computational model inspired by the human brain.\n",
    "\n",
    "* It consists of **nodes (neurons)** connected by **edges (weights)**.\n",
    "* Neural networks are used to **learn patterns** from data and make predictions or decisions.\n",
    "\n",
    "**Key idea:** A neural network approximates a function ( f(x) ) that maps inputs ( x ) to outputs ( y ).\n",
    "\n",
    "---\n",
    "\n",
    "## Structure of a Neural Network\n",
    "\n",
    "Neural networks are organized in **layers**:\n",
    "\n",
    "1. **Input Layer**\n",
    "\n",
    "   * Receives raw features from the dataset.\n",
    "   * Example: In house price prediction: size, bedrooms, zip code, neighborhood wealth.\n",
    "\n",
    "2. **Hidden Layers**\n",
    "\n",
    "   * Intermediate layers that process the inputs.\n",
    "   * Extract higher-level features and patterns.\n",
    "   * Can be **one or many layers** (deep networks = many hidden layers).\n",
    "\n",
    "3. **Output Layer**\n",
    "\n",
    "   * Produces the final result: a number (regression) or class (classification).\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "* Each layer contains **neurons**.\n",
    "* Neurons compute a weighted sum of inputs, apply an **activation function**, and pass the output to the next layer.\n",
    "\n",
    "![alt text](../images/single_neuron.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Neuron Function\n",
    "\n",
    "Each neuron performs:\n",
    "\n",
    "\n",
    "$$z = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "$$\n",
    "a = \\text{activation}(z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_i$ = input feature\n",
    "*  w_i$  = weight of that input\n",
    "* b$  = bias term\n",
    "* z  = weighted sum\n",
    "* a  = output after activation\n",
    "\n",
    "---\n",
    "\n",
    "## Activation Function\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing networks to model complex relationships. Common functions:\n",
    "\n",
    "| Function    | Formula                               | Use Case                                 |\n",
    "| ----------- | ------------------------------------- | ---------------------------------------- |\n",
    "| **ReLU**    | $\\max(0, x)$                        | Hidden layers, avoids vanishing gradient |\n",
    "| **Sigmoid** | $\\frac{1}{1 + e^{-x}}$              | Outputs probability (0–1)                |\n",
    "| **Tanh**    | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Outputs range -1 to 1                    |\n",
    "| **Softmax** | $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$    | Multi-class classification               |\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "* Process of computing outputs from inputs.\n",
    "* Each neuron calculates its weighted sum, applies activation, and passes it forward.\n",
    "* Output of one layer becomes input to the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "* Measures how far the predicted output is from the actual target.\n",
    "* Common loss functions:\n",
    "\n",
    "  * **Mean Squared Error (MSE):** Regression\n",
    "  * **Cross-Entropy Loss:** Classification\n",
    "\n",
    "**Goal:** Minimize the loss during training.\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "* Method to **update network weights** using **gradients** of the loss function.\n",
    "* Steps:\n",
    "\n",
    "  1. Compute loss\n",
    "  2. Calculate gradient of loss w.r.t each weight\n",
    "  3. Update weights in the **opposite direction of gradient**\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "* $\\eta$ = learning rate\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "Algorithms that adjust weights to minimize loss efficiently. Examples:\n",
    "\n",
    "* **SGD (Stochastic Gradient Descent)**\n",
    "* **Adam** (Adaptive Moment Estimation)\n",
    "* **RMSProp**\n",
    "\n",
    "---\n",
    "\n",
    "## Training Process\n",
    "\n",
    "1. Initialize weights randomly.\n",
    "2. Feed inputs through the network (**forward propagation**).\n",
    "3. Calculate loss.\n",
    "4. Propagate error backward (**backpropagation**).\n",
    "5. Update weights using optimizer.\n",
    "6. Repeat for many epochs until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "* **Overfitting:** Network memorizes training data but fails on new data.\n",
    "* **Regularization Techniques:** Dropout, L1/L2 regularization, early stopping.\n",
    "* **Deep Networks:** Many hidden layers can learn very complex patterns.\n",
    "* **Densely Connected Layers:** Every neuron in one layer connects to every neuron in the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of Neural Networks\n",
    "\n",
    "* Image recognition (CNNs)\n",
    "* Speech recognition (RNNs, LSTMs)\n",
    "* Natural language processing (Transformers, GPT models)\n",
    "* Autonomous vehicles\n",
    "* Recommendation systems\n",
    "\n",
    "---\n",
    "**Summary:**\n",
    "A neural network is a system of interconnected neurons that learns to map inputs to outputs. It uses **activation functions**, **forward propagation**, **loss functions**, and **backpropagation** to iteratively improve predictions. Deep networks with multiple hidden layers can model highly complex functions, which is the essence of deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5da18",
   "metadata": {},
   "source": [
    "## Artificial Neural Network (ANN)\n",
    "\n",
    "An ANN is a **computational model inspired by the human brain**.\n",
    "\n",
    "* It consists of **neurons (nodes)** arranged in layers.\n",
    "* These neurons are **connected by weights**, which adjust during learning.\n",
    "* ANNs are designed to **learn complex relationships** from data, both linear and nonlinear.\n",
    "\n",
    "**Analogy:**\n",
    "\n",
    "* Input features → sensory neurons in brain.\n",
    "* Hidden layers → processing neurons in brain that extract patterns.\n",
    "* Output → brain’s decision or response.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition \n",
    "\n",
    "The **core idea**:\n",
    "\n",
    "* A neural network **combines inputs in weighted ways** to compute an output.\n",
    "* The network **learns the best weights** to approximate a target function.\n",
    "\n",
    "**Example:** Predicting house prices.\n",
    "\n",
    "* Inputs: size, location, age.\n",
    "* Neurons combine these inputs in different ways (weighted sum + bias).\n",
    "* Activation function transforms them → captures nonlinear patterns (e.g., big house in a bad neighborhood might cost less).\n",
    "* Output: predicted price.\n",
    "\n",
    "**Key intuition points:**\n",
    "\n",
    "1. Each neuron is a **function approximator**.\n",
    "2. Multiple neurons → can model **complex functions**.\n",
    "3. Layers allow **hierarchical feature extraction**:\n",
    "\n",
    "   * Early layers → basic features (edges in images).\n",
    "   * Deeper layers → complex features (objects in images).\n",
    "\n",
    "---\n",
    "\n",
    "### Components of an ANN\n",
    "\n",
    "#### Neurons\n",
    "\n",
    "* Input: (x_1, x_2, \\dots, x_n)\n",
    "* Weights: (w_1, w_2, \\dots, w_n)\n",
    "* Bias: (b)\n",
    "* Output: (y = f(\\sum w_i x_i + b))\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Weights = importance of each input.\n",
    "* Bias = shifts the decision boundary.\n",
    "* Activation function = introduces **non-linearity**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Layers**\n",
    "\n",
    "* **Input layer**: raw features.\n",
    "* **Hidden layers**: learn **intermediate patterns**.\n",
    "* **Output layer**: final prediction.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Without hidden layers → linear models.\n",
    "* Hidden layers → network can model **nonlinear relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Activation Functions**\n",
    "\n",
    "* Transform neuron output.\n",
    "* **Why needed?** Without them, multiple layers collapse into a single linear layer.\n",
    "\n",
    "Common functions:\n",
    "\n",
    "1. **Sigmoid** → maps output to (0,1), used in probabilities.\n",
    "2. **Tanh** → maps output to (-1,1), zero-centered.\n",
    "3. **ReLU** → outputs max(0, x), solves vanishing gradient problem.\n",
    "4. **Softmax** → converts outputs into probabilities for multi-class classification.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Activation decides **which neurons “fire”**.\n",
    "* Like brain neurons: only active neurons contribute.\n",
    "\n",
    "---\n",
    "\n",
    "### **Forward Propagation**\n",
    "\n",
    "* **Goal:** compute output from inputs.\n",
    "* Each neuron → weighted sum → activation → next layer.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Signals flow like **electrical signals in brain**.\n",
    "* Each layer extracts increasingly complex features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Loss Function**\n",
    "\n",
    "* Measures **error between predicted and true output**.\n",
    "* Examples:\n",
    "\n",
    "  * Regression → MSE\n",
    "  * Classification → Cross-Entropy\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Loss tells the network **how wrong it is**.\n",
    "* Guides learning through gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **Backpropagation**\n",
    "\n",
    "* Computes **gradients of loss w.r.t weights** using chain rule.\n",
    "* Updates weights to **reduce loss** (learning).\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Like adjusting knobs to minimize error.\n",
    "* Deeper layers receive **gradient feedback** to improve pattern detection.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizers**\n",
    "\n",
    "* Algorithms to adjust weights efficiently.\n",
    "* Examples: SGD, Momentum, RMSProp, Adam.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Optimizer = **strategy to climb down the error hill** toward minimum.\n",
    "* Adam → combines momentum (smoother path) + adaptive learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### **Regularization**\n",
    "\n",
    "* Prevents overfitting.\n",
    "* Techniques: dropout, L1/L2 penalties, early stopping.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Forces network to **generalize**, not memorize training data.\n",
    "* Dropout = randomly deactivate neurons → network learns **robust features**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why ANNs are powerful**\n",
    "\n",
    "* Can approximate **any function** (Universal Approximation Theorem).\n",
    "* Handle **high-dimensional data** (images, text, speech).\n",
    "* Learn **hierarchical features** automatically.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Instead of manually engineering features, ANN **learns features by itself**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Training Process Summary**\n",
    "\n",
    "1. Initialize weights & biases.\n",
    "2. Forward pass → compute outputs.\n",
    "3. Compute loss.\n",
    "4. Backpropagation → compute gradients.\n",
    "5. Update weights via optimizer.\n",
    "6. Repeat for multiple epochs until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Key Problems in ANN**\n",
    "\n",
    "* **Vanishing gradients** → small gradients → slow learning (sigmoid/tanh).\n",
    "* **Exploding gradients** → large gradients → unstable learning.\n",
    "* **Overfitting** → network memorizes training data → poor generalization.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Proper weight initialization, activation choice, and regularization solve these.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
