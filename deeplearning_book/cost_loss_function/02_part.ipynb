{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c850223",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Loss Functions for Regression in Neural Networks\n",
    "\n",
    "Regression problems use **continuous output**, so these loss functions measure how far predictions $ŷ$ are from actual values $y$.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Loss (single sample):**\n",
    "$$\n",
    "(y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "**Cost (all samples):**\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### ✔ Advantages\n",
    "\n",
    "* Differentiable → works smoothly with gradient descent\n",
    "* Single global minimum\n",
    "* Fast convergence due to smooth parabola shape\n",
    "\n",
    "### ✘ Disadvantage\n",
    "\n",
    "* **Not robust to outliers**\n",
    "  Squaring errors makes large deviations influence the model heavily.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mean Absolute Error (MAE)\n",
    "\n",
    "**Loss:**\n",
    "$$\n",
    "|y - \\hat{y}|\n",
    "$$\n",
    "\n",
    "**Cost:**\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "### ✔ Advantages\n",
    "\n",
    "* Robust to outliers → no squaring, so extreme values don't dominate\n",
    "\n",
    "### ✘ Disadvantages\n",
    "\n",
    "* No smooth parabola → gradient is constant\n",
    "* Slower convergence\n",
    "* Need **sub-gradient** methods (not standard gradient descent)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Huber Loss\n",
    "\n",
    "Hybrid of MSE and MAE. Uses a threshold δ (hyperparameter).\n",
    "\n",
    "$$\n",
    "\\text{If } |y - \\hat{y}| < \\delta:\n",
    "\\quad \\frac{1}{2}(y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Else: }\n",
    "\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2\n",
    "$$\n",
    "\n",
    "### ✔ Why use?\n",
    "\n",
    "* Behaves like **MSE** for small errors → smooth optimization\n",
    "* Behaves like **MAE** for large errors → ignores outliers\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\n",
    "\\sqrt{\n",
    "\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "Just the **square root of MSE**.\n",
    "\n",
    "### Notes:\n",
    "\n",
    "* Same sensitivity to outliers as MSE\n",
    "* Output is in the **same unit as target variable**\n",
    "\n",
    "You were asked to think about pros/cons — here they are:\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Interpretable (same units as data)\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Still penalizes outliers heavily\n",
    "\n",
    "---\n",
    "\n",
    "**Quick Use-Case Guide**\n",
    "\n",
    "| Loss Function | Outliers? | Convergence | Smooth Gradient | Best For                      |\n",
    "| ------------- | --------- | ----------- | --------------- | ----------------------------- |\n",
    "| MSE           | No        | Fast        | Yes             | Clean data                    |\n",
    "| MAE           | Yes       | Slower      | No              | Noisy data                    |\n",
    "| Huber         | Some      | Moderate    | Yes (mostly)    | Mixed data                    |\n",
    "| RMSE          | No        | Fast        | Yes             | When interpretability matters |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65d06f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
