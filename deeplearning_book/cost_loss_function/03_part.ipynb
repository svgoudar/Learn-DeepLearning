{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4912b4f8",
   "metadata": {},
   "source": [
    "## Loss Functions for Classification\n",
    "\n",
    "Once the model predicts (\\hat{y}), a **loss function** measures how far it is from the actual output (y). For classification, we use **cross-entropy loss** in three variants depending on the problem.\n",
    "\n",
    "---\n",
    "\n",
    "###  Binary Cross Entropy (BCE)\n",
    "\n",
    "**Use when:** Only **two classes** (0 or 1)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\left[y \\cdot \\log(\\hat{y}) + (1-y) \\cdot \\log(1-\\hat{y})\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "üîπ Output layer uses **Sigmoid activation**\n",
    "üîπ $\\hat{y}$ is a probability between 0 and 1\n",
    "üîπ Works for problems like spam detection, fraud detection, yes/no classification\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Email: spam (1) / not spam (0)\n",
    "* Disease: present (1) / absent (0)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Categorical Cross Entropy (CCE)\n",
    "\n",
    "**Use when:** More than 2 classes **AND** target is **one-hot encoded**\n",
    "\n",
    "Example target:\n",
    "\n",
    "```\n",
    "good ‚Üí [1, 0, 0]\n",
    "bad ‚Üí [0, 1, 0]\n",
    "neutral ‚Üí [0, 0, 1]\n",
    "```\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{j=1}^{C} y_{ij} \\cdot \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "üîπ Output layer uses **Softmax**\n",
    "üîπ Produces **probability for each class**\n",
    "üîπ Retains info about how likely each class is\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Sentiment: positive / neutral / negative\n",
    "* Digit recognition (0‚Äì9)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Sparse Categorical Cross Entropy (Sparse CCE)\n",
    "\n",
    "**Use when:** More than 2 classes **AND** target is given as a **single integer label**, not one-hot\n",
    "\n",
    "Example target:\n",
    "\n",
    "```\n",
    "good ‚Üí 0  \n",
    "bad  ‚Üí 1  \n",
    "neutral ‚Üí 2\n",
    "```\n",
    "\n",
    "Underlying formula is same as CCE but WITHOUT one-hot encoding.\n",
    "\n",
    "üîπ Still uses **Softmax**\n",
    "üîπ Only cares about the correct index\n",
    "üîπ Does **not** store probability of other classes explicitly\n",
    "\n",
    "**Output Example (Softmax):**\n",
    "\n",
    "```\n",
    "[0.2, 0.3, 0.5]\n",
    "```\n",
    "\n",
    "Sparse CCE only needs the index with the highest probability ‚Üí 2\n",
    "\n",
    "‚úÖ Good when labels are large or one-hot encoding is memory-heavy\n",
    "‚ùå Loses the probability info of other classes\n",
    "\n",
    "---\n",
    "\n",
    "### üîç When to Use What?\n",
    "\n",
    "| Problem Type                 | Loss Function         | Output Format | Activation |\n",
    "| ---------------------------- | --------------------- | ------------- | ---------- |\n",
    "| Binary Classification        | Binary Cross Entropy  | 0/1           | Sigmoid    |\n",
    "| Multi-class + One-hot labels | Categorical CE        | $$0,1,0,0...$$  | Softmax    |\n",
    "| Multi-class + Integer labels | Sparse Categorical CE | 2, 4, 1, ...  | Softmax    |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway**\n",
    "\n",
    "* **Binary ‚Üí BCE + Sigmoid**\n",
    "* **Multi-class (one-hot) ‚Üí CCE + Softmax**\n",
    "* **Multi-class (integer labels) ‚Üí Sparse CCE + Softmax**\n",
    "* Use **CCE** if probabilities of all classes matter\n",
    "* Use **Sparse CCE** if only the winning class index matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249babe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
