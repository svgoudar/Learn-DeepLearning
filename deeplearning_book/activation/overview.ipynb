{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d52ea3",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Activation Function\n",
    "\n",
    "An **activation function** is a mathematical function applied to the output of a neuron. Its role is to determine whether a neuron should â€œactivateâ€ (i.e., pass its value forward) or not.\n",
    "\n",
    "Without activation functions, neural networks would just be **linear models**, no matter how many layers they have â€” meaning they couldnâ€™t learn complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do We Need Activation Functions?\n",
    "\n",
    "### ðŸ”¹ 1. Introduce Non-Linearity\n",
    "\n",
    "Real-world data and problems (like vision, speech, and language) are **non-linear**. Activation functions allow neural networks to **model complex relationships**.\n",
    "\n",
    "### ðŸ”¹ 2. Help with Hierarchical Learning\n",
    "\n",
    "Activation functions allow deeper layers to progressively learn higher-level features:\n",
    "\n",
    "* Early layers â†’ edges, curves\n",
    "* Middle layers â†’ shapes, patterns\n",
    "* Final layers â†’ objects, decisions\n",
    "\n",
    "### ðŸ”¹ 3. Control the Flow of Information\n",
    "\n",
    "They decide:\n",
    "\n",
    "* How much signal should pass forward (forward propagation)\n",
    "* How much gradient should pass backward (backpropagation)\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Activation Functions\n",
    "\n",
    "### 1. **Linear Activation**\n",
    "\n",
    "âž¤ *Used rarely* (except in some output layers).\n",
    "`f(x) = x` â†’ No non-linearity, so limited learning capability.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Sigmoid (Logistic)**\n",
    "\n",
    "**Formula:**\n",
    "[\n",
    "\\sigma(z)= \\frac{1}{1+e^{-z}}\n",
    "]\n",
    "\n",
    "âœ” Output range: (0, 1)\n",
    "âœ” Good for **binary classification** (output layer)\n",
    "\n",
    "âŒ Main problems:\n",
    "\n",
    "* **Vanishing gradient**\n",
    "* Saturates for large +ve / -ve values\n",
    "* Not zero-centered\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Tanh (Hyperbolic Tangent)**\n",
    "\n",
    "**Formula:**\n",
    "[\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "]\n",
    "\n",
    "âœ” Output range: (âˆ’1, 1)\n",
    "âœ” Zero-centered\n",
    "\n",
    "âŒ Still suffers from **vanishing gradients**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "**Formula:**\n",
    "[\n",
    "f(x) = \\max(0, x)\n",
    "]\n",
    "\n",
    "âœ” Fast and simple\n",
    "âœ” Works well for **hidden layers**\n",
    "âœ” Doesnâ€™t saturate for positive values\n",
    "âœ” Helps avoid vanishing gradients\n",
    "\n",
    "âŒ Can cause **â€œdead neuronâ€ problem** (output always 0)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Leaky ReLU**\n",
    "\n",
    "Fixes â€œdead ReLUâ€ issue\n",
    "[\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & x > 0 \\\n",
    "0.01x & x \\le 0\n",
    "\\end{cases}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Softmax**\n",
    "\n",
    "Used in **multi-class classification** (output layer)\n",
    "\n",
    "Converts raw values into **probabilities** that sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "**Where to Use Which Activation Function?**\n",
    "\n",
    "| Layer Type           | Recommended Activation   |\n",
    "| -------------------- | ------------------------ |\n",
    "| Hidden Layers        | ReLU / Leaky ReLU / Tanh |\n",
    "| Output (Binary)      | Sigmoid                  |\n",
    "| Output (Multi-class) | Softmax                  |\n",
    "| Regression Output    | Linear                   |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Role in Deep Learning**\n",
    "\n",
    "Activation functions impact:\n",
    "\n",
    "* **Training speed**\n",
    "* **Gradient flow**\n",
    "* **Accuracy**\n",
    "* **Stability**\n",
    "\n",
    "Choosing the right activation function is **critical** for solving vanishing gradient, exploding gradient, or dead neuron problems.\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
