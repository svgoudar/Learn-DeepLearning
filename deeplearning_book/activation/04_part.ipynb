{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec93bc3",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## ELU (Exponential Linear Unit)\n",
    "\n",
    "* ELU is designed to **fix the limitations of ReLU, Leaky ReLU, and Parametric ReLU**.\n",
    "\n",
    "* **Formula:**\n",
    "  \n",
    "  $$\n",
    "  f(x) =\n",
    "  \\begin{cases}\n",
    "  x & \\text{if } x > 0 \\\n",
    "  \\alpha (e^x - 1) & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  Here, α is usually set to 1.\n",
    "\n",
    "* **Key features:**\n",
    "\n",
    "  1. **No dead ReLU problem** – because negative inputs are not zeroed out completely; they are mapped to negative values.\n",
    "  2. **Zero-centered** – the mean of outputs is closer to zero, which improves weight updates during backpropagation.\n",
    "  3. **Smooth for negative values** – unlike Leaky ReLU, which is linear for negatives, ELU is exponential, which can lead to better learning.\n",
    "\n",
    "---\n",
    "\n",
    "![](../images/elu.png)\n",
    "\n",
    "### Context: Activation Functions and ReLU Problems\n",
    "\n",
    "* **Activation functions** introduce non-linearity in neural networks. Without them, neural networks would behave like linear regression models.\n",
    "\n",
    "* **ReLU (Rectified Linear Unit):**\n",
    "\n",
    "  * Formula: $f(x) = \\max(0, x)$\n",
    "  * Works great for avoiding vanishing gradients.\n",
    "  * **Problem:** Dead ReLU neurons – if the input to a neuron is always negative, the gradient is 0, so that neuron stops learning.\n",
    "\n",
    "* **Leaky ReLU and Parametric ReLU:**\n",
    "\n",
    "  * They fix the dead neuron problem by allowing a small slope for negative values:\n",
    "\n",
    "    * Leaky ReLU: $f(x) = x$ if $x > 0$, else $f(x) = \\alpha x$ (α is small, like 0.01)\n",
    "    * Parametric ReLU: similar, but α is learned during training.\n",
    "  * **Limitation:** They are **not zero-centered**. This can make weight updates less efficient because the mean of the outputs isn’t around zero.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "###  Forward and Backward Propagation\n",
    "\n",
    "* **Forward pass:**\n",
    "\n",
    "  * Positive values behave like ReLU (output = input).\n",
    "  * Negative values decay exponentially toward a negative asymptote.\n",
    "* **Backward pass (gradients):**\n",
    "\n",
    "  * Positive: gradient = 1\n",
    "  * Negative: gradient gradually decreases toward zero, but not exactly zero.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Solves dead ReLU issue.\n",
    "* Zero-centered outputs → better learning.\n",
    "* Can handle negative inputs more effectively.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Slightly more **computationally intensive** than ReLU or Leaky ReLU because of the exponential calculation.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Activation | Dead Neuron Problem | Zero-Centered | Complexity |\n",
    "| ---------- | ------------------- | ------------- | ---------- |\n",
    "| ReLU       | Yes                 | No            | Low        |\n",
    "| Leaky ReLU | No                  | No            | Low        |\n",
    "| PReLU      | No                  | No            | Low-Med    |\n",
    "| ELU        | No                  | Yes           | Medium     |\n",
    "\n",
    "**Intuition:** ELU is like ReLU but smarter – it keeps neurons alive in the negative region and ensures outputs are centered around zero, which helps the network learn faster and more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6feff7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
