{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa49cf7",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "\n",
    "### ✔ Formula\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### ✔ Output Range\n",
    "\n",
    "Transforms any input into a value between **0 and 1**.\n",
    "\n",
    "This made sigmoid initially useful for:\n",
    "\n",
    "* Small neural networks\n",
    "* Binary classification\n",
    "* Probabilistic interpretation (to some extent)\n",
    "\n",
    "!$alt text$(../images/sigmoid.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Derivative & Vanishing Gradient Problem\n",
    "\n",
    "* The derivative of sigmoid ranges between **0 and 0.25**\n",
    "* During **backpropagation**, many layers multiply small gradients repeatedly\n",
    "* With deep networks, gradients shrink drastically → **vanishing gradient problem**\n",
    "* Result: **weights stop updating**, slowing or halting learning\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Drawbacks of Sigmoid\n",
    "\n",
    "### ❌ 1. Vanishing Gradients\n",
    "\n",
    "* Due to tiny derivatives (0 to 0.25), weight updates get smaller layer by layer.\n",
    "\n",
    "### ❌ 2. Not Zero-Centered\n",
    "\n",
    "* The output is between 0 and 1 (mean ≈ 0.5, not 0)\n",
    "* This causes **inefficient weight updates** during optimization\n",
    "\n",
    "Zero-centered activations help:\n",
    "\n",
    "* Faster convergence\n",
    "* Better gradient flow\n",
    "\n",
    "### ❌ 3. Computationally Slow\n",
    "\n",
    "* Uses exponential operation: ( e^{-x} ), which is **computationally expensive**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Advantages of Sigmoid\n",
    "\n",
    "✔ Smooth and continuous\n",
    "✔ Output strictly between **0 and 1**\n",
    "✔ Useful when output interpretation is binary\n",
    "✔ Prevents sharp jumps in output\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Disadvantages Recap\n",
    "\n",
    "1. ❌ Prone to **vanishing gradients**\n",
    "2. ❌ **Not zero-centered**\n",
    "3. ❌ Uses costly **exponential operations**\n",
    "\n",
    "Because of these issues, sigmoid is rarely preferred in **deep networks** today.\n",
    "\n",
    "---\n",
    "\n",
    "## Transition to Better Alternatives\n",
    "\n",
    "Due to sigmoid’s drawbacks, the next activation functions like **tanh** (and later **ReLU**) were introduced to overcome these problems.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "The sigmoid function was widely used in early deep learning models, but due to:\n",
    "\n",
    "* Vanishing gradients\n",
    "* Non-zero-centered output\n",
    "* Computation inefficiency\n",
    "\n",
    "…it has mostly been replaced by more effective activations like **tanh** and **ReLU**.\n",
    "\n",
    "The next step in the lesson is understanding **tanh activation** — its benefits and limitations.\n",
    "\n",
    "\n",
    "Here’s a **clear and structured summary** of the explanation you provided on **Sigmoid Activation Function**:\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Sigmoid Activation Function?\n",
    "\n",
    "The **sigmoid function** is defined as:\n",
    "\n",
    "$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$\n",
    "\n",
    "It is applied during **forward propagation** after:\n",
    "\n",
    "1. Inputs × Weights\n",
    "2. Adding Bias\n",
    "3. Applying Activation Function\n",
    "\n",
    "It **squashes values to the range (0, 1)** and is commonly used in early neural network models.\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative of Sigmoid & Its Impact\n",
    "\n",
    "* The derivative of the sigmoid function always lies between **0 and 0.25**\n",
    "* This is important during **backpropagation**, when gradients are used to update weights\n",
    "* Due to chain rule, multiplying many small derivatives causes the **vanishing gradient problem**\n",
    "\n",
    "Example:\n",
    "\n",
    "* Multiple small gradients get multiplied (e.g., 0.2 × 0.01 × 0.1…)\n",
    "* Final gradient becomes extremely small\n",
    "* Weight updates become negligible → learning stops\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} \\approx w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "This prevents proper training in deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Main Disadvantages of Sigmoid\n",
    "\n",
    "### ❌ 1. Vanishing Gradient Problem\n",
    "\n",
    "* Small derivatives (0 to 0.25) lead to tiny weight updates during backpropagation.\n",
    "\n",
    "### ❌ 2. Not Zero-Centered\n",
    "\n",
    "* Output lies between **0 and 1**, not symmetric around zero\n",
    "* Makes gradient descent inefficient\n",
    "* Nullifies efficient weight updates\n",
    "\n",
    "✅ Zero-centered functions help optimize weights faster\n",
    "(e.g., tanh outputs from −1 to 1)\n",
    "\n",
    "### ❌ 3. Computationally Expensive\n",
    "\n",
    "* Sigmoid uses **exponential operations (e⁻ˣ)**\n",
    "* Slower on processors compared to simpler functions like ReLU\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Sigmoid\n",
    "\n",
    "✔ Works well for **binary classification**\n",
    "✔ Outputs values close to **0 or 1** (good for interpretation)\n",
    "✔ Smooth and differentiable function\n",
    "\n",
    "However, it's only practical in:\n",
    "\n",
    "* Output layers of binary classification models\n",
    "* Small or shallow neural networks\n",
    "\n",
    "---\n",
    "\n",
    "## Why Move Beyond Sigmoid?\n",
    "\n",
    "Because of its:\n",
    "\n",
    "* Vanishing gradient issues\n",
    "* Non-zero-centered output\n",
    "* Computational inefficiency\n",
    "\n",
    "Researchers introduced better activation functions like:\n",
    "\n",
    "* **tanh** (fixes zero-centering issue)\n",
    "* **ReLU** and its variants (avoids vanishing gradients)\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Sigmoid was historically important but is **not ideal for deep networks** because:\n",
    "\n",
    "* It slows or stops learning\n",
    "* It doesn’t update weights efficiently\n",
    "* It adds unnecessary computation\n",
    "\n",
    "It is now mostly used:\n",
    "✅ In **output layers** for binary classification\n",
    "❌ Not in **hidden layers** of deep neural networks\n",
    "\n",
    "Next, we move to **tanh activation function** as an improvement.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
