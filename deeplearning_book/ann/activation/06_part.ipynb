{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719d5935",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Activation choice\n",
    "\n",
    "**1. Hidden layers**\n",
    "Never use sigmoid or tanh in hidden layers of deep networks. They compress values into a small range and cause **vanishing gradients** during backpropagation. Gradients shrink as they move backward, training stalls.\n",
    "\n",
    "Use **ReLU or its variants** instead:\n",
    "\n",
    "* ReLU\n",
    "* Leaky ReLU\n",
    "* PReLU\n",
    "* ELU\n",
    "\n",
    "These avoid vanishing gradients because their derivatives do not collapse to zero for positive inputs.\n",
    "\n",
    "**2. Output layer**\n",
    "Depends on the task type:\n",
    "\n",
    "* **Binary classification (1 output neuron)** → use **sigmoid**\n",
    "  Output range 0–1, gives probability of class 1.\n",
    "\n",
    "* **Multi-class classification (more than 2 classes, one neuron per class)** → use **softmax**\n",
    "  Converts outputs to probabilities that sum to 1.\n",
    "\n",
    "**3. Workflow inside each neuron**\n",
    "\n",
    "1. Weighted sum:\n",
    "     `z = w·x + b`\n",
    "2. Apply activation:\n",
    "     `a = activation(z)`\n",
    "\n",
    "Weights and biases are initialized before training. Weight initialization methods will be discussed separately.\n",
    "\n",
    "**4. Depth and gradient impact**\n",
    "More layers = deeper network. If sigmoid/tanh is used repeatedly in many layers, derivative chains multiply values in the range (0, 1), leading to almost zero gradient.\n",
    "\n",
    "**5. Standard rule**\n",
    "\n",
    "* Hidden layers → ReLU or variants\n",
    "* Output layer → Sigmoid (binary) or Softmax (multi-class)\n",
    "\n",
    "\n",
    "| Activation Function | Output Range      | Derivative Issue          | Pros                                     | Cons / Problems                    | Typical Use Case                    |\n",
    "| ------------------- | ----------------- | ------------------------- | ---------------------------------------- | ---------------------------------- | ----------------------------------- |\n",
    "| **Sigmoid**         | (0, 1)            | Vanishing gradient        | Smooth output, probability mapping       | Kills gradients in deep nets       | Output (binary classification)      |\n",
    "| **Tanh**            | (-1, 1)           | Vanishing gradient        | Zero-centered, stronger gradients        | Still vanishes in deep networks    | Rare in hidden layers today         |\n",
    "| **ReLU**            | [0, ∞)            | Dead neurons (zeros out)  | Fast, simple, no vanishing for positives | Can output only 0 for many neurons | Hidden layers (default)             |\n",
    "| **Leaky ReLU**      | (-∞, ∞)           | Reduced dead neurons      | Allows small negative slope              | Slope is fixed, not trainable      | Hidden layers                       |\n",
    "| **PReLU**           | (-∞, ∞)           | Reduced dead neurons      | Learnable negative slope                 | Slightly more compute cost         | Deep hidden layers                  |\n",
    "| **ELU**             | (-1, ∞)           | Less vanishing            | Negative outputs help mean shift         | Exp computation is slower          | Hidden layers                       |\n",
    "| **Softmax**         | (0, 1), sums to 1 | N/A (used only at output) | Outputs class probabilities              | Only for multi-class output        | Output (multi-class classification) |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
