{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f41d0d8",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# Tanh Activation Function\n",
    "\n",
    "The **tanh (hyperbolic tangent)** activation function is a non-linear function used in neural networks, especially in hidden layers.\n",
    "\n",
    "It transforms input values into a range between **-1 and +1**.\n",
    "\n",
    "### ✅ Formula:\n",
    "\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "z = \\sum_{i=1}^{n}(w_i \\cdot x_i) + b\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "![](../images/tanh.png)\n",
    "\n",
    "## How It Works (Intuition)\n",
    "\n",
    "Think of tanh as a \"scaled and shifted\" version of the sigmoid function.\n",
    "\n",
    "* **Sigmoid** squashes values between **0 and 1**\n",
    "* **Tanh** squashes values between **-1 and +1**\n",
    "\n",
    "So:\n",
    "\n",
    "* Negative inputs become **negative outputs**\n",
    "* Positive inputs become **positive outputs**\n",
    "* Outputs are more centered around **zero**\n",
    "\n",
    "This makes learning **faster and more stable** than sigmoid in many models.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Range:\n",
    "\n",
    "$$\n",
    "-1 ;\\leq; \\tanh(z) ;\\leq; +1\n",
    "$$\n",
    "\n",
    "Example behavior:\n",
    "\n",
    "* Large positive z → output ≈ +1\n",
    "* Large negative z → output ≈ -1\n",
    "* z near 0 → output ≈ 0\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Derivative of Tanh (Used in Backpropagation)\n",
    "\n",
    "$$\n",
    "\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)\n",
    "$$\n",
    "\n",
    "The derivative of tanh ranges from **0 to 1**.\n",
    "\n",
    "This helps with learning, but not perfectly (we’ll see why later).\n",
    "\n",
    "---\n",
    "\n",
    "## Why Tanh is Better Than Sigmoid\n",
    "\n",
    "| Feature           | Sigmoid   | Tanh     |\n",
    "| ----------------- | --------- | -------- |\n",
    "| Output Range      | 0 to 1    | -1 to +1 |\n",
    "| Zero-centered     | ❌ No      | ✅ Yes    |\n",
    "| Derivative Range  | 0 to 0.25 | 0 to 1   |\n",
    "| Speed of learning | Slower    | Faster   |\n",
    "\n",
    "Zero-centered outputs make weight updates more balanced and reduce bias in optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Tanh\n",
    "\n",
    "✔️ **Zero-centered** → Better weight updates\n",
    "✔️ **Larger derivative range** than sigmoid → Less vanishing gradient (for shallow/medium networks)\n",
    "✔️ Works well in **hidden layers**, especially for classification problems\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages of Tanh\n",
    "\n",
    "❌ **Still suffers from vanishing gradient** in deep neural networks\n",
    "\n",
    "* Derivatives shrink layer by layer\n",
    "  ❌ **Computationally expensive**\n",
    "* Uses exponential operations\n",
    "  ❌ Not ideal for **very deep networks**\n",
    "\n",
    "That’s why ReLU and its variants became more popular.\n",
    "\n",
    "---\n",
    "\n",
    "## When is Tanh Used?\n",
    "\n",
    "✅ Good for:\n",
    "\n",
    "* Hidden layers in small or medium neural networks\n",
    "* Situations where negative values help (zero-centered outputs)\n",
    "* Recurrent Neural Networks (older architectures like vanilla RNN)\n",
    "\n",
    "Not preferred in very deep networks — ReLU performs better there.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "**Tanh is a zero-centered, non-linear activation function that maps inputs to $$-1, +1$$. It's better than sigmoid but still limited by the vanishing gradient problem in deeper networks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa72ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
