{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302de9b1",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "A **neural network (NN)** is a computational model inspired by the human brain.\n",
    "\n",
    "* It consists of **nodes (neurons)** connected by **edges (weights)**.\n",
    "* Neural networks are used to **learn patterns** from data and make predictions or decisions.\n",
    "\n",
    "**Key idea:** A neural network approximates a function ( f(x) ) that maps inputs ( x ) to outputs ( y ).\n",
    "\n",
    "---\n",
    "\n",
    "### Structure of a Neural Network\n",
    "\n",
    "Neural networks are organized in **layers**:\n",
    "\n",
    "1. **Input Layer**\n",
    "\n",
    "   * Receives raw features from the dataset.\n",
    "   * Example: In house price prediction: size, bedrooms, zip code, neighborhood wealth.\n",
    "\n",
    "2. **Hidden Layers**\n",
    "\n",
    "   * Intermediate layers that process the inputs.\n",
    "   * Extract higher-level features and patterns.\n",
    "   * Can be **one or many layers** (deep networks = many hidden layers).\n",
    "\n",
    "3. **Output Layer**\n",
    "\n",
    "   * Produces the final result: a number (regression) or class (classification).\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "* Each layer contains **neurons**.\n",
    "* Neurons compute a weighted sum of inputs, apply an **activation function**, and pass the output to the next layer.\n",
    "\n",
    "![alt text](../images/single_neuron.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Neuron Function\n",
    "\n",
    "Each neuron performs:\n",
    "\n",
    "\n",
    "$$z = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "$$\n",
    "a = \\text{activation}(z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_i$ = input feature\n",
    "*  w_i$  = weight of that input\n",
    "* b$  = bias term\n",
    "* z  = weighted sum\n",
    "* a  = output after activation\n",
    "\n",
    "---\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing networks to model complex relationships. Common functions:\n",
    "\n",
    "| Function    | Formula                               | Use Case                                 |\n",
    "| ----------- | ------------------------------------- | ---------------------------------------- |\n",
    "| **ReLU**    | $\\max(0, x)$                        | Hidden layers, avoids vanishing gradient |\n",
    "| **Sigmoid** | $\\frac{1}{1 + e^{-x}}$              | Outputs probability (0â€“1)                |\n",
    "| **Tanh**    | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Outputs range -1 to 1                    |\n",
    "| **Softmax** | $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$    | Multi-class classification               |\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "* Process of computing outputs from inputs.\n",
    "* Each neuron calculates its weighted sum, applies activation, and passes it forward.\n",
    "* Output of one layer becomes input to the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "* Measures how far the predicted output is from the actual target.\n",
    "* Common loss functions:\n",
    "\n",
    "  * **Mean Squared Error (MSE):** Regression\n",
    "  * **Cross-Entropy Loss:** Classification\n",
    "\n",
    "**Goal:** Minimize the loss during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "* Method to **update network weights** using **gradients** of the loss function.\n",
    "* Steps:\n",
    "\n",
    "  1. Compute loss\n",
    "  2. Calculate gradient of loss w.r.t each weight\n",
    "  3. Update weights in the **opposite direction of gradient**\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "\n",
    "* $\\eta$ = learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Algorithms that adjust weights to minimize loss efficiently. Examples:\n",
    "\n",
    "* **SGD (Stochastic Gradient Descent)**\n",
    "* **Adam** (Adaptive Moment Estimation)\n",
    "* **RMSProp**\n",
    "\n",
    "---\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. Initialize weights randomly.\n",
    "2. Feed inputs through the network (**forward propagation**).\n",
    "3. Calculate loss.\n",
    "4. Propagate error backward (**backpropagation**).\n",
    "5. Update weights using optimizer.\n",
    "6. Repeat for many epochs until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "* **Overfitting:** Network memorizes training data but fails on new data.\n",
    "* **Regularization Techniques:** Dropout, L1/L2 regularization, early stopping.\n",
    "* **Deep Networks:** Many hidden layers can learn very complex patterns.\n",
    "* **Densely Connected Layers:** Every neuron in one layer connects to every neuron in the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of Neural Networks\n",
    "\n",
    "* Image recognition (CNNs)\n",
    "* Speech recognition (RNNs, LSTMs)\n",
    "* Natural language processing (Transformers, GPT models)\n",
    "* Autonomous vehicles\n",
    "* Recommendation systems\n",
    "\n",
    "---\n",
    "**Summary:**\n",
    "A neural network is a system of interconnected neurons that learns to map inputs to outputs. It uses **activation functions**, **forward propagation**, **loss functions**, and **backpropagation** to iteratively improve predictions. Deep networks with multiple hidden layers can model highly complex functions, which is the essence of deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5da18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
