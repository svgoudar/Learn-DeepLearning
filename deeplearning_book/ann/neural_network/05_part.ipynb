{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2e03f4",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## Vanishing Gradient Problem in Deep Learning\n",
    "\n",
    "\n",
    "![image.png](../../images/vanish.png)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Forward Propagation Recap\n",
    "\n",
    "A typical network example:\n",
    "\n",
    "* **1 Input Layer**\n",
    "* **2 Hidden Layers** (each with 1 neuron)\n",
    "* **1 Output Layer**\n",
    "\n",
    "For every neuron:\n",
    "\n",
    "1. Compute the weighted sum:\n",
    "   $\n",
    "   Z = \\sum w_i x_i + b\n",
    "   $\n",
    "2. Apply **sigmoid activation**:\n",
    "   $\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $\n",
    "   Output lies in the range **(0, 1)**.\n",
    "\n",
    "The final output is compared with the target label using a **loss function** (e.g., squared loss), and backpropagation is used to update weights.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Backpropagation & Chain Rule\n",
    "\n",
    "To update a weight (e.g., (w_1)), the formula is:\n",
    "\n",
    "$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{\\text{old}}}\n",
    "$\n",
    "\n",
    "Using the **chain rule**, we compute:\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_1} =\n",
    "\\frac{\\partial L}{\\partial O_3}\n",
    "\\cdot\n",
    "\\frac{\\partial O_3}{\\partial O_2}\n",
    "\\cdot\n",
    "\\frac{\\partial O_2}{\\partial O_1}\n",
    "\\cdot\n",
    "\\frac{\\partial O_1}{\\partial w_1}\n",
    "$\n",
    "\n",
    "Each term involves derivatives of activations and weights.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why Sigmoid Causes the Vanishing Gradient Problem\n",
    "\n",
    "**Key observation**:\n",
    "The sigmoid function squashes input between **0 and 1**.\n",
    "Its derivative:\n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "This always lies in the range:\n",
    "$$\n",
    "0 \\leq \\sigma'(z) \\leq 0.25\n",
    "$$\n",
    "\n",
    "So, during backpropagation:\n",
    "\n",
    "* Each derivative term contributes a factor **< 1**\n",
    "* When chained across multiple layers, gradients become **very small**\n",
    "* Example:\n",
    "  $0.25 \\times 0.25 \\times 0.25 \\times \\dots = \\text{tiny number}$\n",
    "\n",
    "As a result:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} \\approx 0\n",
    "\\quad \\Rightarrow \\quad\n",
    "w_{\\text{new}} \\approx w_{\\text{old}}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "* **Weights stop updating**\n",
    "* **Learning slows or completely halts**\n",
    "* The model **fails to converge**\n",
    "\n",
    "This phenomenon is the **Vanishing Gradient Problem**.\n",
    "\n",
    "---\n",
    "\n",
    "### Consequences:\n",
    "\n",
    "* Deep networks using sigmoid **struggle to learn**\n",
    "* Backpropagated gradients shrink to near zero\n",
    "* Optimization stagnates\n",
    "* Training becomes ineffective in deep architectures\n",
    "\n",
    "---\n",
    "\n",
    "### Solution: Use Better Activation Functions\n",
    "\n",
    "To solve this, researchers replaced sigmoid with other activations, such as:\n",
    "\n",
    "1. **Tanh**\n",
    "2. **ReLU**\n",
    "3. **Leaky ReLU / PReLU**\n",
    "4. **Swish**\n",
    "5. **Others (ELU, GELU, SELU, etc.)**\n",
    "\n",
    "These functions avoid the shrinking-gradient effect and support deeper architectures.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Takeaway**\n",
    "\n",
    "* Sigmoid works fine in **shallow networks** or at the **output layer for binary classification**.\n",
    "* But in **deep networks**, its derivative causes gradients to vanish.\n",
    "* Avoid sigmoid in hidden layers â†’ use **ReLU or its variants** instead.\n",
    "* The vanishing gradient problem is the main reason modern architectures donâ€™t rely on sigmoid in deeper layers.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
