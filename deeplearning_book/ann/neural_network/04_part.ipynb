{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c263fc6",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# Multi-Layer Neural Networks\n",
    "\n",
    "The discussion moves from **single-layer perceptrons** to **multi-layer perceptrons (MLPs)**—also known as **artificial neural networks (ANNs)**—to overcome limitations of basic models.\n",
    "\n",
    "Single-layer perceptrons:\n",
    "\n",
    "* Only support **feedforward propagation**\n",
    "* Can solve only **linearly separable problems**\n",
    "* Have no efficient way to update weights\n",
    "\n",
    "To address this, we move to **multi-layer neural networks** that use:\n",
    "\n",
    "* ✅ **Forward Propagation**\n",
    "* ✅ **Backward Propagation** (invented/popularized by Geoffrey Hinton)\n",
    "* ✅ **Loss Functions**\n",
    "* ✅ **Optimizers**\n",
    "* ✅ **Activation Functions**\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture of a Multi-Layer Neural Network\n",
    "\n",
    "Example dataset:\n",
    "\n",
    "* Inputs/features: IQ, Study Hours, Play Hours → **X₁, X₂, X₃**\n",
    "* Output: Pass/Fail → **y (1 or 0)**\n",
    "\n",
    "Network structure:\n",
    "\n",
    "* **Input Layer** → 3 neurons\n",
    "* **Hidden Layer 1** → 1 neuron\n",
    "* **Output Layer** → 1 neuron\n",
    "  (This example shows a simple 2-layer ANN, but real networks can have many layers and neurons.)\n",
    "\n",
    "Each layer includes:\n",
    "\n",
    "* Weights (W)\n",
    "* Biases (b)\n",
    "* Neurons\n",
    "\n",
    "![alt text](..\\images\\mnn.png)\n",
    "---\n",
    "\n",
    "## Forward Propagation (Feedforward)\n",
    "\n",
    "Forward propagation happens in two main steps **for each neuron**:\n",
    "\n",
    "###  Step 1: Weighted Sum\n",
    "\n",
    "$$\n",
    "z = \\sum (x_i \\cdot w_i) + b\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "* (z = 95×0.01 + 4×0.02 + 4×0.03 + b_1 ≈ 1.151)\n",
    "\n",
    "### Step 2: Activation Function\n",
    "\n",
    "Using **sigmoid**:\n",
    "\n",
    "$$\n",
    "σ(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "* Converts value to range **0 to 1**\n",
    "* Example:\n",
    "  $$\n",
    "  σ(1.151) ≈ 0.759\n",
    "  $$\n",
    "\n",
    "This output is passed forward to the next layer, and the process repeats until the final output (**ŷ**) is produced.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function (Error Calculation)\n",
    "\n",
    "After forward propagation:\n",
    "\n",
    "* Predicted output: **ŷ = 0.511**\n",
    "* Actual output: **y = 1**\n",
    "\n",
    "Error (Loss):\n",
    "$$\n",
    "Loss = (y - ŷ)^2 ≈ (1 - 0.511)^2 ≈ 0.49\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Propagation (Backprop)\n",
    "\n",
    "Goal:\n",
    "\n",
    "* Reduce the loss by updating weights\n",
    "* Starts from output layer and moves **backwards**\n",
    "\n",
    "Weights updated in reverse order:\n",
    "\n",
    "1. Output layer weights (e.g., W₄)\n",
    "2. Hidden layer weights (e.g., W₁, W₂, W₃)\n",
    "\n",
    "To do this efficiently, we use:\n",
    "\n",
    "* **Optimizers** (e.g., Gradient Descent)\n",
    "* **Loss functions**\n",
    "* **Activation function derivatives**\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function vs Cost Function\n",
    "\n",
    "* **Loss Function**: Error for **each individual data point**\n",
    "* **Cost Function**: Average (or total) loss over **all data points**\n",
    "  $$\n",
    "  Cost = \\frac{1}{n} \\sum (y - \\hat{y})^2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Activations & Optimizers\n",
    "\n",
    "* **Sigmoid** used for binary outputs\n",
    "* Other activations (ReLU, Tanh, etc.) will be covered later\n",
    "* Optimizers (like SGD, Adam) adjust weights during backprop\n",
    "\n",
    "---\n",
    "\n",
    "##  The Training Loop\n",
    "\n",
    "For each record:\n",
    "\n",
    "1. Forward propagation → compute ŷ\n",
    "2. Calculate loss\n",
    "3. Backward propagation → update weights\n",
    "4. Repeat until loss is minimized\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Neuron as a Function\n",
    "\n",
    "At the core of any neural network is a **neuron**, which is essentially a mathematical function.\n",
    "\n",
    "For a neuron, the **input-output relationship** is:\n",
    "\n",
    "$$\n",
    "y = f(z) = f\\Big(\\sum_{i=1}^{n} w_i x_i + b \\Big)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_1, x_2, ..., x_n$ are the inputs.\n",
    "* $w_1, w_2, ..., w_n$ are the **weights** associated with each input.\n",
    "* $b$ is the **bias**.\n",
    "* $z = \\sum w_i x_i + b$ is called the **logit** or **pre-activation value**.\n",
    "* $f$ is the **activation function** (e.g., Sigmoid, ReLU, Tanh), introducing **nonlinearity**.\n",
    "* $y$ is the output of the neuron.\n",
    "\n",
    "**Intuition:**\n",
    "Weights determine how important each input is, bias allows shifting the activation, and the activation function lets the neuron “decide” whether to activate or not.\n",
    "\n",
    "---\n",
    "\n",
    "## Single Layer vs Multi-Layer\n",
    "\n",
    "* **Single layer network**: Only one layer of neurons between input and output.\n",
    "  $$\n",
    "  \\hat{y} = f(W^T x + b)\n",
    "  $$\n",
    "\n",
    "  * Limitation: Can only model **linear relationships** (or linear separable problems).\n",
    "\n",
    "* **Multi-layer network**: Multiple layers of neurons allow **composing functions**, enabling modeling of **nonlinear relationships**.\n",
    "\n",
    "Mathematically, a **2-layer network** (1 hidden layer) is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h &= f_1(W_1^T x + b_1) \\quad \\text{(hidden layer output)} \\\n",
    "\\hat{y} &= f_2(W_2^T h + b_2) \\quad \\text{(final output)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $x \\in \\mathbb{R}^n$ is input\n",
    "* $h \\in \\mathbb{R}^m$ is hidden layer activation\n",
    "* $\\hat{y}$ is the predicted output\n",
    "\n",
    "**Intuition:** Each layer applies a **nonlinear transformation** to the input from the previous layer, building increasingly complex representations.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "Forward propagation is just **computing outputs layer by layer**:\n",
    "\n",
    "1. Multiply inputs by weights, add bias → (z = W^T x + b)\n",
    "2. Apply activation → (a = f(z))\n",
    "3. Pass (a) as input to the next layer\n",
    "\n",
    "For (L) layers:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^1 &= f^1(W^1 x + b^1) \\\n",
    "a^2 &= f^2(W^2 a^1 + b^2) \\\n",
    "&\\vdots \\\n",
    "\\hat{y} &= f^L(W^L a^{L-1} + b^L)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Intuition:** The network transforms raw input into a hierarchical representation, layer by layer.\n",
    "\n",
    "---\n",
    "\n",
    "##  Loss Function and Training\n",
    "\n",
    "To train the network, we define a **loss function**, e.g., Mean Squared Error (MSE) for regression:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "* $\\hat{y}_i$ = predicted output\n",
    "* $y_i$ = true output\n",
    "\n",
    "**Goal:** Minimize the loss by adjusting weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation (Intuition)\n",
    "\n",
    "**Backpropagation** is a way to compute **how much each weight contributed to the error** using the **chain rule of calculus**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^l} = \\frac{\\partial \\mathcal{L}}{\\partial a^l} \\cdot \\frac{\\partial a^l}{\\partial z^l} \\cdot \\frac{\\partial z^l}{\\partial W^l}\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Start from the output, compute the error\n",
    "* Propagate the error backward through each layer\n",
    "* Update weights in the direction that reduces loss\n",
    "\n",
    "Weight update formula (gradient descent):\n",
    "\n",
    "$$\n",
    "W^l := W^l - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^l}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "1. Multi-layer networks **compose functions**: Each layer transforms the representation of input data.\n",
    "2. **Nonlinearity is essential**: Without activation functions, multiple layers collapse to a single linear transformation.\n",
    "3. Forward propagation = compute predictions; backward propagation = compute gradients and update weights.\n",
    "4. Hidden layers automatically **learn hierarchical features**, reducing the need for manual feature engineering.\n",
    "5. Multilayer networks use **hidden layers**\n",
    "6. Forward and backward propagation make learning possible\n",
    "7. **Sigmoid** activation converts values to (0,1)\n",
    "8. Loss tells us how wrong the model is\n",
    "9. Optimizers reduce error by adjusting weights\n",
    "10. Backpropagation enables efficient learning\n",
    "11. This is the foundation of deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ce886",
   "metadata": {},
   "source": [
    "## **Problem Statement: Predicting Student Exam Success**\n",
    "\n",
    "Imagine we want to predict whether a student will **pass or fail an exam** based on the following features:\n",
    "\n",
    "| Feature                | Symbol | Example |\n",
    "| ---------------------- | ------ | ------- |\n",
    "| IQ                     | (x_1)  | 95      |\n",
    "| Hours of Study per day | (x_2)  | 4       |\n",
    "| Hours of Play per day  | (x_3)  | 2       |\n",
    "\n",
    "The **output** is:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "1 & \\text{if student passes} \\\n",
    "0 & \\text{if student fails}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Representing as a Neural Network**\n",
    "\n",
    "We can design a **simple multi-layer neural network**:\n",
    "\n",
    "* **Input Layer:** 3 neurons (IQ, study hours, play hours)\n",
    "* **Hidden Layer 1:** 2 neurons\n",
    "* **Output Layer:** 1 neuron (pass/fail)\n",
    "\n",
    "Each neuron has:\n",
    "\n",
    "$$\n",
    "z = \\sum w_i x_i + b\n",
    "$$\n",
    "$$\n",
    "a = f(z) \\quad (\\text{activation function})\n",
    "$$\n",
    "\n",
    "* **Weights ((w))** determine the importance of each feature.\n",
    "* **Bias ((b))** shifts the activation threshold.\n",
    "* **Activation function (sigmoid)** squashes the output between 0 and 1, representing a probability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Forward Propagation**\n",
    "\n",
    "**1. Hidden Layer 1**\n",
    "\n",
    "For neuron 1 in hidden layer 1:\n",
    "\n",
    "$$\n",
    "z_1^{(1)} = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + b_1\n",
    "$$\n",
    "$$\n",
    "a_1^{(1)} = f(z_1^{(1)}) \\quad \\text{(sigmoid)}\n",
    "$$\n",
    "\n",
    "For neuron 2:\n",
    "\n",
    "$$\n",
    "z_2^{(1)} = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + b_2\n",
    "$$\n",
    "$$\n",
    "a_2^{(1)} = f(z_2^{(1)})\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "The hidden layer transforms the raw features (IQ, study hours, play hours) into an **intermediate representation** that captures nonlinear combinations of the inputs.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Output Layer**\n",
    "\n",
    "The output neuron takes inputs from the hidden layer:\n",
    "\n",
    "$$\n",
    "z^{(2)} = w_{31} a_1^{(1)} + w_{32} a_2^{(1)} + b_3\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = f(z^{(2)}) \\quad \\text{(sigmoid, probability of passing)}\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "The output layer combines the learned features from hidden layers to make the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Calculating Loss**\n",
    "\n",
    "We compare the predicted output ((\\hat{y})) with the true label ((y)) using a **loss function**, e.g., **Mean Squared Error**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Predicted probability: (\\hat{y} = 0.6)\n",
    "* True output: (y = 1)\n",
    "* Loss: ((0.6 - 1)^2 = 0.16)\n",
    "\n",
    "**Intuition:**\n",
    "This tells us **how wrong the prediction is**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Backward Propagation (Weight Updates)**\n",
    "\n",
    "To reduce the loss, we adjust the weights **proportionally to their contribution to the error**:\n",
    "\n",
    "$$\n",
    "w := w - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "\n",
    "* $\\eta$ = learning rate\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial w}$ = gradient of loss w.r.t weight\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Neurons that contributed more to the error get updated more.\n",
    "* This process repeats over many examples until the network predicts accurately.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Key Takeaways from This Example**\n",
    "\n",
    "1. **Hidden Layers Learn Features Automatically**\n",
    "\n",
    "   * The network learns how to combine IQ, study, and play hours into features that help predict pass/fail.\n",
    "\n",
    "2. **Nonlinearity Matters**\n",
    "\n",
    "   * Without activation functions like sigmoid, the network would **only be able to learn linear rules** (e.g., a simple weighted sum).\n",
    "\n",
    "3. **Forward + Backward Propagation**\n",
    "\n",
    "   * Forward: compute predictions\n",
    "   * Backward: update weights based on error\n",
    "\n",
    "4. **Output as Probability**\n",
    "\n",
    "   * The network can output a probability (0-1) of passing, which can be thresholded to make a binary prediction.\n",
    "\n",
    "![alt text](..\\images\\bp_wu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028c30c",
   "metadata": {},
   "source": [
    "## Chain Rule of Derivatives in Neural Networks\n",
    "\n",
    "This video explains the **chain rule of derivatives** and its importance in **weight updating** during backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Weight Update Formula Recap\n",
    "\n",
    "The generic weight update rule is:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial w_{\\text{old}}}\n",
    "$$\n",
    "\n",
    "The key challenge is calculating the derivative (\\frac{\\partial \\text{Loss}}{\\partial w_{\\text{old}}}) efficiently when a weight influences multiple neurons across layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Applying the Chain Rule\n",
    "\n",
    "The chain rule helps decompose the derivative of the loss with respect to a weight into a product of simpler derivatives across layers:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w} = \\frac{\\partial \\text{Loss}}{\\partial O_{\\text{next}}} \\cdot \\frac{\\partial O_{\\text{next}}}{\\partial O_{\\text{current}}} \\cdot \\frac{\\partial O_{\\text{current}}}{\\partial w}\n",
    "$$\n",
    "\n",
    "* For a hidden neuron affecting multiple outputs, the chain rule ensures all paths are accounted for.\n",
    "* This allows the calculation of gradients even for weights deep in the network.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Example\n",
    "\n",
    "* Consider a network with one input layer, one hidden layer, and one output layer.\n",
    "* To update a weight (w_4), the derivative is computed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_4} = \\frac{\\partial \\text{Loss}}{\\partial O_2} \\cdot \\frac{\\partial O_2}{\\partial w_4}\n",
    "$$\n",
    "\n",
    "* For a weight in the first layer ((w_1)) influencing multiple downstream neurons, the derivative sums contributions through all paths:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_1} = \\frac{\\partial \\text{Loss}}{\\partial O_2} \\cdot \\frac{\\partial O_2}{\\partial O_1} \\cdot \\frac{\\partial O_1}{\\partial w_1} + \\dots\n",
    "$$\n",
    "\n",
    "This illustrates how **backpropagation relies on the chain rule** to propagate errors backward through multiple layers.\n",
    "\n",
    "![](../images/cod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34605fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* Chain rule allows calculating gradients for weights in **deep networks**.\n",
    "* Gradients account for all paths from a weight to the output.\n",
    "* These gradients are multiplied by the learning rate to update the weights.\n",
    "* This process enables the network to **learn efficiently** via backpropagation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
