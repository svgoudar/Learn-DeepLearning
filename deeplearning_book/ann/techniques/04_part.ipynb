{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec615a8",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## Optmization Techniques\n",
    "\n",
    "### 1. Weight Initialization\n",
    "\n",
    "Proper initialization prevents slow learning or gradient problems.\n",
    "\n",
    "* **Random Initialization** – Basic start, but can cause issues\n",
    "* **Xavier/Glorot Initialization** – For sigmoid/tanh activations\n",
    "* **He Initialization** – For ReLU and variants\n",
    "  Goal: Keep activations and gradients stable across layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Learning Rate Scheduling\n",
    "\n",
    "Controls how fast weights are updated.\n",
    "\n",
    "* **Step Decay** – Reduce learning rate every few epochs\n",
    "* **Exponential Decay** – Reduce LR gradually every step\n",
    "* **Reduce on Plateau** – Lower LR when validation stops improving\n",
    "* **Warm Restarts / Cyclical LR** – Vary LR in cycles\n",
    "  Goal: Fast initial learning + stable convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Gradient Clipping\n",
    "\n",
    "Prevents **exploding gradients**, especially in deep and recurrent networks.\n",
    "\n",
    "* Sets a max norm or value for gradients\n",
    "* Keeps training stable\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Regularization\n",
    "\n",
    "Prevents overfitting by reducing model complexity.\n",
    "\n",
    "* **L1 Regularization** – Forces sparsity (weights can become 0)\n",
    "* **L2 Regularization (Weight Decay)** – Shrinks large weights\n",
    "* **Dropout** – Randomly drops neurons during training\n",
    "* **Max Norm Constraints** – Limits weight values\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Batch Normalization\n",
    "\n",
    "Normalizes layer inputs during training.\n",
    "\n",
    "* Stabilizes and speeds up convergence\n",
    "* Reduces internal covariate shift\n",
    "* Acts as mild regularization\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Early Stopping\n",
    "\n",
    "Stops training when validation loss stops improving.\n",
    "\n",
    "* Prevents overfitting\n",
    "* Saves compute time\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Data Augmentation (for vision/NLP tasks)\n",
    "\n",
    "Artificially expands training data.\n",
    "\n",
    "* Rotations, flips, crops (for images)\n",
    "* Noise addition, masking, synonyms (for text/audio)\n",
    "* Improves generalization\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Momentum-Based Training (if supported by optimizer)\n",
    "\n",
    "Helps escape local minima and speeds up convergence.\n",
    "\n",
    "* Adds a fraction of past gradients to new updates\n",
    "  (Not an optimizer itself but used inside many optimizers.)\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Label Smoothing\n",
    "\n",
    "Prevents overconfident predictions by softening labels.\n",
    "\n",
    "Example: Instead of class \"cat\" = 1.0, others = 0.0 → use 0.9 and 0.1.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Gradient Accumulation\n",
    "\n",
    "Useful when batch size is limited due to memory.\n",
    "\n",
    "* Accumulates gradients over small batches\n",
    "* Updates weights after N steps\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ In Short:\n",
    "\n",
    "| Goal                | Techniques                                                |\n",
    "| ------------------- | --------------------------------------------------------- |\n",
    "| Prevent Overfitting | Dropout, L1/L2, Early Stopping, Data Augmentation         |\n",
    "| Stabilize Training  | Batch Norm, Gradient Clipping, Weight Init                |\n",
    "| Improve Convergence | Learning Rate Scheduling, Momentum, Gradient Accumulation |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
