{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b021531e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Regularization\n",
    "\n",
    "### ‚úÖ Why Regularization Is Needed\n",
    "\n",
    "Neural networks often:\n",
    "\n",
    "* Have **many parameters**\n",
    "* Can **memorize** training data\n",
    "* Perform well on training set but **fail on real-world data**\n",
    "\n",
    "Regularization fixes this by **restricting or modifying the learning process**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Main Regularization Techniques\n",
    "\n",
    "#### ‚úÖ 1. L1 and L2 Regularization (Weight Penalties)\n",
    "\n",
    "Add a penalty to the loss function so the model avoids large weights.\n",
    "\n",
    "##### ‚úî L1 Regularization (Lasso)\n",
    "\n",
    "* Adds absolute values of weights as penalty\n",
    "* Encourages **sparse models** (some weights become zero)\n",
    "\n",
    "[\n",
    "Loss = Loss_{original} + \\lambda \\sum |w_i|\n",
    "]\n",
    "\n",
    "##### ‚úî L2 Regularization (Ridge / Weight Decay)\n",
    "\n",
    "* Adds squared weights as penalty\n",
    "* Keeps weights small and smooth\n",
    "\n",
    "[\n",
    "Loss = Loss_{original} + \\lambda \\sum w_i^2\n",
    "]\n",
    "\n",
    "üîπ Œª (lambda) controls the strength of regularization.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Dropout\n",
    "\n",
    "Randomly ‚Äúdrops‚Äù neurons during training (e.g., 20‚Äì50%).\n",
    "\n",
    "‚úî Forces the network to not depend on specific neurons\n",
    "‚úî Reduces co-adaptation\n",
    "‚úî Acts like training multiple smaller networks simultaneously\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Early Stopping\n",
    "\n",
    "Stop training when validation loss stops improving.\n",
    "\n",
    "‚úî Prevents the model from memorizing training data\n",
    "‚úî Saves time and resources\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Data Augmentation\n",
    "\n",
    "Used mainly in Vision/NLP to make the dataset more diverse.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Rotate, flip, crop, add noise to images\n",
    "* Change words, mask tokens in text\n",
    "\n",
    "‚úî Helps model learn **robust features**, not specific samples.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Batch Normalization (Indirect Regularizer)\n",
    "\n",
    "Normalizes layer outputs.\n",
    "\n",
    "‚úî Stabilizes training\n",
    "‚úî Adds slight noise with mini-batches ‚Üí reduces overfitting\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Max Norm Constraints\n",
    "\n",
    "Limits how large weights can grow.\n",
    "\n",
    "‚úî Prevents exploding weights\n",
    "‚úî Works well with dropout\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Label Smoothing\n",
    "\n",
    "Instead of using hard labels like:\n",
    "\n",
    "* Cat = 1, Dog = 0 ‚Üí use softer targets like 0.9 and 0.1\n",
    "\n",
    "‚úî Avoids overconfident predictions\n",
    "‚úî Helps generalization\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Technique         | Purpose               | Key Benefit            |\n",
    "| ----------------- | --------------------- | ---------------------- |\n",
    "| L1/L2 Penalties   | Limit weight size     | Simpler models         |\n",
    "| Dropout           | Random neuron disable | Prevents co-adaptation |\n",
    "| Early Stopping    | Stop before overfit   | Better generalization  |\n",
    "| Data Augmentation | Add variation         | More robust learning   |\n",
    "| Batch Norm        | Normalize activations | Stable, regularized    |\n",
    "| Max Norm          | Cap weight values     | Avoid large weights    |\n",
    "| Label Smoothing   | Soften labels         | Avoid overconfidence   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f1e69",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
