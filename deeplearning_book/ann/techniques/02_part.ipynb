{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ca994f",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## Weight Initializing Techniques\n",
    "\n",
    "### **Why Weight Initialization Matters**\n",
    "\n",
    "1. **Small weights** → prevent exploding gradients.\n",
    "2. **Different weights** → each neuron can learn different features; avoid symmetry.\n",
    "3. **Good variance** → ensures effective learning across neurons.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Uniform Distribution Initialization**\n",
    "\n",
    "* Weights are randomly initialized from a **uniform distribution** within a specific range:\n",
    "  $$\n",
    "  W_{ij} \\sim U\\left(-\\frac{1}{\\sqrt{\\text{input}}}, \\frac{1}{\\sqrt{\\text{input}}}\\right)\n",
    "  $$\n",
    "* Example: If 3 input neurons, range is ($$-1/\\sqrt{3}, 1/\\sqrt{3}$$).\n",
    "* Ensures weights are small and diverse.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Xavier / Glorot Initialization**\n",
    "\n",
    "* Designed for **sigmoid/tanh activations**.\n",
    "* Balances variance of input and output to avoid vanishing/exploding gradients.\n",
    "\n",
    "**Two types:**\n",
    "\n",
    "1. **Xavier Normal**\n",
    "   $$\n",
    "   W_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right), \\quad \\sigma^2 = \\frac{2}{\\text{input} + \\text{output}}\n",
    "   $$\n",
    "\n",
    "2. **Xavier Uniform**\n",
    "   $$\n",
    "   W_{ij} \\sim U\\left(-\\sqrt{\\frac{6}{\\text{input} + \\text{output}}}, \\sqrt{\\frac{6}{\\text{input} + \\text{output}}}\\right)\n",
    "   $$\n",
    "\n",
    "* Keeps variance of activations consistent across layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. He / Kaiming Initialization**\n",
    "\n",
    "* Designed for **ReLU and variants**.\n",
    "* Takes into account that ReLU outputs are zero for half of inputs → needs slightly larger variance.\n",
    "\n",
    "**Two types:**\n",
    "\n",
    "1. **He Normal**\n",
    "   $$\n",
    "   W_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right), \\quad \\sigma^2 = \\frac{2}{\\text{input}}\n",
    "   $$\n",
    "\n",
    "2. **He Uniform**\n",
    "   $$\n",
    "   W_{ij} \\sim U\\left(-\\sqrt{\\frac{6}{\\text{input}}}, \\sqrt{\\frac{6}{\\text{input}}}\\right)\n",
    "   $$\n",
    "\n",
    "* Prevents vanishing/exploding gradients with ReLU networks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "| Technique       | Activation   | Distribution   | Range/Variance Formula                                       |\n",
    "| --------------- | ------------ | -------------- | ------------------------------------------------------------ |\n",
    "| Uniform         | Any          | Uniform        | ($$-1/√input, 1/√input$$)                                      |\n",
    "| Xavier / Glorot | Sigmoid/Tanh | Normal/Uniform | Normal: σ² = 2/(input+output)<br>Uniform: ±√6/(input+output) |\n",
    "| He / Kaiming    | ReLU         | Normal/Uniform | Normal: σ² = 2/input<br>Uniform: ±√6/input                   |\n",
    "\n",
    "* **Rule of thumb:** Use Xavier for sigmoid/tanh, He for ReLU, and uniform if unsure.\n",
    "* Modern frameworks (TensorFlow, PyTorch) implement these automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03750bd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
