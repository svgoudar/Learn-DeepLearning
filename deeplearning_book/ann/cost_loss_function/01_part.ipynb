{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0934f6b",
   "metadata": {},
   "source": [
    "## Cost Function vs Loss Function \n",
    "\n",
    "Both measure **error**, but they work at **different scales**.\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "**Error for a single training example**\n",
    "\n",
    "When one input goes through the network and produces prediction ŷ:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = L(y, \\hat{y})\n",
    "$$\n",
    "\n",
    "Example (Mean Squared Error for one sample):\n",
    "\n",
    "$$\n",
    "L = (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Used during:\n",
    "\n",
    "* Forward pass (per sample)\n",
    "* Backpropagation (per sample or per mini-batch)\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "**Average (or total) loss over the entire dataset or batch**\n",
    "\n",
    "If there are $N$ samples:\n",
    "\n",
    "$$\n",
    "\\text{Cost} = J = \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Example (MSE over many samples):\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Used by:\n",
    "\n",
    "* Optimizers like Gradient Descent, Adam, RMSProp\n",
    "* Backpropagation to update all weights\n",
    "\n",
    "---\n",
    "\n",
    "### Key Difference\n",
    "\n",
    "| Feature | Loss Function               | Cost Function                         |\n",
    "| ------- | --------------------------- | ------------------------------------- |\n",
    "| Scope   | One sample                  | Many samples (batch or full set)      |\n",
    "| Purpose | Measures individual error   | Measures overall model performance    |\n",
    "| Used in | Forward/backprop per sample | Optimization step                     |\n",
    "| Example | ((y - \\hat{y})^2)           | (\\frac{1}{N}\\sum (y_i - \\hat{y}_i)^2) |\n",
    "\n",
    "---\n",
    "\n",
    "**Why They Matter**\n",
    "\n",
    "* **Loss** tells how wrong prediction is for one example.\n",
    "* **Cost** aggregates loss, and the optimizer minimizes it by adjusting weights.\n",
    "\n",
    "Training loop:\n",
    "\n",
    "1. Forward pass → compute ŷ\n",
    "2. Compute loss\n",
    "3. Combine to form cost\n",
    "4. Backprop through cost\n",
    "5. Update weights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
