{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4931d3e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Choice od Loss Function\n",
    "When designing a neural network, the choice of **activation function** in the output layer directly determines which **loss function** you should use. The hidden layers usually use ReLU or its variants.\n",
    "\n",
    "---\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "* Hidden layers: ReLU\n",
    "* Output layer: Sigmoid\n",
    "\n",
    "**Reason:** Sigmoid outputs a probability between 0 and 1 for two classes.\n",
    "\n",
    "**Loss Function:**\n",
    "\n",
    "* **Binary Cross Entropy (BCE)**\n",
    "\n",
    "**Example:** Spam detection, yes/no classification\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-class Classification\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "* Hidden layers: ReLU\n",
    "* Output layer: Softmax\n",
    "\n",
    "**Reason:** Softmax outputs probabilities across multiple classes that sum to 1.\n",
    "\n",
    "**Loss Function:**\n",
    "\n",
    "* **Categorical Cross Entropy (CCE)** if labels are one-hot encoded\n",
    "* **Sparse Categorical Cross Entropy** if labels are integers\n",
    "\n",
    "**Example:** Digit recognition (0–9), sentiment analysis with multiple classes\n",
    "\n",
    "---\n",
    "\n",
    "### Regression\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "* Hidden layers: ReLU (or variants like Leaky ReLU, PReLU, ELU)\n",
    "* Output layer: Linear\n",
    "\n",
    "**Reason:** Linear activation allows the output to take any continuous value.\n",
    "\n",
    "**Loss Functions:**\n",
    "\n",
    "* **Mean Squared Error (MSE)**\n",
    "* **Mean Absolute Error (MAE)**\n",
    "* **Huber Loss**\n",
    "* **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "**Example:** Predicting house prices, stock prices\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "1. **Output layer activation defines loss function:**\n",
    "\n",
    "   * Sigmoid → Binary Cross Entropy\n",
    "   * Softmax → Categorical/Sparse Cross Entropy\n",
    "   * Linear → MSE, MAE, Huber, RMSE\n",
    "\n",
    "2. **Hidden layers** usually use ReLU or its variants.\n",
    "\n",
    "3. Choosing the correct combination ensures:\n",
    "\n",
    "   * Faster convergence\n",
    "   * Correct gradient calculation\n",
    "   * Efficient training\n",
    "\n",
    "This ensures your loss decreases properly during training and your network learns efficiently.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
