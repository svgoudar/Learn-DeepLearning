{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02779e70-5844-47e0-98ca-a2c4e2c0c4d3",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "---\n",
    "\n",
    "## What is Hyperparameter Tuning\n",
    "\n",
    "**Hyperparameter tuning** means selecting the *best configuration* (learning rate, hidden layers, activation, etc.) to make your ANN perform optimally for a given task.\n",
    "\n",
    "Unlike model **parameters** (weights and biases) which are learned automatically,\n",
    "**hyperparameters** are chosen *before training* and directly affect:\n",
    "\n",
    "* Learning efficiency\n",
    "* Model capacity\n",
    "* Generalization to unseen data\n",
    "\n",
    "---\n",
    "\n",
    "## Key ANN Problem Types\n",
    "\n",
    "| Problem Type                      | Goal                              | Example Outputs                       | Common Loss Function               |\n",
    "| --------------------------------- | --------------------------------- | ------------------------------------- | ---------------------------------- |\n",
    "| **Classification**                | Predict discrete categories       | Spam / not spam, dog / cat            | Cross-Entropy Loss                 |\n",
    "| **Regression**                    | Predict continuous values         | House price, stock value              | Mean Squared Error (MSE)           |\n",
    "| **Representation / Unsupervised** | Learn structure in unlabeled data | Feature extraction, anomaly detection | Reconstruction loss (Autoencoders) |\n",
    "\n",
    "Each type requires different **hyperparameters**, **architecture**, and **evaluation metrics**.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Hyperparameters\n",
    "\n",
    "### Architecture-Related\n",
    "\n",
    "| Hyperparameter            | Meaning              | Typical Range                             |\n",
    "| ------------------------- | -------------------- | ----------------------------------------- |\n",
    "| **Hidden layers**         | Depth of the network | 1–10 for simple models, 10+ for deep nets |\n",
    "| **Neurons per layer**     | Capacity per layer   | 16–1024                                   |\n",
    "| **Activation functions**  | Non-linearity type   | ReLU, Leaky ReLU, Sigmoid, Tanh           |\n",
    "| **Initialization method** | How weights start    | He normal (ReLU), Xavier (Tanh/Sigmoid)   |\n",
    "\n",
    "---\n",
    "\n",
    "### Optimization-Related\n",
    "\n",
    "| Hyperparameter        | Purpose                         | Common Values      |\n",
    "| --------------------- | ------------------------------- | ------------------ |\n",
    "| **Learning rate (η)** | Step size for gradient descent  | 1e-4 to 1e-1       |\n",
    "| **Optimizer**         | Controls how weights update     | Adam, RMSprop, SGD |\n",
    "| **Batch size**        | Number of samples per update    | 16–256             |\n",
    "| **Epochs**            | Number of full passes over data | 50–1000+           |\n",
    "| **Momentum / β**      | Memory in gradient updates      | 0.8–0.99           |\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization & Generalization\n",
    "\n",
    "| Method                  | Hyperparameter       | Description                                     |\n",
    "| ----------------------- | -------------------- | ----------------------------------------------- |\n",
    "| **Dropout**             | Dropout rate         | Randomly deactivate neurons (0.2–0.5)           |\n",
    "| **Weight decay (L2)**   | λ (penalty strength) | Adds constraint to weights                      |\n",
    "| **Early stopping**      | Patience             | Stops when validation loss stops improving      |\n",
    "| **Batch normalization** | -                    | Stabilizes learning by normalizing layer inputs |\n",
    "\n",
    "---\n",
    "\n",
    "### Task-Specific\n",
    "\n",
    "| Problem Type                   | Key Hyperparameters                                               |\n",
    "| ------------------------------ | ----------------------------------------------------------------- |\n",
    "| **Classification**             | Activation (Softmax/Sigmoid), loss (Cross-Entropy), learning rate |\n",
    "| **Regression**                 | Output activation (Linear), loss (MSE/MAE), normalization         |\n",
    "| **Autoencoder / Unsupervised** | Latent dimension size, reconstruction loss weight                 |\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning Strategies\n",
    "\n",
    "### Manual / Empirical Search\n",
    "\n",
    "Try combinations based on intuition.\n",
    "\n",
    "> ✅ Simple but inefficient.\n",
    "> Used in early experiments or for small models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Grid Search**\n",
    "\n",
    "Evaluate *every* combination of predefined hyperparameters.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "param_grid = {\n",
    "  'hidden_layer_sizes': [(50,), (100,), (100,50)],\n",
    "  'activation': ['relu', 'tanh'],\n",
    "  'learning_rate_init': [0.001, 0.01],\n",
    "  'solver': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(MLPClassifier(max_iter=300), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "```\n",
    "\n",
    "✅ Exhaustive\n",
    "❌ Very slow (explodes with parameters)\n",
    "\n",
    "---\n",
    "\n",
    "### **Random Search**\n",
    "\n",
    "Randomly samples combinations from ranges.\n",
    "\n",
    "✅ Finds near-optimal results faster than Grid Search.\n",
    "❌ Doesn’t explore systematically.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayesian Optimization**\n",
    "\n",
    "Learns a *probabilistic model* of performance vs. hyperparameters and picks next candidates smartly.\n",
    "\n",
    "✅ Very efficient for expensive training (deep models)\n",
    "❌ More complex to implement\n",
    "\n",
    "Libraries: `Optuna`, `scikit-optimize`, `Hyperopt`, `Ray Tune`\n",
    "\n",
    "Example (Optuna):\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=trial.suggest_categorical(\"hidden_layer_sizes\", [(64,), (128,), (128,64)]),\n",
    "        learning_rate_init=trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "        activation=trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "    )\n",
    "    return cross_val_score(model, X_train, y_train, cv=3).mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Evolutionary Algorithms / Genetic Search**\n",
    "\n",
    "Uses biological evolution analogy — mutation, selection, crossover.\n",
    "\n",
    "✅ Good for complex non-smooth search spaces.\n",
    "❌ Computationally expensive.\n",
    "\n",
    "Used in NAS (Neural Architecture Search).\n",
    "\n",
    "---\n",
    "\n",
    "### **Hyperband / BOHB**\n",
    "\n",
    "* **Hyperband**: Trains many models for few epochs, kills the bad ones early.\n",
    "* **BOHB**: Combines Bayesian Optimization + Hyperband.\n",
    "\n",
    "✅ Efficient, scalable for large deep learning setups (e.g., CNN, Transformer).\n",
    "\n",
    "---\n",
    "\n",
    "## **Hyperparameter Tuning per Problem Type**\n",
    "\n",
    "| Problem Type                            | Objective Metric            | Example Tuned Parameters                          |\n",
    "| --------------------------------------- | --------------------------- | ------------------------------------------------- |\n",
    "| **Binary / Multi-class Classification** | Accuracy, F1-score, ROC-AUC | Hidden layers, learning rate, batch size, dropout |\n",
    "| **Regression**                          | MAE, RMSE, R²               | Hidden neurons, learning rate, regularization     |\n",
    "| **Autoencoders (Unsupervised)**         | Reconstruction loss (MSE)   | Bottleneck size, optimizer, activation            |\n",
    "| **Time-Series Forecasting (RNN/LSTM)**  | MAPE, RMSE                  | Sequence length, learning rate, dropout           |\n",
    "| **Image / Vision (CNN)**                | Accuracy, Top-5 Accuracy    | Filters, kernel size, learning rate, epochs       |\n",
    "\n",
    "---\n",
    "\n",
    "## **Example: Hyperparameter Tuning for Classification (Keras)**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def build_model(lr=0.001, dropout=0.3, neurons=64):\n",
    "    model = Sequential([\n",
    "        Dense(neurons, activation='relu', input_dim=30),\n",
    "        Dropout(dropout),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "```\n",
    "\n",
    "You can then wrap this with `KerasTuner` or `Optuna` to auto-tune `lr`, `dropout`, and `neurons`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Best Practices**\n",
    "\n",
    "1. **Normalize inputs** (StandardScaler) before training.\n",
    "2. Use **early stopping** to prevent overfitting.\n",
    "3. Monitor **validation metrics** (not only training).\n",
    "4. Combine **Random + Bayesian** approaches for efficiency.\n",
    "5. Always record results (e.g., via TensorBoard or Weights & Biases).\n",
    "6. Limit search space — too large causes computational waste.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Summary Table**\n",
    "\n",
    "| Category       | Hyperparameter    | Typical Range         | Purpose                |\n",
    "| -------------- | ----------------- | --------------------- | ---------------------- |\n",
    "| Architecture   | Hidden layers     | 1–5                   | Model complexity       |\n",
    "| Architecture   | Neurons per layer | 16–512                | Representation power   |\n",
    "| Learning       | Learning rate     | 1e-4–1e-2             | Convergence control    |\n",
    "| Optimization   | Batch size        | 16–256                | Gradient stability     |\n",
    "| Regularization | Dropout           | 0.2–0.5               | Prevent overfitting    |\n",
    "| Regularization | L2 weight decay   | 1e-5–1e-2             | Penalize large weights |\n",
    "| Training       | Epochs            | 50–1000               | Learning duration      |\n",
    "| Activation     | Function          | ReLU / Tanh / Sigmoid | Nonlinearity           |\n",
    "| Task           | Loss function     | CE / MSE              | Learning objective     |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Intuition**\n",
    "\n",
    "* **Classification:** Adjust architecture + activation to separate categories.\n",
    "* **Regression:** Control learning rate and regularization for smooth predictions.\n",
    "* **Unsupervised:** Balance compression vs. reconstruction accuracy.\n",
    "* **Tuning Goal:** Find configuration minimizing validation loss while maintaining generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edef32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
