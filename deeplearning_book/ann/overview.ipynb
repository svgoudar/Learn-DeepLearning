{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3009a1b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Artificial Neural Network (ANN)\n",
    "\n",
    "An ANN is a **computational model inspired by the human brain**.\n",
    "\n",
    "* It consists of **neurons (nodes)** arranged in layers.\n",
    "* These neurons are **connected by weights**, which adjust during learning.\n",
    "* ANNs are designed to **learn complex relationships** from data, both linear and nonlinear.\n",
    "\n",
    "**Analogy:**\n",
    "\n",
    "* Input features → sensory neurons in brain.\n",
    "* Hidden layers → processing neurons in brain that extract patterns.\n",
    "* Output → brain’s decision or response.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition \n",
    "\n",
    "The **core idea**:\n",
    "\n",
    "* A neural network **combines inputs in weighted ways** to compute an output.\n",
    "* The network **learns the best weights** to approximate a target function.\n",
    "\n",
    "**Example:** Predicting house prices.\n",
    "\n",
    "* Inputs: size, location, age.\n",
    "* Neurons combine these inputs in different ways (weighted sum + bias).\n",
    "* Activation function transforms them → captures nonlinear patterns (e.g., big house in a bad neighborhood might cost less).\n",
    "* Output: predicted price.\n",
    "\n",
    "**Key intuition points:**\n",
    "\n",
    "1. Each neuron is a **function approximator**.\n",
    "2. Multiple neurons → can model **complex functions**.\n",
    "3. Layers allow **hierarchical feature extraction**:\n",
    "\n",
    "   * Early layers → basic features (edges in images).\n",
    "   * Deeper layers → complex features (objects in images).\n",
    "\n",
    "---\n",
    "\n",
    "### Components of an ANN\n",
    "\n",
    "#### Neurons\n",
    "\n",
    "* Input: (x_1, x_2, \\dots, x_n)\n",
    "* Weights: (w_1, w_2, \\dots, w_n)\n",
    "* Bias: (b)\n",
    "* Output: (y = f(\\sum w_i x_i + b))\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Weights = importance of each input.\n",
    "* Bias = shifts the decision boundary.\n",
    "* Activation function = introduces **non-linearity**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Layers**\n",
    "\n",
    "* **Input layer**: raw features.\n",
    "* **Hidden layers**: learn **intermediate patterns**.\n",
    "* **Output layer**: final prediction.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Without hidden layers → linear models.\n",
    "* Hidden layers → network can model **nonlinear relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Activation Functions**\n",
    "\n",
    "* Transform neuron output.\n",
    "* **Why needed?** Without them, multiple layers collapse into a single linear layer.\n",
    "\n",
    "Common functions:\n",
    "\n",
    "1. **Sigmoid** → maps output to (0,1), used in probabilities.\n",
    "2. **Tanh** → maps output to (-1,1), zero-centered.\n",
    "3. **ReLU** → outputs max(0, x), solves vanishing gradient problem.\n",
    "4. **Softmax** → converts outputs into probabilities for multi-class classification.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Activation decides **which neurons “fire”**.\n",
    "* Like brain neurons: only active neurons contribute.\n",
    "\n",
    "---\n",
    "\n",
    "### **Forward Propagation**\n",
    "\n",
    "* **Goal:** compute output from inputs.\n",
    "* Each neuron → weighted sum → activation → next layer.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Signals flow like **electrical signals in brain**.\n",
    "* Each layer extracts increasingly complex features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Loss Function**\n",
    "\n",
    "* Measures **error between predicted and true output**.\n",
    "* Examples:\n",
    "\n",
    "  * Regression → MSE\n",
    "  * Classification → Cross-Entropy\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Loss tells the network **how wrong it is**.\n",
    "* Guides learning through gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **Backpropagation**\n",
    "\n",
    "* Computes **gradients of loss w.r.t weights** using chain rule.\n",
    "* Updates weights to **reduce loss** (learning).\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Like adjusting knobs to minimize error.\n",
    "* Deeper layers receive **gradient feedback** to improve pattern detection.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizers**\n",
    "\n",
    "* Algorithms to adjust weights efficiently.\n",
    "* Examples: SGD, Momentum, RMSProp, Adam.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Optimizer = **strategy to climb down the error hill** toward minimum.\n",
    "* Adam → combines momentum (smoother path) + adaptive learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### **Regularization**\n",
    "\n",
    "* Prevents overfitting.\n",
    "* Techniques: dropout, L1/L2 penalties, early stopping.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Forces network to **generalize**, not memorize training data.\n",
    "* Dropout = randomly deactivate neurons → network learns **robust features**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why ANNs are powerful\n",
    "\n",
    "* Can approximate **any function** (Universal Approximation Theorem).\n",
    "* Handle **high-dimensional data** (images, text, speech).\n",
    "* Learn **hierarchical features** automatically.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Instead of manually engineering features, ANN **learns features by itself**.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Process Summary\n",
    "\n",
    "1. Initialize weights & biases.\n",
    "2. Forward pass → compute outputs.\n",
    "3. Compute loss.\n",
    "4. Backpropagation → compute gradients.\n",
    "5. Update weights via optimizer.\n",
    "6. Repeat for multiple epochs until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Key Problems in ANN**\n",
    "\n",
    "* **Vanishing gradients** → small gradients → slow learning (sigmoid/tanh).\n",
    "* **Exploding gradients** → large gradients → unstable learning.\n",
    "* **Overfitting** → network memorizes training data → poor generalization.\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Proper weight initialization, activation choice, and regularization solve these.\n",
    "\n",
    "\n",
    "### Types of Neural Network\n",
    "\n",
    "#### Feedforward Neural Network (FNN)\n",
    "\n",
    "* **Structure:** Data moves only forward — input → hidden → output layers.\n",
    "* **No feedback or memory.**\n",
    "* **Use:** Basic tasks like image or text classification, regression, tabular data.\n",
    "* **Example:** Predicting house prices, digit recognition (MNIST).\n",
    "\n",
    "---\n",
    "\n",
    "#### Convolutional Neural Network (CNN)\n",
    "\n",
    "* **Structure:** Uses convolution layers to extract spatial patterns from data.\n",
    "* **Key idea:** Detects features like edges → shapes → objects.\n",
    "* **Use:** Image recognition, video analysis, medical imaging.\n",
    "* **Example:** Face detection, self-driving car vision.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recurrent Neural Network (RNN)\n",
    "\n",
    "* **Structure:** Loops within layers, allowing information from previous steps to influence the current output.\n",
    "* **Purpose:** Handles sequential data.\n",
    "* **Use:** Time-series forecasting, language modeling, speech recognition.\n",
    "* **Variants:**\n",
    "\n",
    "  * **LSTM (Long Short-Term Memory):** Handles long dependencies.\n",
    "  * **GRU (Gated Recurrent Unit):** Simplified version of LSTM.\n",
    "\n",
    "---\n",
    "\n",
    "#### Autoencoder\n",
    "\n",
    "* **Structure:** Encoder compresses input → Decoder reconstructs it.\n",
    "* **Goal:** Learn efficient data representation (latent space).\n",
    "* **Use:** Data compression, denoising, anomaly detection, feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "#### Generative Adversarial Network (GAN)\n",
    "\n",
    "* **Structure:** Two networks —\n",
    "\n",
    "  * **Generator:** creates fake data.\n",
    "  * **Discriminator:** detects if data is real or fake.\n",
    "* **Use:** Image generation, data augmentation, deepfakes, art creation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Transformer\n",
    "\n",
    "* **Structure:** Based on *self-attention* — each token (word, image patch, etc.) attends to others.\n",
    "* **No recurrence.**\n",
    "* **Use:** NLP (translation, summarization), vision (ViT), multimodal GenAI (GPT, BERT, CLIP).\n",
    "\n",
    "---\n",
    "\n",
    "#### Radial Basis Function Network (RBFN)\n",
    "\n",
    "* **Structure:** Hidden layer uses radial basis (Gaussian) functions.\n",
    "* **Use:** Function approximation, classification with smooth decision boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "#### Graph Neural Network (GNN)\n",
    "\n",
    "* **Structure:** Operates on graph data (nodes + edges).\n",
    "* **Goal:** Aggregate information from connected nodes.\n",
    "* **Use:** Social network analysis, molecular prediction, recommender systems.\n",
    "\n",
    "---\n",
    "\n",
    "#### Modular / Hybrid Networks\n",
    "\n",
    "* Combine different types for complex tasks.\n",
    "* **Examples:**\n",
    "\n",
    "  * CNN + LSTM → video activity recognition.\n",
    "  * Transformer + CNN → vision-language models.\n",
    "\n",
    "| **Network Type**                         | **Input Type**                       | **Core Structure / Idea**                             | **Memory / Feedback**        | **Main Applications**                                   |\n",
    "| ---------------------------------------- | ------------------------------------ | ----------------------------------------------------- | ---------------------------- | ------------------------------------------------------- |\n",
    "| **Feedforward Neural Network (FNN)**     | Fixed-size vectors (tabular, images) | Sequential layers, data flows forward only            | ❌ No                         | Classification, regression, pattern recognition         |\n",
    "| **Convolutional Neural Network (CNN)**   | Images, videos, spatial data         | Convolution + pooling layers extract spatial features | ❌ No                         | Image recognition, object detection, medical imaging    |\n",
    "| **Recurrent Neural Network (RNN)**       | Sequential/time-series data          | Loops connect previous outputs to current inputs      | ✅ Yes                        | Text, speech, time-series forecasting                   |\n",
    "| **LSTM (Long Short-Term Memory)**        | Sequential data                      | Specialized RNN with gates for long-term memory       | ✅ Yes                        | NLP, speech-to-text, stock prediction                   |\n",
    "| **GRU (Gated Recurrent Unit)**           | Sequential data                      | Simplified LSTM with fewer parameters                 | ✅ Yes                        | Similar to LSTM but faster                              |\n",
    "| **Autoencoder**                          | Any numeric data                     | Encoder compresses, decoder reconstructs input        | ⚙️ Partial                   | Denoising, anomaly detection, dimensionality reduction  |\n",
    "| **GAN (Generative Adversarial Network)** | Any structured data                  | Generator vs. discriminator in adversarial setup      | ⚙️ Indirect                  | Image synthesis, data augmentation, creative generation |\n",
    "| **Transformer**                          | Sequential or tokenized data         | Self-attention layers learn relationships globally    | ✅ Contextual (not recurrent) | NLP (GPT, BERT), multimodal AI, vision transformers     |\n",
    "| **Radial Basis Function Network (RBFN)** | Numeric data                         | Uses Gaussian basis functions for hidden activations  | ❌ No                         | Function approximation, simple classification           |\n",
    "| **Graph Neural Network (GNN)**           | Graphs (nodes & edges)               | Aggregates features from connected nodes              | ✅ Yes                        | Social networks, molecule analysis, recommender systems |\n",
    "| **Hybrid / Modular Networks**            | Mixed data                           | Combines multiple architectures (e.g., CNN + RNN)     | ⚙️ Mixed                     | Complex multimodal tasks (video, audio-text fusion)     |\n",
    "\n",
    "\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
