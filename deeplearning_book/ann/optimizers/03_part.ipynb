{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b5ac3a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Mini-batch Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "* Combines **Gradient Descent (GD)** and **Stochastic Gradient Descent (SGD)**.\n",
    "* Instead of using **all data points** (GD) or **one data point** (SGD), it uses a **small batch of data points** in each iteration.\n",
    "* Introduces a **batch size** parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### How it works\n",
    "\n",
    "1. Split dataset into **batches** (e.g., batch size = 1000).\n",
    "2. For each batch:\n",
    "\n",
    "   * Forward propagate all batch samples.\n",
    "   * Compute **batch loss** (e.g., MSE for regression):\n",
    "     [\n",
    "     \\text{Cost} = \\frac{1}{\\text{batch size}}\\sum_{i=1}^{\\text{batch size}}(y_i - \\hat{y}_i)^2\n",
    "     ]\n",
    "   * Backpropagate and **update weights** using the optimizer.\n",
    "3. Repeat for all batches → **one epoch** is complete.\n",
    "4. Repeat for multiple epochs until convergence.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Dataset = 100,000 samples\n",
    "* Batch size = 1,000\n",
    "* Iterations per epoch = 100 (100,000 ÷ 1,000)\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Reduces noise** compared to SGD:\n",
    "\n",
    "   * Updates are smoother because each batch averages the gradient over multiple samples.\n",
    "2. **Faster convergence than SGD:**\n",
    "\n",
    "   * More stable gradient updates.\n",
    "3. **Efficient resource usage:**\n",
    "\n",
    "   * Uses manageable memory and GPU load compared to full-batch GD.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Noise still exists:**\n",
    "\n",
    "   * Not completely smooth like full-batch GD.\n",
    "2. **May still require time to converge:**\n",
    "\n",
    "   * Especially for very large datasets or small batch sizes.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Optimizer Evolution**\n",
    "\n",
    "| Optimizer      | Data per update | Convergence | Noise   | Resource usage |\n",
    "| -------------- | --------------- | ----------- | ------- | -------------- |\n",
    "| GD             | All data        | Smooth      | Low     | High           |\n",
    "| SGD            | 1 sample        | Slow        | High    | Low            |\n",
    "| Mini-batch SGD | Small batch     | Moderate    | Reduced | Moderate       |\n",
    "\n",
    "* **Mini-batch SGD** balances **speed, stability, and memory efficiency**.\n",
    "* Next step: **SGD with momentum** → further reduces noise and smoothens convergence.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
