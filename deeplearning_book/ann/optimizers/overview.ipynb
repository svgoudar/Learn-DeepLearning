{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf1c462",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Optmizers\n",
    "\n",
    "**Optimizers in neural networks** are algorithms that **adjust the weights and biases** of the network to **minimize the loss function** during training. Their goal is to make the modelâ€™s predictions closer to the actual outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "* After forward propagation, the network predicts outputs (\\hat{y}).\n",
    "* Loss function (L(y, \\hat{y})) measures error.\n",
    "* Optimizers use **gradients** from backpropagation to update weights and reduce this error.\n",
    "\n",
    "**Weight update formula (general):**\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w_{\\text{old}}}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "* ( \\eta ) = learning rate\n",
    "* ( \\frac{\\partial L}{\\partial w} ) = gradient of the loss w.r.t the weight\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Optimizers\n",
    "\n",
    "| Optimizer                 | Description                                                     | Key Advantage                       |\n",
    "| ------------------------- | --------------------------------------------------------------- | ----------------------------------- |\n",
    "| **Gradient Descent (GD)** | Uses all training data to compute gradient                      | Stable convergence                  |\n",
    "| **Stochastic GD (SGD)**   | Uses one training example at a time                             | Faster updates, less memory         |\n",
    "| **Mini-batch GD**         | Uses a small batch of data per update                           | Balance between GD and SGD          |\n",
    "| **SGD with Momentum**     | Adds momentum to gradients                                      | Helps escape local minima           |\n",
    "| **Adagrad**               | Adaptive learning rates per parameter                           | Good for sparse data                |\n",
    "| **RMSProp**               | Scales learning rates using moving average of squared gradients | Works well in practice              |\n",
    "| **Adam**                  | Combines momentum and RMSProp                                   | Most commonly used, fast and stable |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "* Optimizers control **how the network learns**.\n",
    "* Learning rate ((\\eta)) determines **step size** in weight updates.\n",
    "* Choice of optimizer affects **training speed, convergence, and stability**.\n",
    "\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
