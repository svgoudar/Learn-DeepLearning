{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c481a6a4",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Stochastic Gradient Descent (SGD) Optimizer\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Concept\n",
    "\n",
    "* **Goal:** Reduce the loss function by updating weights during training.\n",
    "* **Difference from Gradient Descent:**\n",
    "\n",
    "  * Gradient Descent (GD) uses **all training data** in one forward and backward pass.\n",
    "  * SGD uses **one data point at a time** for forward and backward propagation.\n",
    "\n",
    "**Weight update formula:**\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w_{\\text{old}}}\n",
    "$$\n",
    "\n",
    "* $\\eta$ = learning rate\n",
    "* $L$ = loss for **one sample**\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ How it works\n",
    "\n",
    "1. Pick **one data point**.\n",
    "2. Compute predicted output $\\hat{y}$.\n",
    "3. Calculate loss $L(y, \\hat{y})$.\n",
    "4. Update weights using gradient.\n",
    "5. Repeat for next data point.\n",
    "\n",
    "* **One epoch:** passing **all data points once**.\n",
    "* If 1000 points → 1000 iterations per epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Advantages\n",
    "\n",
    "* **Memory efficient:** Can train on systems with limited RAM or GPU.\n",
    "* **Scales to large datasets:** Avoids loading all data at once.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Disadvantages\n",
    "\n",
    "1. **Time-consuming:** Updating weights per data point is slow for large datasets.\n",
    "2. **Noisy updates:** Convergence path jumps around, not smooth like GD.\n",
    "3. **Slower convergence:** May take more epochs to reach the global minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Noise explanation\n",
    "\n",
    "* Because each weight update is based on **one sample**, the loss function fluctuates.\n",
    "* The training curve is jagged rather than smooth.\n",
    "* The global minimum is eventually reached, but the path is less stable.\n",
    "\n",
    "---\n",
    "\n",
    "### 6️⃣ Solution to noise\n",
    "\n",
    "* **Mini-batch SGD:** Uses a small batch of data points (e.g., 32, 64) instead of one.\n",
    "* Reduces noise and speeds up convergence while keeping memory requirements low.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* **SGD = weight update per sample** → memory efficient, introduces noise, slower convergence.\n",
    "* **Mini-batch SGD = weight update per batch** → balances speed, stability, and memory.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
